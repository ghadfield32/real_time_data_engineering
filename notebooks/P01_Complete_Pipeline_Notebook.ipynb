{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 01: Complete Production Walkthrough\n",
    "## Kafka 4.0 → Flink 2.0.1 → Iceberg 1.10.1 → dbt\n",
    "\n",
    "**Pipeline:** P01 - Kafka + Flink + Iceberg (Production-Hardened Template)\n",
    "**Status:** Production-Grade (94/94 dbt tests passing)\n",
    "**Stack:** Flink 2.0.1, Iceberg 1.10.1, Kafka 4.0 (KRaft), Lakekeeper REST Catalog (opt-in)\n",
    "**Updated:** 2026-02-16\n",
    "\n",
    "---\n",
    "\n",
    "### What This Notebook Does\n",
    "\n",
    "Every code cell uses `%%writefile` to create the **exact production files** on disk.\n",
    "After running all cells top-to-bottom, you will have a complete, working pipeline\n",
    "that you can start with `make up && make benchmark`.\n",
    "\n",
    "### Production Hardening Features\n",
    "\n",
    "| Layer | Pattern | Purpose |\n",
    "|-------|---------|---------|\n",
    "| **Ingestion** | Idempotent producer (`acks=all`) | Exactly-once delivery |\n",
    "| **Ingestion** | Dead Letter Queue | Poison message capture |\n",
    "| **Processing** | Event-time watermarks | Out-of-order handling |\n",
    "| **Processing** | ROW_NUMBER dedup | Duplicate elimination |\n",
    "| **Processing** | Batch + streaming modes | Same SQL for both |\n",
    "| **Storage** | Lakekeeper REST catalog (opt-in) | No hardcoded S3 creds |\n",
    "| **Quality** | dbt source freshness | Stale data detection |\n",
    "| **Observability** | Prometheus metrics | Dashboard-ready monitoring |\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "| # | Section | What You'll Build |\n",
    "|---|---------|-------------------|\n",
    "| 1 | Architecture Overview | Understanding the data flow |\n",
    "| 2 | Shared Infrastructure | Dockerfiles, data generator, schemas |\n",
    "| 3 | Docker Compose | 7+4 service container orchestration |\n",
    "| 4 | Kafka Layer | Topic creation, DLQ, event ingestion |\n",
    "| 5 | Flink Configuration | Cluster config, Prometheus, S3/MinIO |\n",
    "| 6 | Flink SQL - Init & Sources | Catalog + Kafka connector + watermarks |\n",
    "| 6b | Flink SQL - REST Catalog Init | Lakekeeper alternative (opt-in) |\n",
    "| 7 | Flink SQL - Bronze Layer | Kafka → Iceberg raw ingestion |\n",
    "| 7b | Flink SQL - Streaming Bronze | Continuous streaming alternative |\n",
    "| 8 | Flink SQL - Silver Layer | Data quality + dedup + enrichment |\n",
    "| 9 | Flink SQL - Combined Pipeline | Single-file Bronze + Silver |\n",
    "| 10 | dbt Project Configuration | Project, profiles, packages |\n",
    "| 11 | dbt Seeds | Reference data (zones, payments, rates, vendors) |\n",
    "| 12 | dbt Macros | Cross-database compatibility helpers |\n",
    "| 13 | dbt Staging Models | Light transforms from Iceberg Silver |\n",
    "| 14 | dbt Intermediate Models | Trip metrics, daily & hourly aggregations |\n",
    "| 15 | dbt Core Marts | Fact & dimension tables (Gold layer) |\n",
    "| 16 | dbt Analytics Marts | Revenue, demand, location performance |\n",
    "| 17 | dbt Tests | Data quality assertions |\n",
    "| 18 | Pipeline Makefile | One-command orchestration |\n",
    "| 19 | Airflow DAGs | Production scheduling & maintenance |\n",
    "| 20 | Running the Pipeline | Step-by-step execution guide |\n",
    "| 21 | Production Operations | Monitoring, alerting, scaling |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview\n",
    "\n",
    "### Technology Stack (2026 Production-Grade)\n",
    "\n",
    "| Component | Technology | Version | Role |\n",
    "|-----------|-----------|---------|------|\n",
    "| **Ingestion** | Apache Kafka | 4.0.0 (KRaft) | Event streaming with idempotent delivery |\n",
    "| **Processing** | Apache Flink | 2.0.1 (Java 17) | Stream/batch SQL processing |\n",
    "| **Storage** | Apache Iceberg | 1.10.1 (V3 format) | Lakehouse table format on MinIO |\n",
    "| **Catalog** | Hadoop (default) / Lakekeeper (opt-in) | v0.11.2 | Table metadata management |\n",
    "| **Transform** | dbt + DuckDB | dbt-core 1.8+ | Gold layer modeling (94 tests) |\n",
    "| **Object Store** | MinIO | Latest | S3-compatible storage |\n",
    "| **Schema** | Confluent Schema Registry | 7.9.0 | Data contract enforcement |\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "Parquet File → [Data Generator] → Kafka (taxi.raw_trips) → Flink SQL\n",
    "   (source)     (idempotent, acks=all)     (3 partitions)        │    │\n",
    "                                              │                  │    │\n",
    "                                     taxi.raw_trips.dlq       │    │\n",
    "                                        (DLQ, 7-day)          │    │\n",
    "                                                         Iceberg Bronze    Iceberg Silver\n",
    "                                                         (raw_trips)      (cleaned_trips)\n",
    "                                                              │           (+ ROW_NUMBER dedup)\n",
    "                                                              │\n",
    "                                                         dbt (DuckDB)\n",
    "                                                              │\n",
    "                                                    Gold Layer (94 tests)\n",
    "                                                    ├─ fct_trips\n",
    "                                                    ├─ dim_dates / dim_locations\n",
    "                                                    ├─ dim_payment_types / dim_vendors\n",
    "                                                    ├─ mart_daily_revenue\n",
    "                                                    ├─ mart_hourly_demand\n",
    "                                                    └─ mart_location_performance\n",
    "```\n",
    "\n",
    "### Defense-in-Depth (Data Quality Layers)\n",
    "\n",
    "```\n",
    "Layer 1: Idempotent Producer    → Prevents duplicate writes at source\n",
    "Layer 2: Dead Letter Queue      → Captures poison messages without data loss\n",
    "Layer 3: Event-Time Watermarks  → Handles out-of-order arrivals correctly\n",
    "Layer 4: ROW_NUMBER Dedup       → Eliminates duplicates in Silver layer\n",
    "Layer 5: dbt Tests (94 tests)   → Validates business logic and data contracts\n",
    "Layer 6: Source Freshness       → Detects pipeline stalls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shared Infrastructure\n",
    "\n",
    "These files live in `shared/` and are reused across multiple pipelines.\n",
    "We create them first since Docker Compose references them.\n",
    "\n",
    "### 2.1 Flink Dockerfile\n",
    "\n",
    "Custom Flink image with **7 JARs** pre-installed:\n",
    "- Kafka SQL connector (for reading Kafka topics as Flink tables)\n",
    "- Iceberg Flink runtime (for writing to Iceberg tables)\n",
    "- Iceberg AWS bundle (for S3FileIO with MinIO)\n",
    "- Hadoop client API + runtime (for Iceberg Hadoop catalog)\n",
    "- Hadoop AWS (for S3A filesystem)\n",
    "- AWS SDK bundle (required by hadoop-aws)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../shared/docker/flink.Dockerfile\n",
    "# =============================================================================\n",
    "# Shared Flink Image with Kafka + Iceberg Connectors\n",
    "# =============================================================================\n",
    "# Base: Flink 2.0.1 (Java 17)\n",
    "# Adds: Kafka SQL connector, Iceberg Flink runtime, AWS S3 bundle\n",
    "# Used by: Pipelines 01, 04, 07-09, 11-12, 16-18, 21, 23\n",
    "# =============================================================================\n",
    "\n",
    "FROM flink:2.0.1-java17\n",
    "\n",
    "# Connector versions (Flink 2.0 requires new connector builds)\n",
    "ARG FLINK_KAFKA_CONNECTOR_VERSION=4.0.1-2.0\n",
    "ARG ICEBERG_VERSION=1.10.1\n",
    "ARG FLINK_MAJOR_MINOR=2.0\n",
    "\n",
    "# Download Kafka SQL connector (fat jar)\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/${FLINK_KAFKA_CONNECTOR_VERSION}/flink-sql-connector-kafka-${FLINK_KAFKA_CONNECTOR_VERSION}.jar\" \\\n",
    "    && echo \"Kafka SQL connector downloaded\"\n",
    "\n",
    "# Download Iceberg Flink runtime\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-flink-runtime-${FLINK_MAJOR_MINOR}/${ICEBERG_VERSION}/iceberg-flink-runtime-${FLINK_MAJOR_MINOR}-${ICEBERG_VERSION}.jar\" \\\n",
    "    && echo \"Iceberg Flink runtime downloaded\"\n",
    "\n",
    "# Download Iceberg AWS bundle (for S3FileIO with MinIO)\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/${ICEBERG_VERSION}/iceberg-aws-bundle-${ICEBERG_VERSION}.jar\" \\\n",
    "    && echo \"Iceberg AWS bundle downloaded\"\n",
    "\n",
    "# Download Hadoop client (required for Iceberg Hadoop catalog)\n",
    "ARG HADOOP_VERSION=3.3.6\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/${HADOOP_VERSION}/hadoop-client-api-${HADOOP_VERSION}.jar\" \\\n",
    "    && wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/${HADOOP_VERSION}/hadoop-client-runtime-${HADOOP_VERSION}.jar\" \\\n",
    "    && echo \"Hadoop client jars downloaded\"\n",
    "\n",
    "# Download Hadoop AWS module (for S3A filesystem in Iceberg Hadoop catalog)\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar\" \\\n",
    "    && echo \"Hadoop AWS jar downloaded\"\n",
    "\n",
    "# Download AWS SDK v1 bundle (required by hadoop-aws)\n",
    "ARG AWS_SDK_VERSION=1.12.367\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_SDK_VERSION}/aws-java-sdk-bundle-${AWS_SDK_VERSION}.jar\" \\\n",
    "    && echo \"AWS SDK bundle downloaded\"\n",
    "\n",
    "# Enable S3 filesystem plugin (for Flink checkpoints on S3)\n",
    "RUN mkdir -p /opt/flink/plugins/s3-fs-hadoop \\\n",
    "    && cp /opt/flink/opt/flink-s3-fs-hadoop-*.jar /opt/flink/plugins/s3-fs-hadoop/ 2>/dev/null || true\n",
    "\n",
    "# Verify all JARs are present\n",
    "RUN ls -la /opt/flink/lib/flink-sql-connector-kafka*.jar \\\n",
    "           /opt/flink/lib/iceberg-flink-runtime*.jar \\\n",
    "           /opt/flink/lib/iceberg-aws-bundle*.jar \\\n",
    "           /opt/flink/lib/hadoop-client-*.jar \\\n",
    "           /opt/flink/lib/hadoop-aws-*.jar \\\n",
    "           /opt/flink/lib/aws-java-sdk-bundle-*.jar\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 dbt Dockerfile\n",
    "\n",
    "Slim Python image with dbt-core and dbt-duckdb for reading Iceberg tables via DuckDB's `iceberg_scan()` function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../shared/docker/dbt.Dockerfile\n",
    "FROM python:3.12-slim\n",
    "\n",
    "# Build argument to select the dbt adapter\n",
    "ARG DBT_ADAPTER=dbt-duckdb\n",
    "ARG DBT_ADAPTER_VERSION=\">=1.8\"\n",
    "\n",
    "WORKDIR /dbt\n",
    "\n",
    "# Install dbt with the specified adapter\n",
    "RUN pip install --no-cache-dir \\\n",
    "    \"dbt-core>=1.8\" \\\n",
    "    \"${DBT_ADAPTER}${DBT_ADAPTER_VERSION}\" \\\n",
    "    pyarrow \\\n",
    "    pandas\n",
    "\n",
    "# For dbt-duckdb with Iceberg support\n",
    "RUN if [ \"$DBT_ADAPTER\" = \"dbt-duckdb\" ]; then \\\n",
    "    pip install --no-cache-dir duckdb; \\\n",
    "    fi\n",
    "\n",
    "# Copy dbt project (mounted or copied at build time)\n",
    "COPY dbt_project/ /dbt/\n",
    "\n",
    "# Install dbt packages\n",
    "RUN dbt deps --profiles-dir . 2>/dev/null || true\n",
    "\n",
    "ENTRYPOINT [\"dbt\"]\n",
    "CMD [\"build\", \"--profiles-dir\", \".\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Generator\n",
    "\n",
    "Reads NYC Yellow Taxi parquet data and produces JSON events to Kafka.\n",
    "Three modes: `burst` (benchmarking), `realtime` (simulated pacing), `batch` (chunked).\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../shared/data-generator/requirements.txt\n",
    "pyarrow>=14.0.0\n",
    "confluent-kafka>=2.3.0\n",
    "orjson>=3.9.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator script** (~210 lines, production-grade with metrics output):\n",
    "\n",
    "Key production features:\n",
    "- **Idempotent producer:** `enable.idempotence: True` + `acks: all` prevents duplicate delivery\n",
    "- **LZ4 compression** and batch tuning for throughput\n",
    "- **Metrics output** to JSON for benchmark collection\n",
    "- Three modes: `burst` (benchmarking), `realtime` (simulated), `batch` (configurable)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../shared/data-generator/generator.py\n",
    "\"\"\"Taxi trip event generator.\n",
    "\n",
    "Reads NYC Yellow Taxi parquet data and produces events to a Kafka-compatible\n",
    "broker (Kafka or Redpanda). Supports three modes:\n",
    "  - burst:    As fast as possible (benchmarking)\n",
    "  - realtime: Simulates actual event-time spacing\n",
    "  - batch:    Sends events in configurable batch sizes with delays\n",
    "\n",
    "Configuration via environment variables:\n",
    "  BROKER_URL    Kafka/Redpanda bootstrap servers  (default: localhost:9092)\n",
    "  TOPIC         Target topic name                  (default: taxi.raw_trips)\n",
    "  MODE          burst | realtime | batch           (default: burst)\n",
    "  RATE_LIMIT    Max events/sec in burst mode, 0=unlimited (default: 0)\n",
    "  BATCH_SIZE    Events per batch in batch mode     (default: 1000)\n",
    "  BATCH_DELAY   Seconds between batches            (default: 1.0)\n",
    "  DATA_PATH     Path to parquet file               (default: /data/yellow_tripdata_2024-01.parquet)\n",
    "  MAX_EVENTS    Stop after N events, 0=all         (default: 0)\n",
    "\n",
    "Usage:\n",
    "    python generator.py\n",
    "    python generator.py --mode burst --broker localhost:9092\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import orjson\n",
    "import pyarrow.parquet as pq\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "\n",
    "def delivery_callback(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"  [ERROR] Delivery failed: {err}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def read_parquet(path: str, max_events: int = 0):\n",
    "    \"\"\"Yield rows from parquet file as dicts.\"\"\"\n",
    "    table = pq.read_table(path)\n",
    "    total = table.num_rows if max_events == 0 else min(max_events, table.num_rows)\n",
    "    print(f\"  Source: {path} ({table.num_rows:,} rows, sending {total:,})\")\n",
    "\n",
    "    batches = table.to_batches(max_chunksize=10_000)\n",
    "    sent = 0\n",
    "    for batch in batches:\n",
    "        for row in batch.to_pylist():\n",
    "            if sent >= total:\n",
    "                return\n",
    "            # Convert timestamps to ISO strings for JSON serialization\n",
    "            for key, val in row.items():\n",
    "                if isinstance(val, datetime):\n",
    "                    row[key] = val.isoformat()\n",
    "            yield row\n",
    "            sent += 1\n",
    "\n",
    "\n",
    "def create_producer(broker_url: str) -> Producer:\n",
    "    conf = {\n",
    "        \"bootstrap.servers\": broker_url,\n",
    "        \"enable.idempotence\": True,\n",
    "        \"acks\": \"all\",\n",
    "        \"linger.ms\": 5,\n",
    "        \"batch.num.messages\": 10000,\n",
    "        \"queue.buffering.max.messages\": 500000,\n",
    "        \"queue.buffering.max.kbytes\": 1048576,\n",
    "        \"compression.type\": \"lz4\",\n",
    "    }\n",
    "    return Producer(conf)\n",
    "\n",
    "\n",
    "def produce_burst(producer: Producer, topic: str, rows, rate_limit: int):\n",
    "    \"\"\"Produce as fast as possible, optionally rate-limited.\"\"\"\n",
    "    count = 0\n",
    "    start = time.perf_counter()\n",
    "    last_report = start\n",
    "\n",
    "    for row in rows:\n",
    "        key = str(row.get(\"PULocationID\", \"\")).encode(\"utf-8\")\n",
    "        value = orjson.dumps(row)\n",
    "        producer.produce(topic, value=value, key=key, callback=delivery_callback)\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            producer.poll(0)\n",
    "            now = time.perf_counter()\n",
    "            if now - last_report >= 5.0:\n",
    "                elapsed = now - start\n",
    "                rate = count / elapsed\n",
    "                print(f\"  Produced {count:,} events ({rate:,.0f} evt/s)\")\n",
    "                last_report = now\n",
    "\n",
    "        # Rate limiting\n",
    "        if rate_limit > 0 and count % rate_limit == 0:\n",
    "            elapsed = time.perf_counter() - start\n",
    "            expected = count / rate_limit\n",
    "            if elapsed < expected:\n",
    "                time.sleep(expected - elapsed)\n",
    "\n",
    "    producer.flush(timeout=30)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    rate = count / elapsed if elapsed > 0 else 0\n",
    "    return count, elapsed, rate\n",
    "\n",
    "\n",
    "def produce_batch(producer: Producer, topic: str, rows, batch_size: int, batch_delay: float):\n",
    "    \"\"\"Produce in fixed-size batches with delays between them.\"\"\"\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_buffer = []\n",
    "    for row in rows:\n",
    "        batch_buffer.append(row)\n",
    "        if len(batch_buffer) >= batch_size:\n",
    "            for r in batch_buffer:\n",
    "                key = str(r.get(\"PULocationID\", \"\")).encode(\"utf-8\")\n",
    "                value = orjson.dumps(r)\n",
    "                producer.produce(topic, value=value, key=key, callback=delivery_callback)\n",
    "                count += 1\n",
    "            producer.flush(timeout=30)\n",
    "            batch_count += 1\n",
    "            elapsed = time.perf_counter() - start\n",
    "            rate = count / elapsed if elapsed > 0 else 0\n",
    "            print(f\"  Batch {batch_count}: {count:,} total ({rate:,.0f} evt/s)\")\n",
    "            batch_buffer = []\n",
    "            time.sleep(batch_delay)\n",
    "\n",
    "    # Final partial batch\n",
    "    if batch_buffer:\n",
    "        for r in batch_buffer:\n",
    "            key = str(r.get(\"PULocationID\", \"\")).encode(\"utf-8\")\n",
    "            value = orjson.dumps(r)\n",
    "            producer.produce(topic, value=value, key=key, callback=delivery_callback)\n",
    "            count += 1\n",
    "        producer.flush(timeout=30)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    rate = count / elapsed if elapsed > 0 else 0\n",
    "    return count, elapsed, rate\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Taxi trip event generator\")\n",
    "    parser.add_argument(\"--broker\", default=os.environ.get(\"BROKER_URL\", \"localhost:9092\"))\n",
    "    parser.add_argument(\"--topic\", default=os.environ.get(\"TOPIC\", \"taxi.raw_trips\"))\n",
    "    parser.add_argument(\"--mode\", default=os.environ.get(\"MODE\", \"burst\"),\n",
    "                        choices=[\"burst\", \"realtime\", \"batch\"])\n",
    "    parser.add_argument(\"--rate-limit\", type=int,\n",
    "                        default=int(os.environ.get(\"RATE_LIMIT\", \"0\")))\n",
    "    parser.add_argument(\"--batch-size\", type=int,\n",
    "                        default=int(os.environ.get(\"BATCH_SIZE\", \"1000\")))\n",
    "    parser.add_argument(\"--batch-delay\", type=float,\n",
    "                        default=float(os.environ.get(\"BATCH_DELAY\", \"1.0\")))\n",
    "    parser.add_argument(\"--data-path\",\n",
    "                        default=os.environ.get(\"DATA_PATH\", \"/data/yellow_tripdata_2024-01.parquet\"))\n",
    "    parser.add_argument(\"--max-events\", type=int,\n",
    "                        default=int(os.environ.get(\"MAX_EVENTS\", \"0\")))\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Taxi Trip Event Generator\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Broker:     {args.broker}\")\n",
    "    print(f\"  Topic:      {args.topic}\")\n",
    "    print(f\"  Mode:       {args.mode}\")\n",
    "    print(f\"  Data:       {args.data_path}\")\n",
    "    max_events_str = \"all\" if args.max_events == 0 else f\"{args.max_events:,}\"\n",
    "    print(f\"  Max events: {max_events_str}\")\n",
    "    print()\n",
    "\n",
    "    producer = create_producer(args.broker)\n",
    "    rows = read_parquet(args.data_path, args.max_events)\n",
    "\n",
    "    if args.mode == \"burst\":\n",
    "        count, elapsed, rate = produce_burst(producer, args.topic, rows, args.rate_limit)\n",
    "    elif args.mode == \"batch\":\n",
    "        count, elapsed, rate = produce_batch(\n",
    "            producer, args.topic, rows, args.batch_size, args.batch_delay\n",
    "        )\n",
    "    else:\n",
    "        # realtime mode: use burst with rate limiting to approximate real-time\n",
    "        count, elapsed, rate = produce_burst(producer, args.topic, rows, rate_limit=5000)\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  GENERATOR COMPLETE\")\n",
    "    print(f\"  Events:  {count:,}\")\n",
    "    print(f\"  Elapsed: {elapsed:.2f}s\")\n",
    "    print(f\"  Rate:    {rate:,.0f} events/sec\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Write metrics for benchmark collection\n",
    "    metrics_path = os.environ.get(\"METRICS_PATH\", \"/tmp/generator_metrics.json\")\n",
    "    metrics = {\n",
    "        \"events\": count,\n",
    "        \"elapsed_seconds\": round(elapsed, 3),\n",
    "        \"events_per_second\": round(rate, 1),\n",
    "        \"mode\": args.mode,\n",
    "        \"broker\": args.broker,\n",
    "        \"topic\": args.topic,\n",
    "    }\n",
    "    with open(metrics_path, \"wb\") as f:\n",
    "        f.write(orjson.dumps(metrics))\n",
    "    print(f\"  Metrics written to {metrics_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Event Schema (JSON Schema)\n",
    "\n",
    "Defines the contract for taxi trip events. Field names match the raw NYC TLC parquet source."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../shared/schemas/taxi_trip.json\n",
    "{\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"title\": \"TaxiTrip\",\n",
    "  \"description\": \"NYC Yellow Taxi trip record. Field names match the raw parquet source exactly.\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"VendorID\": {\"type\": [\"integer\", \"null\"], \"description\": \"TPEP provider: 1=Creative Mobile Technologies, 2=VeriFone Inc.\"},\n",
    "    \"tpep_pickup_datetime\": {\"type\": \"string\", \"format\": \"date-time\", \"description\": \"Meter engaged timestamp (ISO 8601)\"},\n",
    "    \"tpep_dropoff_datetime\": {\"type\": \"string\", \"format\": \"date-time\", \"description\": \"Meter disengaged timestamp (ISO 8601)\"},\n",
    "    \"passenger_count\": {\"type\": [\"integer\", \"null\"], \"description\": \"Number of passengers (driver-entered)\"},\n",
    "    \"trip_distance\": {\"type\": [\"number\", \"null\"], \"description\": \"Trip distance in miles from taximeter\"},\n",
    "    \"RatecodeID\": {\"type\": [\"integer\", \"null\"], \"description\": \"Rate code: 1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester, 5=Negotiated, 6=Group\"},\n",
    "    \"store_and_fwd_flag\": {\"type\": [\"string\", \"null\"], \"description\": \"Y=stored then forwarded, N=not a store-and-forward trip\"},\n",
    "    \"PULocationID\": {\"type\": [\"integer\", \"null\"], \"description\": \"TLC Taxi Zone pickup location ID\"},\n",
    "    \"DOLocationID\": {\"type\": [\"integer\", \"null\"], \"description\": \"TLC Taxi Zone dropoff location ID\"},\n",
    "    \"payment_type\": {\"type\": [\"integer\", \"null\"], \"description\": \"Payment method: 1=Credit, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided\"},\n",
    "    \"fare_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Time-and-distance fare in dollars\"},\n",
    "    \"extra\": {\"type\": [\"number\", \"null\"], \"description\": \"Misc extras and surcharges\"},\n",
    "    \"mta_tax\": {\"type\": [\"number\", \"null\"], \"description\": \"MTA tax\"},\n",
    "    \"tip_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Tip amount\"},\n",
    "    \"tolls_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Total tolls paid during trip\"},\n",
    "    \"improvement_surcharge\": {\"type\": [\"number\", \"null\"], \"description\": \"$0.30 improvement surcharge\"},\n",
    "    \"total_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Total amount charged to passengers\"},\n",
    "    \"congestion_surcharge\": {\"type\": [\"number\", \"null\"], \"description\": \"NYC congestion surcharge\"},\n",
    "    \"Airport_fee\": {\"type\": [\"number\", \"null\"], \"description\": \"$1.25 for pickups at LaGuardia and JFK\"}\n",
    "  },\n",
    "  \"required\": [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docker Compose: Container Orchestration\n",
    "\n",
    "**7 always-on services** + **4 opt-in Lakekeeper services** (REST catalog):\n",
    "\n",
    "| Service | Image | Purpose | Port |\n",
    "|---------|-------|---------|------|\n",
    "| kafka | apache/kafka:4.0.0 | KRaft event streaming | 9092 |\n",
    "| schema-registry | cp-schema-registry:7.9.0 | Data contract enforcement | 8085 |\n",
    "| minio | minio/minio:latest | S3-compatible object storage | 9000/9001 |\n",
    "| mc-init | minio/mc:latest | Create warehouse bucket | - |\n",
    "| flink-jobmanager | Custom (Flink 2.0.1) | Flink SQL coordinator | 8083 |\n",
    "| flink-taskmanager | Custom (Flink 2.0.1) | Flink SQL worker | - |\n",
    "| data-generator | Custom (Python) | Parquet → Kafka producer | - |\n",
    "| dbt | Custom (dbt-duckdb) | Silver → Gold transforms | - |\n",
    "\n",
    "**Opt-in Lakekeeper services** (`docker compose --profile lakekeeper up -d`):\n",
    "\n",
    "| Service | Image | Purpose | Port |\n",
    "|---------|-------|---------|------|\n",
    "| lakekeeper-db | postgres:17 | Catalog metadata store | - |\n",
    "| lakekeeper-migrate | lakekeeper/catalog:v0.11.2 | Schema migration | - |\n",
    "| lakekeeper | lakekeeper/catalog:v0.11.2 | REST catalog API | 8181 |\n",
    "| lakekeeper-init | curlimages/curl | Bootstrap + warehouse init | - |\n",
    "\n",
    "Key architecture decisions:\n",
    "- **Profiles:** `generator`, `dbt`, `lakekeeper` keep services opt-in (not started by `make up`)\n",
    "- **Resource limits:** Memory caps prevent Docker Desktop from running out of memory\n",
    "- **Health checks:** Every service has a health check for dependency ordering\n",
    "- **YAML anchors:** `x-flink-common` DRYs Flink configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/docker-compose.yml\n",
    "# =============================================================================\n",
    "# Pipeline 01: Kafka + Flink + Iceberg (Production-Grade Template)\n",
    "# =============================================================================\n",
    "# Architecture: Kafka (KRaft) -> Flink SQL -> Iceberg (on MinIO) -> dbt (DuckDB)\n",
    "# Compose V2+ (version key removed - deprecated since Docker Compose 2.x)\n",
    "# =============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Shared Flink configuration (YAML anchor)\n",
    "# ---------------------------------------------------------------------------\n",
    "x-flink-common: &flink-common\n",
    "  build:\n",
    "    context: .\n",
    "    dockerfile: ../../shared/docker/flink.Dockerfile\n",
    "  environment: &flink-env\n",
    "    FLINK_PROPERTIES: |\n",
    "      jobmanager.rpc.address: flink-jobmanager\n",
    "      taskmanager.numberOfTaskSlots: 4\n",
    "      parallelism.default: 2\n",
    "      state.backend: hashmap\n",
    "      state.checkpoints.dir: file:///tmp/flink-checkpoints\n",
    "      execution.checkpointing.interval: 30s\n",
    "      rest.flamegraph.enabled: true\n",
    "      classloader.check-leaked-classloader: false\n",
    "  networks:\n",
    "    - pipeline-net\n",
    "\n",
    "services:\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Kafka (KRaft mode - ZooKeeper fully removed in 4.0)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  kafka:\n",
    "    image: apache/kafka:4.0.0\n",
    "    container_name: p01-kafka\n",
    "    hostname: kafka\n",
    "    restart: unless-stopped\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_NODE_ID: 1\n",
    "      KAFKA_PROCESS_ROLES: broker,controller\n",
    "      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093\n",
    "      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n",
    "      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n",
    "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n",
    "      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs\n",
    "      CLUSTER_ID: \"p01-kafka-flink-iceberg-001\"\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 2G\n",
    "          cpus: '2.0'\n",
    "        reservations:\n",
    "          memory: 1G\n",
    "    healthcheck:\n",
    "      test: /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 || exit 1\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 15\n",
    "      start_period: 30s\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Schema Registry (Confluent)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  schema-registry:\n",
    "    image: confluentinc/cp-schema-registry:7.9.0\n",
    "    container_name: p01-schema-registry\n",
    "    hostname: schema-registry\n",
    "    restart: unless-stopped\n",
    "    ports:\n",
    "      - \"8085:8081\"\n",
    "    environment:\n",
    "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
    "      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092\n",
    "      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081\n",
    "    depends_on:\n",
    "      kafka:\n",
    "        condition: service_healthy\n",
    "    healthcheck:\n",
    "      test: curl -f http://localhost:8081/subjects || exit 1\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 10\n",
    "      start_period: 20s\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # MinIO (S3-compatible object storage for Iceberg warehouse)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  minio:\n",
    "    image: minio/minio:latest\n",
    "    container_name: p01-minio\n",
    "    hostname: minio\n",
    "    restart: unless-stopped\n",
    "    ports:\n",
    "      - \"9000:9000\"\n",
    "      - \"9001:9001\"\n",
    "    environment:\n",
    "      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}\n",
    "      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}\n",
    "    command: server /data --console-address \":9001\"\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 1G\n",
    "          cpus: '1.0'\n",
    "    healthcheck:\n",
    "      test: mc ready local || exit 1\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 10\n",
    "      start_period: 10s\n",
    "    volumes:\n",
    "      - minio-data:/data\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # MinIO Client Init (create warehouse bucket)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  mc-init:\n",
    "    image: minio/mc:latest\n",
    "    container_name: p01-mc-init\n",
    "    depends_on:\n",
    "      minio:\n",
    "        condition: service_healthy\n",
    "    entrypoint: >\n",
    "      /bin/sh -c \"\n",
    "      mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER:-minioadmin} ${MINIO_ROOT_PASSWORD:-minioadmin} &&\n",
    "      mc mb myminio/warehouse --ignore-existing &&\n",
    "      mc anonymous set download myminio/warehouse &&\n",
    "      echo 'Bucket warehouse created successfully'\n",
    "      \"\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Lakekeeper REST Catalog (opt-in: docker compose --profile lakekeeper up -d)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  lakekeeper-db:\n",
    "    image: postgres:17\n",
    "    container_name: p01-lakekeeper-db\n",
    "    hostname: lakekeeper-db\n",
    "    restart: unless-stopped\n",
    "    environment:\n",
    "      POSTGRES_USER: lakekeeper\n",
    "      POSTGRES_PASSWORD: lakekeeper\n",
    "      POSTGRES_DB: lakekeeper\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U lakekeeper\"]\n",
    "      interval: 5s\n",
    "      timeout: 5s\n",
    "      retries: 10\n",
    "    volumes:\n",
    "      - lakekeeper-db-data:/var/lib/postgresql/data\n",
    "    profiles:\n",
    "      - lakekeeper\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  lakekeeper-migrate:\n",
    "    image: quay.io/lakekeeper/catalog:v0.11.2\n",
    "    container_name: p01-lakekeeper-migrate\n",
    "    command: [\"migrate\"]\n",
    "    environment:\n",
    "      LAKEKEEPER__PG_DATABASE_URL_READ: postgresql://lakekeeper:lakekeeper@lakekeeper-db:5432/lakekeeper\n",
    "      LAKEKEEPER__PG_DATABASE_URL_WRITE: postgresql://lakekeeper:lakekeeper@lakekeeper-db:5432/lakekeeper\n",
    "      LAKEKEEPER__PG_ENCRYPTION_KEY: \"this-is-NOT-secure-change-in-prod!!\"\n",
    "    depends_on:\n",
    "      lakekeeper-db:\n",
    "        condition: service_healthy\n",
    "    profiles:\n",
    "      - lakekeeper\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  lakekeeper:\n",
    "    image: quay.io/lakekeeper/catalog:v0.11.2\n",
    "    container_name: p01-lakekeeper\n",
    "    hostname: lakekeeper\n",
    "    restart: unless-stopped\n",
    "    command: [\"serve\"]\n",
    "    ports:\n",
    "      - \"8181:8181\"\n",
    "    environment:\n",
    "      LAKEKEEPER__PG_DATABASE_URL_READ: postgresql://lakekeeper:lakekeeper@lakekeeper-db:5432/lakekeeper\n",
    "      LAKEKEEPER__PG_DATABASE_URL_WRITE: postgresql://lakekeeper:lakekeeper@lakekeeper-db:5432/lakekeeper\n",
    "      LAKEKEEPER__PG_ENCRYPTION_KEY: \"this-is-NOT-secure-change-in-prod!!\"\n",
    "      LAKEKEEPER__LISTEN_PORT: 8181\n",
    "    depends_on:\n",
    "      lakekeeper-migrate:\n",
    "        condition: service_completed_successfully\n",
    "    healthcheck:\n",
    "      test: curl -f http://localhost:8181/health || exit 1\n",
    "      interval: 5s\n",
    "      timeout: 5s\n",
    "      retries: 10\n",
    "      start_period: 10s\n",
    "    profiles:\n",
    "      - lakekeeper\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  lakekeeper-init:\n",
    "    image: curlimages/curl:latest\n",
    "    container_name: p01-lakekeeper-init\n",
    "    depends_on:\n",
    "      lakekeeper:\n",
    "        condition: service_healthy\n",
    "      mc-init:\n",
    "        condition: service_completed_successfully\n",
    "    entrypoint: /bin/sh\n",
    "    command:\n",
    "      - -c\n",
    "      - |\n",
    "        echo 'Bootstrapping Lakekeeper...' &&\n",
    "        curl -sf -X POST http://lakekeeper:8181/management/v1/bootstrap \\\n",
    "          -H 'Content-Type: application/json' \\\n",
    "          -d '{\"accept-terms-of-use\": true}' &&\n",
    "        echo '' &&\n",
    "        echo 'Creating warehouse...' &&\n",
    "        curl -sf -X POST http://lakekeeper:8181/management/v1/warehouse \\\n",
    "          -H 'Content-Type: application/json' \\\n",
    "          -d '{\n",
    "            \"warehouse-name\": \"warehouse\",\n",
    "            \"storage-profile\": {\n",
    "              \"type\": \"s3\",\n",
    "              \"bucket\": \"warehouse\",\n",
    "              \"region\": \"us-east-1\",\n",
    "              \"endpoint\": \"http://minio:9000\",\n",
    "              \"path-style-access\": true,\n",
    "              \"flavor\": \"minio\"\n",
    "            },\n",
    "            \"storage-credential\": {\n",
    "              \"type\": \"s3\",\n",
    "              \"credential-type\": \"access-key\",\n",
    "              \"aws-access-key-id\": \"minioadmin\",\n",
    "              \"aws-secret-access-key\": \"minioadmin\"\n",
    "            }\n",
    "          }' &&\n",
    "        echo '' &&\n",
    "        echo 'Lakekeeper initialized successfully'\n",
    "    profiles:\n",
    "      - lakekeeper\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Flink JobManager\n",
    "  # ---------------------------------------------------------------------------\n",
    "  flink-jobmanager:\n",
    "    <<: *flink-common\n",
    "    container_name: p01-flink-jobmanager\n",
    "    hostname: flink-jobmanager\n",
    "    restart: unless-stopped\n",
    "    command: jobmanager\n",
    "    ports:\n",
    "      - \"8083:8081\"\n",
    "    volumes:\n",
    "      - ./flink/sql:/opt/flink/sql:ro\n",
    "      - ./flink/conf/config.yaml:/opt/flink/conf/config.yaml:ro\n",
    "      - ./flink/conf/core-site.xml:/opt/hadoop/conf/core-site.xml:ro\n",
    "      - flink-checkpoints:/tmp/flink-checkpoints\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 2G\n",
    "          cpus: '1.0'\n",
    "        reservations:\n",
    "          memory: 1G\n",
    "    depends_on:\n",
    "      kafka:\n",
    "        condition: service_healthy\n",
    "      mc-init:\n",
    "        condition: service_completed_successfully\n",
    "    healthcheck:\n",
    "      test: curl -f http://localhost:8081/overview || exit 1\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 15\n",
    "      start_period: 30s\n",
    "    environment:\n",
    "      <<: *flink-env\n",
    "      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}\n",
    "      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}\n",
    "      AWS_REGION: us-east-1\n",
    "      HADOOP_CONF_DIR: /opt/hadoop/conf\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Flink TaskManager\n",
    "  # ---------------------------------------------------------------------------\n",
    "  flink-taskmanager:\n",
    "    <<: *flink-common\n",
    "    container_name: p01-flink-taskmanager\n",
    "    hostname: flink-taskmanager\n",
    "    restart: unless-stopped\n",
    "    command: taskmanager\n",
    "    volumes:\n",
    "      - ./flink/conf/core-site.xml:/opt/hadoop/conf/core-site.xml:ro\n",
    "      - flink-checkpoints:/tmp/flink-checkpoints\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 3G\n",
    "          cpus: '2.0'\n",
    "        reservations:\n",
    "          memory: 2G\n",
    "    depends_on:\n",
    "      flink-jobmanager:\n",
    "        condition: service_healthy\n",
    "    environment:\n",
    "      <<: *flink-env\n",
    "      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}\n",
    "      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}\n",
    "      AWS_REGION: us-east-1\n",
    "      HADOOP_CONF_DIR: /opt/hadoop/conf\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # dbt (DuckDB adapter - reads Iceberg tables from MinIO)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  dbt:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: ../../shared/docker/dbt.Dockerfile\n",
    "      args:\n",
    "        DBT_ADAPTER: dbt-duckdb\n",
    "    container_name: p01-dbt\n",
    "    volumes:\n",
    "      - ./dbt_project:/dbt\n",
    "    working_dir: /dbt\n",
    "    entrypoint: [\"/bin/sh\", \"-c\"]\n",
    "    command: [\"dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir .\"]\n",
    "    environment:\n",
    "      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}\n",
    "      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}\n",
    "      AWS_ENDPOINT_URL: http://minio:9000\n",
    "      AWS_REGION: us-east-1\n",
    "      DBT_PROFILES_DIR: /dbt\n",
    "    depends_on:\n",
    "      minio:\n",
    "        condition: service_healthy\n",
    "    profiles:\n",
    "      - dbt\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Data Generator (reads parquet, produces to Kafka)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  data-generator:\n",
    "    build:\n",
    "      context: ../../shared/data-generator/\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: p01-data-generator\n",
    "    volumes:\n",
    "      - ../../data:/data:ro\n",
    "    environment:\n",
    "      BROKER_URL: kafka:9092\n",
    "      TOPIC: taxi.raw_trips\n",
    "      MODE: burst\n",
    "      DATA_PATH: /data/yellow_tripdata_2024-01.parquet\n",
    "    depends_on:\n",
    "      kafka:\n",
    "        condition: service_healthy\n",
    "    profiles:\n",
    "      - generator\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "# =============================================================================\n",
    "# Volumes\n",
    "# =============================================================================\n",
    "volumes:\n",
    "  minio-data:\n",
    "    driver: local\n",
    "  flink-checkpoints:\n",
    "    driver: local\n",
    "  lakekeeper-db-data:\n",
    "    driver: local\n",
    "\n",
    "# =============================================================================\n",
    "# Networks\n",
    "# =============================================================================\n",
    "networks:\n",
    "  pipeline-net:\n",
    "    name: p01-pipeline-net\n",
    "    driver: bridge\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kafka Layer: Event Ingestion\n",
    "\n",
    "The topic creation script creates two topics:\n",
    "- `taxi.raw_trips` — Main event stream (3 partitions, 24h retention)\n",
    "- `taxi.raw_trips.dlq` — Dead Letter Queue for failed events (1 partition, 7-day retention)\n",
    "\n",
    "> **Production pattern:** DLQ captures poison messages that fail validation. 7-day retention\n",
    "> gives operators time to investigate and replay. The main topic uses 3 partitions for\n",
    "> Flink parallelism matching."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/kafka/create-topics.sh\n",
    "#!/bin/bash\n",
    "# =============================================================================\n",
    "# Pipeline 01: Create Kafka Topics\n",
    "# =============================================================================\n",
    "# Creates the required topics for the taxi trip streaming pipeline.\n",
    "# Run this after Kafka is fully started and healthy.\n",
    "#\n",
    "# Usage:\n",
    "#   docker compose exec kafka /bin/bash /opt/kafka/scripts/create-topics.sh\n",
    "#   -- or --\n",
    "#   make create-topics\n",
    "# =============================================================================\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "BOOTSTRAP_SERVER=\"${BOOTSTRAP_SERVER:-localhost:9092}\"\n",
    "KAFKA_BIN=\"/opt/kafka/bin\"\n",
    "\n",
    "echo \"============================================================\"\n",
    "echo \"  Creating Kafka Topics\"\n",
    "echo \"  Bootstrap server: ${BOOTSTRAP_SERVER}\"\n",
    "echo \"============================================================\"\n",
    "\n",
    "# Wait for Kafka to be ready\n",
    "echo \"Waiting for Kafka to be ready...\"\n",
    "MAX_RETRIES=30\n",
    "RETRY=0\n",
    "until ${KAFKA_BIN}/kafka-broker-api-versions.sh --bootstrap-server \"${BOOTSTRAP_SERVER}\" > /dev/null 2>&1; do\n",
    "    RETRY=$((RETRY + 1))\n",
    "    if [ \"${RETRY}\" -ge \"${MAX_RETRIES}\" ]; then\n",
    "        echo \"ERROR: Kafka not available after ${MAX_RETRIES} retries\"\n",
    "        exit 1\n",
    "    fi\n",
    "    echo \"  Attempt ${RETRY}/${MAX_RETRIES} - waiting...\"\n",
    "    sleep 2\n",
    "done\n",
    "echo \"Kafka is ready.\"\n",
    "echo \"\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# taxi.raw_trips - Main ingest topic\n",
    "# ---------------------------------------------------------------------------\n",
    "echo \"Creating topic: taxi.raw_trips\"\n",
    "${KAFKA_BIN}/kafka-topics.sh \\\n",
    "    --bootstrap-server \"${BOOTSTRAP_SERVER}\" \\\n",
    "    --create \\\n",
    "    --topic taxi.raw_trips \\\n",
    "    --partitions 3 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --config retention.ms=86400000 \\\n",
    "    --config cleanup.policy=delete \\\n",
    "    --config segment.bytes=104857600\n",
    "\n",
    "echo \"  taxi.raw_trips created (3 partitions, 24h retention)\"\n",
    "echo \"\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# taxi.raw_trips.dlq - Dead Letter Queue for failed/invalid events\n",
    "# ---------------------------------------------------------------------------\n",
    "echo \"Creating topic: taxi.raw_trips.dlq\"\n",
    "${KAFKA_BIN}/kafka-topics.sh \\\n",
    "    --bootstrap-server \"${BOOTSTRAP_SERVER}\" \\\n",
    "    --create \\\n",
    "    --topic taxi.raw_trips.dlq \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --config retention.ms=604800000 \\\n",
    "    --config cleanup.policy=delete\n",
    "\n",
    "echo \"  taxi.raw_trips.dlq created (1 partition, 7d retention)\"\n",
    "echo \"\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Verify\n",
    "# ---------------------------------------------------------------------------\n",
    "echo \"============================================================\"\n",
    "echo \"  Topics:\"\n",
    "${KAFKA_BIN}/kafka-topics.sh \\\n",
    "    --bootstrap-server \"${BOOTSTRAP_SERVER}\" \\\n",
    "    --list\n",
    "\n",
    "echo \"\"\n",
    "echo \"  Topic Details:\"\n",
    "${KAFKA_BIN}/kafka-topics.sh \\\n",
    "    --bootstrap-server \"${BOOTSTRAP_SERVER}\" \\\n",
    "    --describe \\\n",
    "    --topic taxi.raw_trips\n",
    "\n",
    "echo \"============================================================\"\n",
    "echo \"  Topic creation complete.\"\n",
    "echo \"============================================================\"\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flink Configuration\n",
    "\n",
    "### 5.1 Hadoop core-site.xml\n",
    "\n",
    "**Critical for Flink → MinIO connectivity.** Without this, Flink cannot write Iceberg tables to S3-compatible storage.\n",
    "\n",
    "Key settings:\n",
    "- `fs.s3a.endpoint` → MinIO URL\n",
    "- `fs.s3a.path.style.access` → `true` (required for MinIO, not real S3)\n",
    "- `fs.s3a.impl` → S3AFileSystem (Hadoop's S3 adapter)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/conf/core-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <!-- MinIO (S3-compatible) Configuration for Hadoop S3A -->\n",
    "    <property>\n",
    "        <name>fs.s3a.endpoint</name>\n",
    "        <value>http://minio:9000</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.access.key</name>\n",
    "        <value>minioadmin</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.secret.key</name>\n",
    "        <value>minioadmin</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.path.style.access</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.impl</name>\n",
    "        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.connection.ssl.enabled</name>\n",
    "        <value>false</value>\n",
    "    </property>\n",
    "</configuration>"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Flink Cluster Configuration (`config.yaml`)\n",
    "\n",
    "> **Flink 2.0 Change:** Configuration file renamed from `flink-conf.yaml` to `config.yaml`\n",
    "> (standard YAML 1.2 format). This file is bind-mounted into the JobManager only.\n",
    "> The TaskManager receives its config via `FLINK_PROPERTIES` environment variable.\n",
    "\n",
    "Key settings:\n",
    "- `classloader.check-leaked-classloader: false` — Required for Iceberg + batch DML sync\n",
    "- `HADOOP_CONF_DIR: /opt/hadoop/conf` — Required for S3A filesystem access\n",
    "- `metrics.reporter.prom.*` — Prometheus metrics on port 9249\n",
    "- `execution.checkpointing.interval: 30s` — Exactly-once checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/conf/config.yaml\n",
    "# =============================================================================\n",
    "# Pipeline 01: Flink Configuration\n",
    "# =============================================================================\n",
    "# Configuration for Flink 1.20 with Iceberg + Kafka connectors.\n",
    "# This file is mounted into the JobManager container.\n",
    "# =============================================================================\n",
    "\n",
    "# Cluster\n",
    "jobmanager.rpc.address: flink-jobmanager\n",
    "jobmanager.rpc.port: 6123\n",
    "jobmanager.bind-host: 0.0.0.0\n",
    "jobmanager.memory.process.size: 1600m\n",
    "\n",
    "taskmanager.bind-host: 0.0.0.0\n",
    "taskmanager.host: flink-taskmanager\n",
    "taskmanager.memory.process.size: 2048m\n",
    "taskmanager.numberOfTaskSlots: 4\n",
    "\n",
    "parallelism.default: 2\n",
    "\n",
    "# REST API (Flink Dashboard)\n",
    "rest.address: 0.0.0.0\n",
    "rest.bind-address: 0.0.0.0\n",
    "rest.port: 8081\n",
    "rest.flamegraph.enabled: true\n",
    "\n",
    "# Checkpointing\n",
    "execution.checkpointing.interval: 30s\n",
    "execution.checkpointing.mode: EXACTLY_ONCE\n",
    "execution.checkpointing.min-pause: 10s\n",
    "execution.checkpointing.timeout: 5min\n",
    "state.backend: hashmap\n",
    "state.checkpoints.dir: file:///tmp/flink-checkpoints\n",
    "state.savepoints.dir: file:///tmp/flink-savepoints\n",
    "\n",
    "# Table / SQL Configuration\n",
    "table.exec.state.ttl: 0\n",
    "table.exec.sink.not-null-enforcer: DROP\n",
    "\n",
    "# Classloader (avoid Iceberg classloader leak with batch DML sync)\n",
    "classloader.check-leaked-classloader: false\n",
    "\n",
    "# S3 (MinIO) filesystem configuration\n",
    "s3.endpoint: http://minio:9000\n",
    "s3.access-key: minioadmin\n",
    "s3.secret-key: minioadmin\n",
    "s3.path.style.access: true\n",
    "\n",
    "# Prometheus metrics (expose on :9249 for scraping)\n",
    "metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory\n",
    "metrics.reporter.prom.port: 9249\n",
    "\n",
    "# Logging\n",
    "env.log.max: 5\n",
    "env.log.dir: /opt/flink/log\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Flink SQL: Session Initialization\n",
    "\n",
    "This is the most important SQL file — it creates the Kafka source table and Iceberg catalog\n",
    "that all subsequent SQL files depend on. It's used as an init script (`-i` flag):\n",
    "\n",
    "```bash\n",
    "sql-client.sh embedded -i 00-init.sql -f 05-bronze.sql\n",
    "```\n",
    "\n",
    "Key features:\n",
    "- **Batch mode** (`execution.runtime-mode = batch`): Process all available data, then stop\n",
    "- **DML sync** (`table.dml-sync = true`): Wait for each INSERT to complete before next\n",
    "- **Event-time watermark:** `WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND`\n",
    "  enables correct out-of-order handling (no-op in batch mode, essential in streaming)\n",
    "- **Bounded consumption:** `scan.bounded.mode = latest-offset` reads everything available then stops"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/00-init.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Flink SQL - Session Initialization\n",
    "-- =============================================================================\n",
    "-- Creates the Kafka source table and Iceberg catalog. This file is used as\n",
    "-- an init script (-i flag) for all subsequent SQL files so they have access\n",
    "-- to the catalog within the same session.\n",
    "--\n",
    "-- Uses BATCH execution mode so jobs process all available data and terminate.\n",
    "-- =============================================================================\n",
    "\n",
    "-- Use batch mode (process available data, then stop)\n",
    "SET 'execution.runtime-mode' = 'batch';\n",
    "\n",
    "-- Wait for each INSERT to complete before proceeding to next statement\n",
    "SET 'table.dml-sync' = 'true';\n",
    "\n",
    "-- Create Kafka source table\n",
    "-- NOTE: event_time computed column + WATERMARK enables event-time processing\n",
    "-- in streaming mode. In batch mode (default), the watermark is simply ignored.\n",
    "CREATE TABLE IF NOT EXISTS kafka_raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    STRING,\n",
    "    tpep_dropoff_datetime   STRING,\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    -- Computed column for event-time processing\n",
    "    event_time AS TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss'),\n",
    "    -- Watermark: allow 10s late arrivals (ignored in batch mode)\n",
    "    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'taxi.raw_trips',\n",
    "    'properties.bootstrap.servers' = 'kafka:9092',\n",
    "    'properties.group.id' = 'flink-consumer',\n",
    "    'scan.startup.mode' = 'earliest-offset',\n",
    "    'scan.bounded.mode' = 'latest-offset',\n",
    "    'format' = 'json'\n",
    ");\n",
    "\n",
    "-- Create Iceberg catalog backed by MinIO\n",
    "CREATE CATALOG iceberg_catalog WITH (\n",
    "    'type' = 'iceberg',\n",
    "    'catalog-type' = 'hadoop',\n",
    "    'warehouse' = 's3a://warehouse/',\n",
    "    'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO',\n",
    "    's3.endpoint' = 'http://minio:9000',\n",
    "    's3.access-key-id' = 'minioadmin',\n",
    "    's3.secret-access-key' = 'minioadmin',\n",
    "    's3.path-style-access' = 'true'\n",
    ");\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. REST Catalog Session Init (Lakekeeper Alternative)",
    "",
    "> **When to use:** If you started with `make up-lakekeeper`, use this init file instead of `00-init.sql`.",
    "> The REST catalog eliminates hardcoded S3 credentials in SQL — Lakekeeper handles credential vending.",
    "",
    "```bash",
    "# Usage with REST catalog:",
    "sql-client.sh embedded -i 00-init-rest.sql -f 05-bronze.sql",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/00-init-rest.sql",
    "-- =============================================================================",
    "-- Pipeline 01: Flink SQL - Session Initialization (REST Catalog via Lakekeeper)",
    "-- =============================================================================",
    "-- Alternative to 00-init.sql that uses Lakekeeper REST catalog instead of",
    "-- Hadoop catalog. Requires: docker compose --profile lakekeeper up -d",
    "--",
    "-- Usage:",
    "--   sql-client.sh embedded -i 00-init-rest.sql -f 05-bronze.sql",
    "--   sql-client.sh embedded -i 00-init-rest.sql -f 06-silver.sql",
    "-- =============================================================================",
    "",
    "-- Use batch mode (process available data, then stop)",
    "SET 'execution.runtime-mode' = 'batch';",
    "",
    "-- Wait for each INSERT to complete before proceeding to next statement",
    "SET 'table.dml-sync' = 'true';",
    "",
    "-- Create Kafka source table",
    "-- NOTE: event_time computed column + WATERMARK enables event-time processing",
    "-- in streaming mode. In batch mode (default), the watermark is simply ignored.",
    "CREATE TABLE IF NOT EXISTS kafka_raw_trips (",
    "    VendorID                BIGINT,",
    "    tpep_pickup_datetime    STRING,",
    "    tpep_dropoff_datetime   STRING,",
    "    passenger_count         BIGINT,",
    "    trip_distance           DOUBLE,",
    "    RatecodeID              BIGINT,",
    "    store_and_fwd_flag      STRING,",
    "    PULocationID            BIGINT,",
    "    DOLocationID            BIGINT,",
    "    payment_type            BIGINT,",
    "    fare_amount             DOUBLE,",
    "    extra                   DOUBLE,",
    "    mta_tax                 DOUBLE,",
    "    tip_amount              DOUBLE,",
    "    tolls_amount            DOUBLE,",
    "    improvement_surcharge   DOUBLE,",
    "    total_amount            DOUBLE,",
    "    congestion_surcharge    DOUBLE,",
    "    Airport_fee             DOUBLE,",
    "    -- Computed column for event-time processing",
    "    event_time AS TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss'),",
    "    -- Watermark: allow 10s late arrivals (ignored in batch mode)",
    "    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND",
    ") WITH (",
    "    'connector' = 'kafka',",
    "    'topic' = 'taxi.raw_trips',",
    "    'properties.bootstrap.servers' = 'kafka:9092',",
    "    'properties.group.id' = 'flink-consumer',",
    "    'scan.startup.mode' = 'earliest-offset',",
    "    'scan.bounded.mode' = 'latest-offset',",
    "    'format' = 'json'",
    ");",
    "",
    "-- Create Iceberg catalog via Lakekeeper REST API",
    "-- No S3 credentials needed here - Lakekeeper handles credential vending",
    "CREATE CATALOG iceberg_catalog WITH (",
    "    'type' = 'iceberg',",
    "    'catalog-type' = 'rest',",
    "    'uri' = 'http://lakekeeper:8181/catalog',",
    "    'warehouse' = 'warehouse'",
    ");",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Kafka Source Table (Reference)\n",
    "\n",
    "This is the standalone version of the Kafka table definition. Included in `00-init.sql` but useful as documentation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/01-create-kafka-source.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Flink SQL - Kafka Source Table\n",
    "-- =============================================================================\n",
    "-- Creates a Flink SQL table backed by the Kafka topic taxi.raw_trips.\n",
    "-- The data generator produces JSON records with these exact field names\n",
    "-- matching the NYC Yellow Taxi parquet schema.\n",
    "-- =============================================================================\n",
    "\n",
    "CREATE TABLE kafka_raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    STRING,\n",
    "    tpep_dropoff_datetime   STRING,\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'taxi.raw_trips',\n",
    "    'properties.bootstrap.servers' = 'kafka:9092',\n",
    "    'properties.group.id' = 'flink-consumer',\n",
    "    'scan.startup.mode' = 'earliest-offset',\n",
    "    'format' = 'json'\n",
    ");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Iceberg Catalog (Reference)\n",
    "\n",
    "Standalone catalog creation. Also included in `00-init.sql`.\n",
    "\n",
    "Key properties:\n",
    "- `catalog-type: hadoop` → Uses filesystem-based catalog metadata\n",
    "- `warehouse: s3a://warehouse/` → MinIO bucket for all Iceberg data\n",
    "- `io-impl: S3FileIO` → Iceberg's own S3 implementation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/02-create-iceberg-catalog.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Flink SQL - Iceberg Catalog\n",
    "-- =============================================================================\n",
    "-- Creates a Hadoop-based Iceberg catalog backed by MinIO (S3-compatible).\n",
    "-- All Bronze and Silver tables will be created within this catalog.\n",
    "-- =============================================================================\n",
    "\n",
    "CREATE CATALOG iceberg_catalog WITH (\n",
    "    'type' = 'iceberg',\n",
    "    'catalog-type' = 'hadoop',\n",
    "    'warehouse' = 's3a://warehouse/',\n",
    "    'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO',\n",
    "    's3.endpoint' = 'http://minio:9000',\n",
    "    's3.access-key-id' = 'minioadmin',\n",
    "    's3.secret-access-key' = 'minioadmin',\n",
    "    's3.path-style-access' = 'true'\n",
    ");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Flink SQL: Bronze Layer (Kafka → Iceberg)\n",
    "\n",
    "### What Bronze Does\n",
    "- Preserves **all** original fields from Kafka events\n",
    "- Parses ISO 8601 timestamp strings → `TIMESTAMP(3)` type\n",
    "- Adds `ingestion_ts` metadata column (when the event was processed)\n",
    "- **No filtering**, no validation, no business logic\n",
    "- Writes ACID Iceberg tables to MinIO (`s3a://warehouse/bronze/raw_trips/`)\n",
    "\n",
    "### 7.1 Bronze with Documentation (03-bronze-raw-trips.sql)\n",
    "\n",
    "The verbose version with inline comments explaining each decision:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/03-bronze-raw-trips.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Flink SQL - Bronze Layer (Raw Trips)\n",
    "-- =============================================================================\n",
    "-- Creates the Bronze Iceberg table and starts a continuous INSERT job\n",
    "-- that reads from the Kafka source table.\n",
    "--\n",
    "-- Bronze layer preserves original column names from the source data.\n",
    "-- Timestamps are parsed from ISO 8601 strings to TIMESTAMP type.\n",
    "-- No filtering or cleaning is applied at this layer.\n",
    "-- =============================================================================\n",
    "\n",
    "-- Use the Iceberg catalog\n",
    "USE CATALOG iceberg_catalog;\n",
    "\n",
    "-- Create the Bronze database\n",
    "CREATE DATABASE IF NOT EXISTS bronze;\n",
    "USE bronze;\n",
    "\n",
    "-- Create the Bronze raw trips table\n",
    "CREATE TABLE IF NOT EXISTS raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    TIMESTAMP(3),\n",
    "    tpep_dropoff_datetime   TIMESTAMP(3),\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    ingestion_ts            TIMESTAMP(3)\n",
    ");\n",
    "\n",
    "-- Switch back to default catalog for the Kafka source table reference\n",
    "USE CATALOG default_catalog;\n",
    "USE default_database;\n",
    "\n",
    "-- Continuous INSERT from Kafka into Bronze Iceberg table\n",
    "-- Timestamps are parsed from ISO 8601 string format (e.g. \"2024-01-15T08:30:00\")\n",
    "INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "SELECT\n",
    "    VendorID,\n",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,\n",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    RatecodeID,\n",
    "    store_and_fwd_flag,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    payment_type,\n",
    "    fare_amount,\n",
    "    extra,\n",
    "    mta_tax,\n",
    "    tip_amount,\n",
    "    tolls_amount,\n",
    "    improvement_surcharge,\n",
    "    total_amount,\n",
    "    congestion_surcharge,\n",
    "    Airport_fee,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_raw_trips;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Bronze Standalone (05-bronze.sql)\n",
    "\n",
    "The production version used by `make process-bronze`. Identical logic, minimal comments.\n",
    "Run with: `sql-client.sh embedded -i 00-init.sql -f 05-bronze.sql`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/05-bronze.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Bronze Layer (Kafka → Iceberg)\n",
    "-- =============================================================================\n",
    "-- Run: sql-client.sh embedded -i 00-init.sql -f 05-bronze.sql\n",
    "-- =============================================================================\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    TIMESTAMP(3),\n",
    "    tpep_dropoff_datetime   TIMESTAMP(3),\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    ingestion_ts            TIMESTAMP(3)\n",
    ");\n",
    "\n",
    "USE CATALOG default_catalog;\n",
    "USE default_database;\n",
    "\n",
    "INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "SELECT\n",
    "    VendorID,\n",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,\n",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    RatecodeID,\n",
    "    store_and_fwd_flag,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    payment_type,\n",
    "    fare_amount,\n",
    "    extra,\n",
    "    mta_tax,\n",
    "    tip_amount,\n",
    "    tolls_amount,\n",
    "    improvement_surcharge,\n",
    "    total_amount,\n",
    "    congestion_surcharge,\n",
    "    Airport_fee,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_raw_trips;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Flink SQL: Silver Layer (Bronze → Cleaned Iceberg)\n",
    "\n",
    "The Silver layer applies data quality rules and deduplication:\n",
    "\n",
    "1. **ROW_NUMBER dedup:** `PARTITION BY natural_key ORDER BY ingestion_ts DESC` → keeps latest record\n",
    "2. **Null filtering:** Removes rows with null passenger_count or trip_distance\n",
    "3. **Negative filtering:** Removes negative fares, tips, tolls, totals\n",
    "4. **Date range:** Only January 2024 data passes through\n",
    "5. **Computed columns:** duration_minutes, avg_speed_mph, cost_per_mile, tip_percentage\n",
    "6. **Date dimensions:** pickup_date, pickup_hour, is_weekend\n",
    "7. **Surrogate key:** MD5 hash trip_id for downstream joins"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/04-silver-cleaned-trips.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Flink SQL - Silver Layer (Cleaned Trips)\n",
    "-- =============================================================================\n",
    "-- Creates the Silver Iceberg table and starts a continuous INSERT job\n",
    "-- that reads from the Bronze table, applies data quality filters,\n",
    "-- renames columns to snake_case, and computes enrichment columns.\n",
    "--\n",
    "-- Silver layer transformations:\n",
    "--   1. Column renaming (VendorID -> vendor_id, PULocationID -> pickup_location_id, etc.)\n",
    "--   2. Type casting (BIGINT -> INT where appropriate)\n",
    "--   3. Data quality filters:\n",
    "--      - Reject null timestamps\n",
    "--      - Reject negative fare amounts and trip distances\n",
    "--      - Reject pickup dates outside January 2024\n",
    "--   4. Surrogate key: MD5 hash of composite natural key\n",
    "--   5. Computed columns:\n",
    "--      - duration_minutes\n",
    "--      - avg_speed_mph\n",
    "--      - cost_per_mile\n",
    "--      - tip_percentage\n",
    "--      - pickup_date, pickup_hour\n",
    "--      - is_weekend\n",
    "-- =============================================================================\n",
    "\n",
    "-- Use the Iceberg catalog\n",
    "USE CATALOG iceberg_catalog;\n",
    "\n",
    "-- Create the Silver database\n",
    "CREATE DATABASE IF NOT EXISTS silver;\n",
    "USE silver;\n",
    "\n",
    "-- Create the Silver cleaned trips table\n",
    "CREATE TABLE IF NOT EXISTS cleaned_trips (\n",
    "    -- surrogate key\n",
    "    trip_id                 STRING,\n",
    "\n",
    "    -- identifiers\n",
    "    vendor_id               INT,\n",
    "    rate_code_id            INT,\n",
    "    pickup_location_id      INT,\n",
    "    dropoff_location_id     INT,\n",
    "    payment_type_id         INT,\n",
    "\n",
    "    -- timestamps\n",
    "    pickup_datetime         TIMESTAMP(3),\n",
    "    dropoff_datetime        TIMESTAMP(3),\n",
    "\n",
    "    -- trip info\n",
    "    passenger_count         INT,\n",
    "    trip_distance_miles     DOUBLE,\n",
    "    store_and_fwd_flag      STRING,\n",
    "\n",
    "    -- financials\n",
    "    fare_amount             DECIMAL(10, 2),\n",
    "    extra_amount            DECIMAL(10, 2),\n",
    "    mta_tax                 DECIMAL(10, 2),\n",
    "    tip_amount              DECIMAL(10, 2),\n",
    "    tolls_amount            DECIMAL(10, 2),\n",
    "    improvement_surcharge   DECIMAL(10, 2),\n",
    "    total_amount            DECIMAL(10, 2),\n",
    "    congestion_surcharge    DECIMAL(10, 2),\n",
    "    airport_fee             DECIMAL(10, 2),\n",
    "\n",
    "    -- computed: enrichments\n",
    "    duration_minutes        BIGINT,\n",
    "    avg_speed_mph           DOUBLE,\n",
    "    cost_per_mile           DOUBLE,\n",
    "    tip_percentage          DOUBLE,\n",
    "\n",
    "    -- computed: time dimensions\n",
    "    pickup_date             DATE,\n",
    "    pickup_hour             INT,\n",
    "    is_weekend              BOOLEAN\n",
    ");\n",
    "\n",
    "-- Continuous INSERT from Bronze into Silver with transformations\n",
    "INSERT INTO iceberg_catalog.silver.cleaned_trips\n",
    "SELECT\n",
    "    -- Surrogate key: MD5 hash of composite natural key\n",
    "    MD5(CONCAT_WS('|',\n",
    "        CAST(VendorID AS STRING),\n",
    "        CAST(tpep_pickup_datetime AS STRING),\n",
    "        CAST(tpep_dropoff_datetime AS STRING),\n",
    "        CAST(PULocationID AS STRING),\n",
    "        CAST(DOLocationID AS STRING),\n",
    "        CAST(fare_amount AS STRING),\n",
    "        CAST(total_amount AS STRING)\n",
    "    )) AS trip_id,\n",
    "\n",
    "    -- Identifiers (renamed + cast)\n",
    "    CAST(VendorID AS INT)       AS vendor_id,\n",
    "    CAST(RatecodeID AS INT)     AS rate_code_id,\n",
    "    CAST(PULocationID AS INT)   AS pickup_location_id,\n",
    "    CAST(DOLocationID AS INT)   AS dropoff_location_id,\n",
    "    CAST(payment_type AS INT)   AS payment_type_id,\n",
    "\n",
    "    -- Timestamps\n",
    "    tpep_pickup_datetime        AS pickup_datetime,\n",
    "    tpep_dropoff_datetime       AS dropoff_datetime,\n",
    "\n",
    "    -- Trip info\n",
    "    CAST(passenger_count AS INT) AS passenger_count,\n",
    "    trip_distance               AS trip_distance_miles,\n",
    "    store_and_fwd_flag,\n",
    "\n",
    "    -- Financials (rounded to 2 decimal places)\n",
    "    CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,\n",
    "    CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,\n",
    "    CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,\n",
    "    CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,\n",
    "    CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,\n",
    "    CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,\n",
    "    CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,\n",
    "    CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,\n",
    "    CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,\n",
    "\n",
    "    -- Computed: duration in minutes\n",
    "    TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) AS duration_minutes,\n",
    "\n",
    "    -- Computed: average speed in mph (avoid division by zero)\n",
    "    CASE\n",
    "        WHEN TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) > 0\n",
    "        THEN ROUND(\n",
    "            trip_distance / (CAST(TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) AS DOUBLE) / 60.0),\n",
    "            2\n",
    "        )\n",
    "        ELSE NULL\n",
    "    END AS avg_speed_mph,\n",
    "\n",
    "    -- Computed: cost per mile (avoid division by zero)\n",
    "    CASE\n",
    "        WHEN trip_distance > 0\n",
    "        THEN ROUND(fare_amount / trip_distance, 2)\n",
    "        ELSE NULL\n",
    "    END AS cost_per_mile,\n",
    "\n",
    "    -- Computed: tip percentage (avoid division by zero)\n",
    "    CASE\n",
    "        WHEN fare_amount > 0\n",
    "        THEN ROUND((tip_amount / fare_amount) * 100, 2)\n",
    "        ELSE NULL\n",
    "    END AS tip_percentage,\n",
    "\n",
    "    -- Computed: date dimensions\n",
    "    CAST(tpep_pickup_datetime AS DATE) AS pickup_date,\n",
    "    EXTRACT(HOUR FROM tpep_pickup_datetime) AS pickup_hour,\n",
    "    CASE\n",
    "        WHEN DAYOFWEEK(tpep_pickup_datetime) IN (1, 7) THEN TRUE\n",
    "        ELSE FALSE\n",
    "    END AS is_weekend\n",
    "\n",
    "FROM iceberg_catalog.bronze.raw_trips\n",
    "\n",
    "-- Data quality filters\n",
    "WHERE tpep_pickup_datetime IS NOT NULL\n",
    "  AND tpep_dropoff_datetime IS NOT NULL\n",
    "  AND trip_distance >= 0\n",
    "  AND fare_amount >= 0\n",
    "  AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'\n",
    "  AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01';"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Silver Standalone (06-silver.sql)\n",
    "\n",
    "Production version used by `make process-silver`.\n",
    "Run with: `sql-client.sh embedded -i 00-init.sql -f 06-silver.sql`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/06-silver.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Silver Layer (Bronze Iceberg → Silver Iceberg)\n",
    "-- =============================================================================\n",
    "-- Run: sql-client.sh embedded -i 00-init.sql -f 06-silver.sql\n",
    "-- =============================================================================\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS silver;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS silver.cleaned_trips (\n",
    "    trip_id                 STRING,\n",
    "    vendor_id               INT,\n",
    "    rate_code_id            INT,\n",
    "    pickup_location_id      INT,\n",
    "    dropoff_location_id     INT,\n",
    "    payment_type_id         INT,\n",
    "    pickup_datetime         TIMESTAMP(3),\n",
    "    dropoff_datetime        TIMESTAMP(3),\n",
    "    passenger_count         INT,\n",
    "    trip_distance_miles     DOUBLE,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    fare_amount             DECIMAL(10, 2),\n",
    "    extra_amount            DECIMAL(10, 2),\n",
    "    mta_tax                 DECIMAL(10, 2),\n",
    "    tip_amount              DECIMAL(10, 2),\n",
    "    tolls_amount            DECIMAL(10, 2),\n",
    "    improvement_surcharge   DECIMAL(10, 2),\n",
    "    total_amount            DECIMAL(10, 2),\n",
    "    congestion_surcharge    DECIMAL(10, 2),\n",
    "    airport_fee             DECIMAL(10, 2),\n",
    "    duration_minutes        BIGINT,\n",
    "    avg_speed_mph           DOUBLE,\n",
    "    cost_per_mile           DOUBLE,\n",
    "    tip_percentage          DOUBLE,\n",
    "    pickup_date             DATE,\n",
    "    pickup_hour             INT,\n",
    "    is_weekend              BOOLEAN\n",
    ");\n",
    "\n",
    "-- Deduplication: ROW_NUMBER partitioned by natural key, keeping latest ingestion\n",
    "INSERT INTO iceberg_catalog.silver.cleaned_trips\n",
    "WITH deduped AS (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,\n",
    "                         PULocationID, DOLocationID, fare_amount, total_amount\n",
    "            ORDER BY ingestion_ts DESC\n",
    "        ) AS rn\n",
    "    FROM iceberg_catalog.bronze.raw_trips\n",
    "    WHERE tpep_pickup_datetime IS NOT NULL\n",
    "      AND tpep_dropoff_datetime IS NOT NULL\n",
    "      AND trip_distance >= 0\n",
    "      AND fare_amount >= 0\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01'\n",
    ")\n",
    "SELECT\n",
    "    CAST(MD5(CONCAT_WS('|',\n",
    "        CAST(VendorID AS STRING),\n",
    "        CAST(tpep_pickup_datetime AS STRING),\n",
    "        CAST(tpep_dropoff_datetime AS STRING),\n",
    "        CAST(PULocationID AS STRING),\n",
    "        CAST(DOLocationID AS STRING),\n",
    "        CAST(fare_amount AS STRING),\n",
    "        CAST(total_amount AS STRING)\n",
    "    )) AS STRING) AS trip_id,\n",
    "    CAST(VendorID AS INT)       AS vendor_id,\n",
    "    CAST(RatecodeID AS INT)     AS rate_code_id,\n",
    "    CAST(PULocationID AS INT)   AS pickup_location_id,\n",
    "    CAST(DOLocationID AS INT)   AS dropoff_location_id,\n",
    "    CAST(payment_type AS INT)   AS payment_type_id,\n",
    "    tpep_pickup_datetime        AS pickup_datetime,\n",
    "    tpep_dropoff_datetime       AS dropoff_datetime,\n",
    "    CAST(passenger_count AS INT) AS passenger_count,\n",
    "    trip_distance               AS trip_distance_miles,\n",
    "    store_and_fwd_flag,\n",
    "    CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,\n",
    "    CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,\n",
    "    CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,\n",
    "    CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,\n",
    "    CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,\n",
    "    CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,\n",
    "    CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,\n",
    "    CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,\n",
    "    CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,\n",
    "    CAST(TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) AS BIGINT) AS duration_minutes,\n",
    "    CASE\n",
    "        WHEN TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) > 0\n",
    "        THEN ROUND(\n",
    "            trip_distance / (CAST(TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) AS DOUBLE) / 60.0),\n",
    "            2\n",
    "        )\n",
    "        ELSE NULL\n",
    "    END AS avg_speed_mph,\n",
    "    CASE\n",
    "        WHEN trip_distance > 0\n",
    "        THEN ROUND(fare_amount / trip_distance, 2)\n",
    "        ELSE NULL\n",
    "    END AS cost_per_mile,\n",
    "    CASE\n",
    "        WHEN fare_amount > 0\n",
    "        THEN ROUND((tip_amount / fare_amount) * 100, 2)\n",
    "        ELSE NULL\n",
    "    END AS tip_percentage,\n",
    "    CAST(tpep_pickup_datetime AS DATE) AS pickup_date,\n",
    "    CAST(EXTRACT(HOUR FROM tpep_pickup_datetime) AS INT) AS pickup_hour,\n",
    "    CASE\n",
    "        WHEN DAYOFWEEK(tpep_pickup_datetime) IN (1, 7) THEN TRUE\n",
    "        ELSE FALSE\n",
    "    END AS is_weekend\n",
    "FROM deduped\n",
    "WHERE rn = 1;\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Flink SQL: Combined Bronze + Silver Pipeline\n",
    "\n",
    "This single file runs both layers sequentially. Useful for understanding the\n",
    "complete Flink processing flow in one place.\n",
    "\n",
    "Run with: `sql-client.sh embedded -i 00-init.sql -f 05-run-all.sql`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/05-run-all.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Flink SQL - Full Pipeline (Bronze + Silver)\n",
    "-- =============================================================================\n",
    "-- Run with init: sql-client.sh embedded -i 00-init.sql -f 05-run-all.sql\n",
    "-- =============================================================================\n",
    "\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "-- BRONZE LAYER: Raw data from Kafka → Iceberg\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    TIMESTAMP(3),\n",
    "    tpep_dropoff_datetime   TIMESTAMP(3),\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    ingestion_ts            TIMESTAMP(3)\n",
    ");\n",
    "\n",
    "-- Switch back to default catalog for Kafka source table reference\n",
    "USE CATALOG default_catalog;\n",
    "USE default_database;\n",
    "\n",
    "-- Insert from Kafka into Bronze Iceberg table\n",
    "INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "SELECT\n",
    "    VendorID,\n",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,\n",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    RatecodeID,\n",
    "    store_and_fwd_flag,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    payment_type,\n",
    "    fare_amount,\n",
    "    extra,\n",
    "    mta_tax,\n",
    "    tip_amount,\n",
    "    tolls_amount,\n",
    "    improvement_surcharge,\n",
    "    total_amount,\n",
    "    congestion_surcharge,\n",
    "    Airport_fee,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_raw_trips;\n",
    "\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "-- SILVER LAYER: Cleaned + enriched data from Bronze → Silver\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS silver;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS silver.cleaned_trips (\n",
    "    trip_id                 STRING,\n",
    "    vendor_id               INT,\n",
    "    rate_code_id            INT,\n",
    "    pickup_location_id      INT,\n",
    "    dropoff_location_id     INT,\n",
    "    payment_type_id         INT,\n",
    "    pickup_datetime         TIMESTAMP(3),\n",
    "    dropoff_datetime        TIMESTAMP(3),\n",
    "    passenger_count         INT,\n",
    "    trip_distance_miles     DOUBLE,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    fare_amount             DECIMAL(10, 2),\n",
    "    extra_amount            DECIMAL(10, 2),\n",
    "    mta_tax                 DECIMAL(10, 2),\n",
    "    tip_amount              DECIMAL(10, 2),\n",
    "    tolls_amount            DECIMAL(10, 2),\n",
    "    improvement_surcharge   DECIMAL(10, 2),\n",
    "    total_amount            DECIMAL(10, 2),\n",
    "    congestion_surcharge    DECIMAL(10, 2),\n",
    "    airport_fee             DECIMAL(10, 2),\n",
    "    duration_minutes        BIGINT,\n",
    "    avg_speed_mph           DOUBLE,\n",
    "    cost_per_mile           DOUBLE,\n",
    "    tip_percentage          DOUBLE,\n",
    "    pickup_date             DATE,\n",
    "    pickup_hour             INT,\n",
    "    is_weekend              BOOLEAN\n",
    ");\n",
    "\n",
    "-- Deduplication: ROW_NUMBER partitioned by natural key, keeping latest ingestion\n",
    "INSERT INTO iceberg_catalog.silver.cleaned_trips\n",
    "WITH deduped AS (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,\n",
    "                         PULocationID, DOLocationID, fare_amount, total_amount\n",
    "            ORDER BY ingestion_ts DESC\n",
    "        ) AS rn\n",
    "    FROM iceberg_catalog.bronze.raw_trips\n",
    "    WHERE tpep_pickup_datetime IS NOT NULL\n",
    "      AND tpep_dropoff_datetime IS NOT NULL\n",
    "      AND trip_distance >= 0\n",
    "      AND fare_amount >= 0\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01'\n",
    ")\n",
    "SELECT\n",
    "    CAST(MD5(CONCAT_WS('|',\n",
    "        CAST(VendorID AS STRING),\n",
    "        CAST(tpep_pickup_datetime AS STRING),\n",
    "        CAST(tpep_dropoff_datetime AS STRING),\n",
    "        CAST(PULocationID AS STRING),\n",
    "        CAST(DOLocationID AS STRING),\n",
    "        CAST(fare_amount AS STRING),\n",
    "        CAST(total_amount AS STRING)\n",
    "    )) AS STRING) AS trip_id,\n",
    "    CAST(VendorID AS INT)       AS vendor_id,\n",
    "    CAST(RatecodeID AS INT)     AS rate_code_id,\n",
    "    CAST(PULocationID AS INT)   AS pickup_location_id,\n",
    "    CAST(DOLocationID AS INT)   AS dropoff_location_id,\n",
    "    CAST(payment_type AS INT)   AS payment_type_id,\n",
    "    tpep_pickup_datetime        AS pickup_datetime,\n",
    "    tpep_dropoff_datetime       AS dropoff_datetime,\n",
    "    CAST(passenger_count AS INT) AS passenger_count,\n",
    "    trip_distance               AS trip_distance_miles,\n",
    "    store_and_fwd_flag,\n",
    "    CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,\n",
    "    CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,\n",
    "    CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,\n",
    "    CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,\n",
    "    CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,\n",
    "    CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,\n",
    "    CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,\n",
    "    CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,\n",
    "    CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,\n",
    "    CAST(TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) AS BIGINT) AS duration_minutes,\n",
    "    CASE\n",
    "        WHEN TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) > 0\n",
    "        THEN ROUND(\n",
    "            trip_distance / (CAST(TIMESTAMPDIFF(MINUTE, tpep_pickup_datetime, tpep_dropoff_datetime) AS DOUBLE) / 60.0),\n",
    "            2\n",
    "        )\n",
    "        ELSE NULL\n",
    "    END AS avg_speed_mph,\n",
    "    CASE\n",
    "        WHEN trip_distance > 0\n",
    "        THEN ROUND(fare_amount / trip_distance, 2)\n",
    "        ELSE NULL\n",
    "    END AS cost_per_mile,\n",
    "    CASE\n",
    "        WHEN fare_amount > 0\n",
    "        THEN ROUND((tip_amount / fare_amount) * 100, 2)\n",
    "        ELSE NULL\n",
    "    END AS tip_percentage,\n",
    "    CAST(tpep_pickup_datetime AS DATE) AS pickup_date,\n",
    "    CAST(EXTRACT(HOUR FROM tpep_pickup_datetime) AS INT) AS pickup_hour,\n",
    "    CASE\n",
    "        WHEN DAYOFWEEK(tpep_pickup_datetime) IN (1, 7) THEN TRUE\n",
    "        ELSE FALSE\n",
    "    END AS is_weekend\n",
    "FROM deduped\n",
    "WHERE rn = 1;\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. dbt Project Configuration\n",
    "\n",
    "dbt (data build tool) handles the **Silver → Gold** transformation layer.\n",
    "It reads Iceberg Silver tables via DuckDB's `iceberg_scan()` function and\n",
    "builds dimensional models (facts, dimensions, analytics marts).\n",
    "\n",
    "### 10.1 Project Config (dbt_project.yml)\n",
    "\n",
    "Defines project structure, materialization strategies, and seed column types.\n",
    "\n",
    "Key patterns:\n",
    "- `stg_yellow_trips` is materialized as `table` (not view) because it reads from Iceberg via DuckDB\n",
    "- Intermediate models are `view` (lightweight, computed on-the-fly)\n",
    "- Marts are `table` (materialized for query performance)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/dbt_project.yml\n",
    "name: 'nyc_taxi_dbt'\n",
    "version: '1.0.0'\n",
    "config-version: 2\n",
    "\n",
    "profile: 'nyc_taxi_dbt'\n",
    "\n",
    "model-paths: [\"models\"]\n",
    "analysis-paths: [\"analyses\"]\n",
    "test-paths: [\"tests\"]\n",
    "seed-paths: [\"seeds\"]\n",
    "macro-paths: [\"macros\"]\n",
    "snapshot-paths: [\"snapshots\"]\n",
    "\n",
    "clean-targets:\n",
    "  - \"target\"\n",
    "  - \"dbt_packages\"\n",
    "\n",
    "seeds:\n",
    "  nyc_taxi_dbt:\n",
    "    +schema: raw\n",
    "    taxi_zone_lookup:\n",
    "      +column_types:\n",
    "        LocationID: INTEGER\n",
    "        Borough: VARCHAR\n",
    "        Zone: VARCHAR\n",
    "        service_zone: VARCHAR\n",
    "    payment_type_lookup:\n",
    "      +column_types:\n",
    "        payment_type_id: INTEGER\n",
    "        payment_type_name: VARCHAR\n",
    "    rate_code_lookup:\n",
    "      +column_types:\n",
    "        rate_code_id: INTEGER\n",
    "        rate_code_name: VARCHAR\n",
    "\n",
    "models:\n",
    "  nyc_taxi_dbt:\n",
    "    +materialized: view\n",
    "    staging:\n",
    "      +materialized: view\n",
    "      +schema: staging\n",
    "      stg_yellow_trips:\n",
    "        +materialized: table\n",
    "    intermediate:\n",
    "      +materialized: view\n",
    "      +schema: intermediate\n",
    "    marts:\n",
    "      core:\n",
    "        +materialized: table\n",
    "        +schema: marts\n",
    "      analytics:\n",
    "        +materialized: table\n",
    "        +schema: marts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Streaming Mode Alternative (`07-streaming-bronze.sql`)",
    "",
    "> **Batch vs Streaming:** The default `05-bronze.sql` uses batch mode (process available data, stop).",
    "> This file uses **streaming mode** — it runs continuously, processing events as they arrive in Kafka.",
    "> Same SQL, same tables, same catalog — only the runtime mode changes.",
    "",
    "When to use:",
    "- **Batch mode** (`05-bronze.sql`): Catch-up processing, backfill, benchmarking",
    "- **Streaming mode** (`07-streaming-bronze.sql`): Continuous real-time processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/flink/sql/07-streaming-bronze.sql",
    "-- =============================================================================",
    "-- Pipeline 01: Streaming Bronze Layer (Kafka → Iceberg, continuous)",
    "-- =============================================================================",
    "-- Alternative to 05-bronze.sql that runs in STREAMING mode.",
    "-- Uses event_time watermarks defined in 00-init.sql for event-time processing.",
    "--",
    "-- Run: sql-client.sh embedded -i 00-init-streaming.sql -f 07-streaming-bronze.sql",
    "--   (or override execution.runtime-mode inline)",
    "--",
    "-- NOTE: This job runs continuously until cancelled. It will process new Kafka",
    "-- events as they arrive and write them to the Bronze Iceberg table.",
    "-- =============================================================================",
    "",
    "-- Override to streaming mode (00-init.sql sets batch by default)",
    "SET 'execution.runtime-mode' = 'streaming';",
    "",
    "-- Don't wait for each INSERT to complete (streaming jobs run indefinitely)",
    "RESET 'table.dml-sync';",
    "",
    "-- Checkpoint every 30s for exactly-once guarantees",
    "SET 'execution.checkpointing.interval' = '30s';",
    "",
    "USE CATALOG iceberg_catalog;",
    "CREATE DATABASE IF NOT EXISTS bronze;",
    "",
    "CREATE TABLE IF NOT EXISTS bronze.raw_trips (",
    "    VendorID                BIGINT,",
    "    tpep_pickup_datetime    TIMESTAMP(3),",
    "    tpep_dropoff_datetime   TIMESTAMP(3),",
    "    passenger_count         BIGINT,",
    "    trip_distance           DOUBLE,",
    "    RatecodeID              BIGINT,",
    "    store_and_fwd_flag      STRING,",
    "    PULocationID            BIGINT,",
    "    DOLocationID            BIGINT,",
    "    payment_type            BIGINT,",
    "    fare_amount             DOUBLE,",
    "    extra                   DOUBLE,",
    "    mta_tax                 DOUBLE,",
    "    tip_amount              DOUBLE,",
    "    tolls_amount            DOUBLE,",
    "    improvement_surcharge   DOUBLE,",
    "    total_amount            DOUBLE,",
    "    congestion_surcharge    DOUBLE,",
    "    Airport_fee             DOUBLE,",
    "    ingestion_ts            TIMESTAMP(3)",
    ");",
    "",
    "-- Switch back to default catalog for Kafka source table reference",
    "USE CATALOG default_catalog;",
    "USE default_database;",
    "",
    "-- Streaming INSERT: runs continuously, processing new Kafka events as they arrive",
    "INSERT INTO iceberg_catalog.bronze.raw_trips",
    "SELECT",
    "    VendorID,",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,",
    "    passenger_count,",
    "    trip_distance,",
    "    RatecodeID,",
    "    store_and_fwd_flag,",
    "    PULocationID,",
    "    DOLocationID,",
    "    payment_type,",
    "    fare_amount,",
    "    extra,",
    "    mta_tax,",
    "    tip_amount,",
    "    tolls_amount,",
    "    improvement_surcharge,",
    "    total_amount,",
    "    congestion_surcharge,",
    "    Airport_fee,",
    "    CURRENT_TIMESTAMP AS ingestion_ts",
    "FROM kafka_raw_trips;",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Connection Profile (profiles.yml)\n",
    "\n",
    "Connects dbt to DuckDB with Iceberg + S3 (MinIO) extensions.\n",
    "\n",
    "Key settings:\n",
    "- `extensions: [httpfs, parquet, iceberg]` → DuckDB can read Iceberg tables over S3\n",
    "- `s3_endpoint: minio:9000` → Points to MinIO container\n",
    "- `s3_url_style: path` → Required for MinIO (vs virtual-hosted for real S3)\n",
    "- `memory_limit: 2GB` → DuckDB in-process memory"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/profiles.yml\n",
    "# =============================================================================\n",
    "# dbt Profile for Pipeline 01: Kafka + Flink + Iceberg\n",
    "# =============================================================================\n",
    "# Uses DuckDB with iceberg and httpfs extensions to read Iceberg tables\n",
    "# from MinIO (S3-compatible) object storage.\n",
    "# =============================================================================\n",
    "\n",
    "nyc_taxi_dbt:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: duckdb\n",
    "      path: /tmp/p01_dbt.duckdb\n",
    "      schema: main\n",
    "      threads: 4\n",
    "      extensions:\n",
    "        - httpfs\n",
    "        - parquet\n",
    "        - iceberg\n",
    "      settings:\n",
    "        memory_limit: \"2GB\"\n",
    "        s3_endpoint: \"minio:9000\"\n",
    "        s3_access_key_id: \"minioadmin\"\n",
    "        s3_secret_access_key: \"minioadmin\"\n",
    "        s3_url_style: \"path\"\n",
    "        s3_use_ssl: false\n",
    "        s3_region: \"us-east-1\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Package Dependencies (packages.yml)\n",
    "\n",
    "Only dependency: `dbt-utils` for utility macros (`date_spine`, `accepted_range`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/packages.yml\n",
    "packages:\n",
    "  - package: dbt-labs/dbt_utils\n",
    "    version: [\">=1.1.0\", \"<2.0.0\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Source Definition (sources.yml)\n",
    "\n",
    "> **Production feature:** `freshness` config enables `dbt source freshness` checks.\n",
    "> - `warn_after: 30 days` — alerts if data is stale for batch/historical loads\n",
    "> - `error_after: 365 days` — hard failure for ancient data\n",
    "> - `loaded_at_field: pickup_datetime` — which column to check for freshness"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/sources/sources.yml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: raw_nyc_taxi\n",
    "    description: \"NYC TLC Yellow Taxi data - Iceberg Silver table via DuckDB iceberg_scan\"\n",
    "    schema: main\n",
    "    freshness:\n",
    "      # Adjust thresholds for your SLA (using generous window for historical data)\n",
    "      warn_after: {count: 30, period: day}\n",
    "      error_after: {count: 365, period: day}\n",
    "    loaded_at_field: pickup_datetime\n",
    "    tables:\n",
    "      - name: raw_yellow_trips\n",
    "        description: \"Silver-layer cleaned trips from Iceberg (stream-processed by Flink)\"\n",
    "        config:\n",
    "          external_location: \"iceberg_scan('s3://warehouse/silver/cleaned_trips', allow_moved_paths = true)\"\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. dbt Seeds: Reference Data\n",
    "\n",
    "Seeds are CSV files that dbt loads as tables. They provide lookup/reference data\n",
    "for enriching trip records with human-readable names.\n",
    "\n",
    "### 11.1 Payment Type Lookup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/seeds/payment_type_lookup.csv\n",
    "payment_type_id,payment_type_name\n",
    "1,Credit card\n",
    "2,Cash\n",
    "3,No charge\n",
    "4,Dispute\n",
    "5,Unknown\n",
    "6,Voided trip"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Rate Code Lookup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/seeds/rate_code_lookup.csv\n",
    "rate_code_id,rate_code_name\n",
    "1,Standard rate\n",
    "2,JFK\n",
    "3,Newark\n",
    "4,Nassau or Westchester\n",
    "5,Negotiated fare\n",
    "6,Group ride\n",
    "99,Unknown"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Seed Properties (column types and tests)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/seeds/seed_properties.yml\n",
    "version: 2\n",
    "\n",
    "seeds:\n",
    "  - name: taxi_zone_lookup\n",
    "    description: \"NYC TLC Taxi Zone lookup table (~265 zones)\"\n",
    "    columns:\n",
    "      - name: LocationID\n",
    "        tests: [unique, not_null]\n",
    "      - name: Borough\n",
    "        tests: [not_null]\n",
    "\n",
    "  - name: payment_type_lookup\n",
    "    description: \"Payment type ID to description mapping\"\n",
    "    columns:\n",
    "      - name: payment_type_id\n",
    "        tests: [unique, not_null]\n",
    "\n",
    "  - name: rate_code_lookup\n",
    "    description: \"Rate code ID to description mapping\"\n",
    "    columns:\n",
    "      - name: rate_code_id\n",
    "        tests: [unique, not_null]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Taxi Zone Lookup (265 NYC zones)\n",
    "\n",
    "Maps LocationID to borough and zone name. This is the official NYC TLC taxi zone reference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/seeds/taxi_zone_lookup.csv\n",
    "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\n",
    "1,\"EWR\",\"Newark Airport\",\"EWR\"\n",
    "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\n",
    "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\n",
    "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\n",
    "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\n",
    "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\n",
    "7,\"Queens\",\"Astoria\",\"Boro Zone\"\n",
    "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\n",
    "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\n",
    "10,\"Queens\",\"Baisley Park\",\"Boro Zone\"\n",
    "11,\"Brooklyn\",\"Bath Beach\",\"Boro Zone\"\n",
    "12,\"Manhattan\",\"Battery Park\",\"Yellow Zone\"\n",
    "13,\"Manhattan\",\"Battery Park City\",\"Yellow Zone\"\n",
    "14,\"Brooklyn\",\"Bay Ridge\",\"Boro Zone\"\n",
    "15,\"Queens\",\"Bay Terrace/Fort Totten\",\"Boro Zone\"\n",
    "16,\"Queens\",\"Bayside\",\"Boro Zone\"\n",
    "17,\"Brooklyn\",\"Bedford\",\"Boro Zone\"\n",
    "18,\"Bronx\",\"Bedford Park\",\"Boro Zone\"\n",
    "19,\"Queens\",\"Bellerose\",\"Boro Zone\"\n",
    "20,\"Bronx\",\"Belmont\",\"Boro Zone\"\n",
    "21,\"Brooklyn\",\"Bensonhurst East\",\"Boro Zone\"\n",
    "22,\"Brooklyn\",\"Bensonhurst West\",\"Boro Zone\"\n",
    "23,\"Staten Island\",\"Bloomfield/Emerson Hill\",\"Boro Zone\"\n",
    "24,\"Manhattan\",\"Bloomingdale\",\"Yellow Zone\"\n",
    "25,\"Brooklyn\",\"Boerum Hill\",\"Boro Zone\"\n",
    "26,\"Brooklyn\",\"Borough Park\",\"Boro Zone\"\n",
    "27,\"Queens\",\"Breezy Point/Fort Tilden/Riis Beach\",\"Boro Zone\"\n",
    "28,\"Queens\",\"Briarwood/Jamaica Hills\",\"Boro Zone\"\n",
    "29,\"Brooklyn\",\"Brighton Beach\",\"Boro Zone\"\n",
    "30,\"Queens\",\"Broad Channel\",\"Boro Zone\"\n",
    "31,\"Bronx\",\"Bronx Park\",\"Boro Zone\"\n",
    "32,\"Bronx\",\"Bronxdale\",\"Boro Zone\"\n",
    "33,\"Brooklyn\",\"Brooklyn Heights\",\"Boro Zone\"\n",
    "34,\"Brooklyn\",\"Brooklyn Navy Yard\",\"Boro Zone\"\n",
    "35,\"Brooklyn\",\"Brownsville\",\"Boro Zone\"\n",
    "36,\"Brooklyn\",\"Bushwick North\",\"Boro Zone\"\n",
    "37,\"Brooklyn\",\"Bushwick South\",\"Boro Zone\"\n",
    "38,\"Queens\",\"Cambria Heights\",\"Boro Zone\"\n",
    "39,\"Brooklyn\",\"Canarsie\",\"Boro Zone\"\n",
    "40,\"Brooklyn\",\"Carroll Gardens\",\"Boro Zone\"\n",
    "41,\"Manhattan\",\"Central Harlem\",\"Boro Zone\"\n",
    "42,\"Manhattan\",\"Central Harlem North\",\"Boro Zone\"\n",
    "43,\"Manhattan\",\"Central Park\",\"Yellow Zone\"\n",
    "44,\"Staten Island\",\"Charleston/Tottenville\",\"Boro Zone\"\n",
    "45,\"Manhattan\",\"Chinatown\",\"Yellow Zone\"\n",
    "46,\"Bronx\",\"City Island\",\"Boro Zone\"\n",
    "47,\"Bronx\",\"Claremont/Bathgate\",\"Boro Zone\"\n",
    "48,\"Manhattan\",\"Clinton East\",\"Yellow Zone\"\n",
    "49,\"Brooklyn\",\"Clinton Hill\",\"Boro Zone\"\n",
    "50,\"Manhattan\",\"Clinton West\",\"Yellow Zone\"\n",
    "51,\"Bronx\",\"Co-Op City\",\"Boro Zone\"\n",
    "52,\"Brooklyn\",\"Cobble Hill\",\"Boro Zone\"\n",
    "53,\"Queens\",\"College Point\",\"Boro Zone\"\n",
    "54,\"Brooklyn\",\"Columbia Street\",\"Boro Zone\"\n",
    "55,\"Brooklyn\",\"Coney Island\",\"Boro Zone\"\n",
    "56,\"Queens\",\"Corona\",\"Boro Zone\"\n",
    "57,\"Queens\",\"Corona\",\"Boro Zone\"\n",
    "58,\"Bronx\",\"Country Club\",\"Boro Zone\"\n",
    "59,\"Bronx\",\"Crotona Park\",\"Boro Zone\"\n",
    "60,\"Bronx\",\"Crotona Park East\",\"Boro Zone\"\n",
    "61,\"Brooklyn\",\"Crown Heights North\",\"Boro Zone\"\n",
    "62,\"Brooklyn\",\"Crown Heights South\",\"Boro Zone\"\n",
    "63,\"Brooklyn\",\"Cypress Hills\",\"Boro Zone\"\n",
    "64,\"Queens\",\"Douglaston\",\"Boro Zone\"\n",
    "65,\"Brooklyn\",\"Downtown Brooklyn/MetroTech\",\"Boro Zone\"\n",
    "66,\"Brooklyn\",\"DUMBO/Vinegar Hill\",\"Boro Zone\"\n",
    "67,\"Brooklyn\",\"Dyker Heights\",\"Boro Zone\"\n",
    "68,\"Manhattan\",\"East Chelsea\",\"Yellow Zone\"\n",
    "69,\"Bronx\",\"East Concourse/Concourse Village\",\"Boro Zone\"\n",
    "70,\"Queens\",\"East Elmhurst\",\"Boro Zone\"\n",
    "71,\"Brooklyn\",\"East Flatbush/Farragut\",\"Boro Zone\"\n",
    "72,\"Brooklyn\",\"East Flatbush/Remsen Village\",\"Boro Zone\"\n",
    "73,\"Queens\",\"East Flushing\",\"Boro Zone\"\n",
    "74,\"Manhattan\",\"East Harlem North\",\"Boro Zone\"\n",
    "75,\"Manhattan\",\"East Harlem South\",\"Boro Zone\"\n",
    "76,\"Brooklyn\",\"East New York\",\"Boro Zone\"\n",
    "77,\"Brooklyn\",\"East New York/Pennsylvania Avenue\",\"Boro Zone\"\n",
    "78,\"Bronx\",\"East Tremont\",\"Boro Zone\"\n",
    "79,\"Manhattan\",\"East Village\",\"Yellow Zone\"\n",
    "80,\"Brooklyn\",\"East Williamsburg\",\"Boro Zone\"\n",
    "81,\"Bronx\",\"Eastchester\",\"Boro Zone\"\n",
    "82,\"Queens\",\"Elmhurst\",\"Boro Zone\"\n",
    "83,\"Queens\",\"Elmhurst/Maspeth\",\"Boro Zone\"\n",
    "84,\"Staten Island\",\"Eltingville/Annadale/Prince's Bay\",\"Boro Zone\"\n",
    "85,\"Brooklyn\",\"Erasmus\",\"Boro Zone\"\n",
    "86,\"Queens\",\"Far Rockaway\",\"Boro Zone\"\n",
    "87,\"Manhattan\",\"Financial District North\",\"Yellow Zone\"\n",
    "88,\"Manhattan\",\"Financial District South\",\"Yellow Zone\"\n",
    "89,\"Brooklyn\",\"Flatbush/Ditmas Park\",\"Boro Zone\"\n",
    "90,\"Manhattan\",\"Flatiron\",\"Yellow Zone\"\n",
    "91,\"Brooklyn\",\"Flatlands\",\"Boro Zone\"\n",
    "92,\"Queens\",\"Flushing\",\"Boro Zone\"\n",
    "93,\"Queens\",\"Flushing Meadows-Corona Park\",\"Boro Zone\"\n",
    "94,\"Bronx\",\"Fordham South\",\"Boro Zone\"\n",
    "95,\"Queens\",\"Forest Hills\",\"Boro Zone\"\n",
    "96,\"Queens\",\"Forest Park/Highland Park\",\"Boro Zone\"\n",
    "97,\"Brooklyn\",\"Fort Greene\",\"Boro Zone\"\n",
    "98,\"Queens\",\"Fresh Meadows\",\"Boro Zone\"\n",
    "99,\"Staten Island\",\"Freshkills Park\",\"Boro Zone\"\n",
    "100,\"Manhattan\",\"Garment District\",\"Yellow Zone\"\n",
    "101,\"Queens\",\"Glen Oaks\",\"Boro Zone\"\n",
    "102,\"Queens\",\"Glendale\",\"Boro Zone\"\n",
    "103,\"Manhattan\",\"Governor's Island/Ellis Island/Liberty Island\",\"Yellow Zone\"\n",
    "104,\"Manhattan\",\"Governor's Island/Ellis Island/Liberty Island\",\"Yellow Zone\"\n",
    "105,\"Manhattan\",\"Governor's Island/Ellis Island/Liberty Island\",\"Yellow Zone\"\n",
    "106,\"Brooklyn\",\"Gowanus\",\"Boro Zone\"\n",
    "107,\"Manhattan\",\"Gramercy\",\"Yellow Zone\"\n",
    "108,\"Brooklyn\",\"Gravesend\",\"Boro Zone\"\n",
    "109,\"Staten Island\",\"Great Kills\",\"Boro Zone\"\n",
    "110,\"Staten Island\",\"Great Kills Park\",\"Boro Zone\"\n",
    "111,\"Brooklyn\",\"Green-Wood Cemetery\",\"Boro Zone\"\n",
    "112,\"Brooklyn\",\"Greenpoint\",\"Boro Zone\"\n",
    "113,\"Manhattan\",\"Greenwich Village North\",\"Yellow Zone\"\n",
    "114,\"Manhattan\",\"Greenwich Village South\",\"Yellow Zone\"\n",
    "115,\"Staten Island\",\"Grymes Hill/Clifton\",\"Boro Zone\"\n",
    "116,\"Manhattan\",\"Hamilton Heights\",\"Boro Zone\"\n",
    "117,\"Queens\",\"Hammels/Arverne\",\"Boro Zone\"\n",
    "118,\"Staten Island\",\"Heartland Village/Todt Hill\",\"Boro Zone\"\n",
    "119,\"Bronx\",\"Highbridge\",\"Boro Zone\"\n",
    "120,\"Manhattan\",\"Highbridge Park\",\"Boro Zone\"\n",
    "121,\"Queens\",\"Hillcrest/Pomonok\",\"Boro Zone\"\n",
    "122,\"Queens\",\"Hollis\",\"Boro Zone\"\n",
    "123,\"Brooklyn\",\"Homecrest\",\"Boro Zone\"\n",
    "124,\"Queens\",\"Howard Beach\",\"Boro Zone\"\n",
    "125,\"Manhattan\",\"Hudson Sq\",\"Yellow Zone\"\n",
    "126,\"Bronx\",\"Hunts Point\",\"Boro Zone\"\n",
    "127,\"Manhattan\",\"Inwood\",\"Boro Zone\"\n",
    "128,\"Manhattan\",\"Inwood Hill Park\",\"Boro Zone\"\n",
    "129,\"Queens\",\"Jackson Heights\",\"Boro Zone\"\n",
    "130,\"Queens\",\"Jamaica\",\"Boro Zone\"\n",
    "131,\"Queens\",\"Jamaica Estates\",\"Boro Zone\"\n",
    "132,\"Queens\",\"JFK Airport\",\"Airports\"\n",
    "133,\"Brooklyn\",\"Kensington\",\"Boro Zone\"\n",
    "134,\"Queens\",\"Kew Gardens\",\"Boro Zone\"\n",
    "135,\"Queens\",\"Kew Gardens Hills\",\"Boro Zone\"\n",
    "136,\"Bronx\",\"Kingsbridge Heights\",\"Boro Zone\"\n",
    "137,\"Manhattan\",\"Kips Bay\",\"Yellow Zone\"\n",
    "138,\"Queens\",\"LaGuardia Airport\",\"Airports\"\n",
    "139,\"Queens\",\"Laurelton\",\"Boro Zone\"\n",
    "140,\"Manhattan\",\"Lenox Hill East\",\"Yellow Zone\"\n",
    "141,\"Manhattan\",\"Lenox Hill West\",\"Yellow Zone\"\n",
    "142,\"Manhattan\",\"Lincoln Square East\",\"Yellow Zone\"\n",
    "143,\"Manhattan\",\"Lincoln Square West\",\"Yellow Zone\"\n",
    "144,\"Manhattan\",\"Little Italy/NoLiTa\",\"Yellow Zone\"\n",
    "145,\"Queens\",\"Long Island City/Hunters Point\",\"Boro Zone\"\n",
    "146,\"Queens\",\"Long Island City/Queens Plaza\",\"Boro Zone\"\n",
    "147,\"Bronx\",\"Longwood\",\"Boro Zone\"\n",
    "148,\"Manhattan\",\"Lower East Side\",\"Yellow Zone\"\n",
    "149,\"Brooklyn\",\"Madison\",\"Boro Zone\"\n",
    "150,\"Brooklyn\",\"Manhattan Beach\",\"Boro Zone\"\n",
    "151,\"Manhattan\",\"Manhattan Valley\",\"Yellow Zone\"\n",
    "152,\"Manhattan\",\"Manhattanville\",\"Boro Zone\"\n",
    "153,\"Manhattan\",\"Marble Hill\",\"Boro Zone\"\n",
    "154,\"Brooklyn\",\"Marine Park/Floyd Bennett Field\",\"Boro Zone\"\n",
    "155,\"Brooklyn\",\"Marine Park/Mill Basin\",\"Boro Zone\"\n",
    "156,\"Staten Island\",\"Mariners Harbor\",\"Boro Zone\"\n",
    "157,\"Queens\",\"Maspeth\",\"Boro Zone\"\n",
    "158,\"Manhattan\",\"Meatpacking/West Village West\",\"Yellow Zone\"\n",
    "159,\"Bronx\",\"Melrose South\",\"Boro Zone\"\n",
    "160,\"Queens\",\"Middle Village\",\"Boro Zone\"\n",
    "161,\"Manhattan\",\"Midtown Center\",\"Yellow Zone\"\n",
    "162,\"Manhattan\",\"Midtown East\",\"Yellow Zone\"\n",
    "163,\"Manhattan\",\"Midtown North\",\"Yellow Zone\"\n",
    "164,\"Manhattan\",\"Midtown South\",\"Yellow Zone\"\n",
    "165,\"Brooklyn\",\"Midwood\",\"Boro Zone\"\n",
    "166,\"Manhattan\",\"Morningside Heights\",\"Boro Zone\"\n",
    "167,\"Bronx\",\"Morrisania/Melrose\",\"Boro Zone\"\n",
    "168,\"Bronx\",\"Mott Haven/Port Morris\",\"Boro Zone\"\n",
    "169,\"Bronx\",\"Mount Hope\",\"Boro Zone\"\n",
    "170,\"Manhattan\",\"Murray Hill\",\"Yellow Zone\"\n",
    "171,\"Queens\",\"Murray Hill-Queens\",\"Boro Zone\"\n",
    "172,\"Staten Island\",\"New Dorp/Midland Beach\",\"Boro Zone\"\n",
    "173,\"Queens\",\"North Corona\",\"Boro Zone\"\n",
    "174,\"Bronx\",\"Norwood\",\"Boro Zone\"\n",
    "175,\"Queens\",\"Oakland Gardens\",\"Boro Zone\"\n",
    "176,\"Staten Island\",\"Oakwood\",\"Boro Zone\"\n",
    "177,\"Brooklyn\",\"Ocean Hill\",\"Boro Zone\"\n",
    "178,\"Brooklyn\",\"Ocean Parkway South\",\"Boro Zone\"\n",
    "179,\"Queens\",\"Old Astoria\",\"Boro Zone\"\n",
    "180,\"Queens\",\"Ozone Park\",\"Boro Zone\"\n",
    "181,\"Brooklyn\",\"Park Slope\",\"Boro Zone\"\n",
    "182,\"Bronx\",\"Parkchester\",\"Boro Zone\"\n",
    "183,\"Bronx\",\"Pelham Bay\",\"Boro Zone\"\n",
    "184,\"Bronx\",\"Pelham Bay Park\",\"Boro Zone\"\n",
    "185,\"Bronx\",\"Pelham Parkway\",\"Boro Zone\"\n",
    "186,\"Manhattan\",\"Penn Station/Madison Sq West\",\"Yellow Zone\"\n",
    "187,\"Staten Island\",\"Port Richmond\",\"Boro Zone\"\n",
    "188,\"Brooklyn\",\"Prospect-Lefferts Gardens\",\"Boro Zone\"\n",
    "189,\"Brooklyn\",\"Prospect Heights\",\"Boro Zone\"\n",
    "190,\"Brooklyn\",\"Prospect Park\",\"Boro Zone\"\n",
    "191,\"Queens\",\"Queens Village\",\"Boro Zone\"\n",
    "192,\"Queens\",\"Queensboro Hill\",\"Boro Zone\"\n",
    "193,\"Queens\",\"Queensbridge/Ravenswood\",\"Boro Zone\"\n",
    "194,\"Manhattan\",\"Randalls Island\",\"Yellow Zone\"\n",
    "195,\"Brooklyn\",\"Red Hook\",\"Boro Zone\"\n",
    "196,\"Queens\",\"Rego Park\",\"Boro Zone\"\n",
    "197,\"Queens\",\"Richmond Hill\",\"Boro Zone\"\n",
    "198,\"Queens\",\"Ridgewood\",\"Boro Zone\"\n",
    "199,\"Bronx\",\"Rikers Island\",\"Boro Zone\"\n",
    "200,\"Bronx\",\"Riverdale/North Riverdale/Fieldston\",\"Boro Zone\"\n",
    "201,\"Queens\",\"Rockaway Park\",\"Boro Zone\"\n",
    "202,\"Manhattan\",\"Roosevelt Island\",\"Boro Zone\"\n",
    "203,\"Queens\",\"Rosedale\",\"Boro Zone\"\n",
    "204,\"Staten Island\",\"Rossville/Woodrow\",\"Boro Zone\"\n",
    "205,\"Queens\",\"Saint Albans\",\"Boro Zone\"\n",
    "206,\"Staten Island\",\"Saint George/New Brighton\",\"Boro Zone\"\n",
    "207,\"Queens\",\"Saint Michaels Cemetery/Woodside\",\"Boro Zone\"\n",
    "208,\"Bronx\",\"Schuylerville/Edgewater Park\",\"Boro Zone\"\n",
    "209,\"Manhattan\",\"Seaport\",\"Yellow Zone\"\n",
    "210,\"Brooklyn\",\"Sheepshead Bay\",\"Boro Zone\"\n",
    "211,\"Manhattan\",\"SoHo\",\"Yellow Zone\"\n",
    "212,\"Bronx\",\"Soundview/Bruckner\",\"Boro Zone\"\n",
    "213,\"Bronx\",\"Soundview/Castle Hill\",\"Boro Zone\"\n",
    "214,\"Staten Island\",\"South Beach/Dongan Hills\",\"Boro Zone\"\n",
    "215,\"Queens\",\"South Jamaica\",\"Boro Zone\"\n",
    "216,\"Queens\",\"South Ozone Park\",\"Boro Zone\"\n",
    "217,\"Brooklyn\",\"South Williamsburg\",\"Boro Zone\"\n",
    "218,\"Queens\",\"Springfield Gardens North\",\"Boro Zone\"\n",
    "219,\"Queens\",\"Springfield Gardens South\",\"Boro Zone\"\n",
    "220,\"Bronx\",\"Spuyten Duyvil/Kingsbridge\",\"Boro Zone\"\n",
    "221,\"Staten Island\",\"Stapleton\",\"Boro Zone\"\n",
    "222,\"Brooklyn\",\"Starrett City\",\"Boro Zone\"\n",
    "223,\"Queens\",\"Steinway\",\"Boro Zone\"\n",
    "224,\"Manhattan\",\"Stuy Town/Peter Cooper Village\",\"Yellow Zone\"\n",
    "225,\"Brooklyn\",\"Stuyvesant Heights\",\"Boro Zone\"\n",
    "226,\"Queens\",\"Sunnyside\",\"Boro Zone\"\n",
    "227,\"Brooklyn\",\"Sunset Park East\",\"Boro Zone\"\n",
    "228,\"Brooklyn\",\"Sunset Park West\",\"Boro Zone\"\n",
    "229,\"Manhattan\",\"Sutton Place/Turtle Bay North\",\"Yellow Zone\"\n",
    "230,\"Manhattan\",\"Times Sq/Theatre District\",\"Yellow Zone\"\n",
    "231,\"Manhattan\",\"TriBeCa/Civic Center\",\"Yellow Zone\"\n",
    "232,\"Manhattan\",\"Two Bridges/Seward Park\",\"Yellow Zone\"\n",
    "233,\"Manhattan\",\"UN/Turtle Bay South\",\"Yellow Zone\"\n",
    "234,\"Manhattan\",\"Union Sq\",\"Yellow Zone\"\n",
    "235,\"Bronx\",\"University Heights/Morris Heights\",\"Boro Zone\"\n",
    "236,\"Manhattan\",\"Upper East Side North\",\"Yellow Zone\"\n",
    "237,\"Manhattan\",\"Upper East Side South\",\"Yellow Zone\"\n",
    "238,\"Manhattan\",\"Upper West Side North\",\"Yellow Zone\"\n",
    "239,\"Manhattan\",\"Upper West Side South\",\"Yellow Zone\"\n",
    "240,\"Bronx\",\"Van Cortlandt Park\",\"Boro Zone\"\n",
    "241,\"Bronx\",\"Van Cortlandt Village\",\"Boro Zone\"\n",
    "242,\"Bronx\",\"Van Nest/Morris Park\",\"Boro Zone\"\n",
    "243,\"Manhattan\",\"Washington Heights North\",\"Boro Zone\"\n",
    "244,\"Manhattan\",\"Washington Heights South\",\"Boro Zone\"\n",
    "245,\"Staten Island\",\"West Brighton\",\"Boro Zone\"\n",
    "246,\"Manhattan\",\"West Chelsea/Hudson Yards\",\"Yellow Zone\"\n",
    "247,\"Bronx\",\"West Concourse\",\"Boro Zone\"\n",
    "248,\"Bronx\",\"West Farms/Bronx River\",\"Boro Zone\"\n",
    "249,\"Manhattan\",\"West Village\",\"Yellow Zone\"\n",
    "250,\"Bronx\",\"Westchester Village/Unionport\",\"Boro Zone\"\n",
    "251,\"Staten Island\",\"Westerleigh\",\"Boro Zone\"\n",
    "252,\"Queens\",\"Whitestone\",\"Boro Zone\"\n",
    "253,\"Queens\",\"Willets Point\",\"Boro Zone\"\n",
    "254,\"Bronx\",\"Williamsbridge/Olinville\",\"Boro Zone\"\n",
    "255,\"Brooklyn\",\"Williamsburg (North Side)\",\"Boro Zone\"\n",
    "256,\"Brooklyn\",\"Williamsburg (South Side)\",\"Boro Zone\"\n",
    "257,\"Brooklyn\",\"Windsor Terrace\",\"Boro Zone\"\n",
    "258,\"Queens\",\"Woodhaven\",\"Boro Zone\"\n",
    "259,\"Bronx\",\"Woodlawn/Wakefield\",\"Boro Zone\"\n",
    "260,\"Queens\",\"Woodside\",\"Boro Zone\"\n",
    "261,\"Manhattan\",\"World Trade Center\",\"Yellow Zone\"\n",
    "262,\"Manhattan\",\"Yorkville East\",\"Yellow Zone\"\n",
    "263,\"Manhattan\",\"Yorkville West\",\"Yellow Zone\"\n",
    "264,\"Unknown\",\"N/A\",\"N/A\"\n",
    "265,\"N/A\",\"Outside of NYC\",\"N/A\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Vendor Lookup",
    "",
    "Maps VendorID to vendor name/abbreviation. NYC TLC has two TPEP providers:",
    "- 1 = Creative Mobile Technologies (CMT)",
    "- 2 = VeriFone Inc. (VFI)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/seeds/vendor_lookup.csv",
    "vendor_id,vendor_name,vendor_abbr",
    "1,Creative Mobile Technologies,CMT",
    "2,VeriFone Inc.,VFI",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. dbt Macros: Cross-Database Compatibility\n",
    "\n",
    "These macros use dbt's `adapter.dispatch()` pattern to work across DuckDB, PostgreSQL (RisingWave),\n",
    "and Spark. This means the same dbt models can be reused in Pipelines 01-11.\n",
    "\n",
    "### 12.1 cents_to_dollars"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/macros/cents_to_dollars.sql\n",
    "/*\n",
    "    Macro: Convert a cents column to dollars with rounding.\n",
    "\n",
    "    Usage:\n",
    "        {{ cents_to_dollars('fare_cents') }}\n",
    "        {{ cents_to_dollars('fare_cents', 4) }}\n",
    "*/\n",
    "\n",
    "{% macro cents_to_dollars(column_name, precision=2) %}\n",
    "    round(cast({{ column_name }} as decimal(10, {{ precision }})) / 100, {{ precision }})\n",
    "{% endmacro %}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 dayname_compat, monthname_compat, mode_compat\n",
    "\n",
    "Three adapter-dispatched macros that handle DuckDB/PostgreSQL/Spark syntax differences:\n",
    "- `dayname_compat()` → `dayname()` (DuckDB) vs `to_char(..., 'Day')` (Postgres) vs `date_format(..., 'EEEE')` (Spark)\n",
    "- `monthname_compat()` → Same pattern for month names\n",
    "- `mode_compat()` → Statistical mode (most common value)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/macros/dayname_compat.sql\n",
    "/*\n",
    "    Macro: Get day-of-week name from a timestamp.\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL (RisingWave), and Spark.\n",
    "\n",
    "    Usage:\n",
    "        {{ dayname_compat('pickup_datetime') }}\n",
    "*/\n",
    "\n",
    "{% macro dayname_compat(col) %}\n",
    "    {{ return(adapter.dispatch('dayname_compat', 'nyc_taxi_dbt')(col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__dayname_compat(col) %}\n",
    "    dayname({{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__dayname_compat(col) %}\n",
    "    trim(to_char({{ col }}, 'Day'))\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__dayname_compat(col) %}\n",
    "    date_format({{ col }}, 'EEEE')\n",
    "{% endmacro %}\n",
    "\n",
    "\n",
    "/*\n",
    "    Macro: Get month name from a timestamp.\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL (RisingWave), and Spark.\n",
    "*/\n",
    "\n",
    "{% macro monthname_compat(col) %}\n",
    "    {{ return(adapter.dispatch('monthname_compat', 'nyc_taxi_dbt')(col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__monthname_compat(col) %}\n",
    "    monthname({{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__monthname_compat(col) %}\n",
    "    trim(to_char({{ col }}, 'Month'))\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__monthname_compat(col) %}\n",
    "    date_format({{ col }}, 'MMMM')\n",
    "{% endmacro %}\n",
    "\n",
    "\n",
    "/*\n",
    "    Macro: Statistical mode (most common value).\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL, and Spark.\n",
    "*/\n",
    "\n",
    "{% macro mode_compat(col) %}\n",
    "    {{ return(adapter.dispatch('mode_compat', 'nyc_taxi_dbt')(col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__mode_compat(col) %}\n",
    "    mode({{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__mode_compat(col) %}\n",
    "    mode() WITHIN GROUP (ORDER BY {{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__mode_compat(col) %}\n",
    "    mode({{ col }})\n",
    "{% endmacro %}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 duration_minutes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/macros/duration_minutes.sql\n",
    "/*\n",
    "    Macro: Calculate duration between two timestamps in minutes.\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL (RisingWave), and Spark.\n",
    "\n",
    "    Usage:\n",
    "        {{ duration_minutes('pickup_datetime', 'dropoff_datetime') }}\n",
    "*/\n",
    "\n",
    "{% macro duration_minutes(start_col, end_col) %}\n",
    "    {{ return(adapter.dispatch('duration_minutes', 'nyc_taxi_dbt')(start_col, end_col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__duration_minutes(start_col, end_col) %}\n",
    "    datediff('minute', {{ start_col }}, {{ end_col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__duration_minutes(start_col, end_col) %}\n",
    "    (EXTRACT(EPOCH FROM ({{ end_col }} - {{ start_col }})) / 60)::bigint\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__duration_minutes(start_col, end_col) %}\n",
    "    CAST((UNIX_TIMESTAMP({{ end_col }}) - UNIX_TIMESTAMP({{ start_col }})) / 60 AS BIGINT)\n",
    "{% endmacro %}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 test_positive_value (custom generic test)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/macros/test_positive_value.sql\n",
    "/*\n",
    "    Custom generic test: Asserts that all values in a column are >= 0.\n",
    "\n",
    "    Usage in schema.yml:\n",
    "        columns:\n",
    "          - name: fare_amount\n",
    "            tests:\n",
    "              - positive_value\n",
    "*/\n",
    "\n",
    "{% test positive_value(model, column_name) %}\n",
    "\n",
    "select\n",
    "    {{ column_name }} as invalid_value\n",
    "from {{ model }}\n",
    "where {{ column_name }} < 0\n",
    "\n",
    "{% endtest %}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. dbt Staging Models\n",
    "\n",
    "Staging models are **thin wrappers** over sources. In this pipeline, Flink already\n",
    "did the heavy lifting (column renaming, type casting, quality filtering), so staging\n",
    "is mostly a passthrough with minor re-casting for DuckDB type compatibility.\n",
    "\n",
    "### dbt Lineage (DAG)\n",
    "\n",
    "```\n",
    "source(raw_yellow_trips)  seed(payment_type_lookup)  seed(rate_code_lookup)  seed(taxi_zone_lookup)\n",
    "         │                         │                        │                       │\n",
    "         ▼                         ▼                        ▼                       ▼\n",
    "  stg_yellow_trips          stg_payment_types        stg_rate_codes          stg_taxi_zones\n",
    "         │                         │                        │                       │\n",
    "         ▼                         │                        │                       │\n",
    "  int_trip_metrics                 │                        │                       │\n",
    "    │       │                      │                        │                       │\n",
    "    ▼       ▼                      │                        │                       │\n",
    "int_daily  int_hourly              │                        │                       │\n",
    " _summary   _patterns              │                        │                       │\n",
    "    │       │                      │                        │                       │\n",
    "    ▼       ▼                      ▼                        │                       ▼\n",
    "mart_daily  mart_hourly      dim_payment_types              │               dim_locations\n",
    " _revenue    _demand               │                        │                  │\n",
    "                                   │                        │                  │\n",
    "                                   └──────────┬─────────────┘                  │\n",
    "                                              ▼                                │\n",
    "                                          fct_trips ◄──────────────────────────┘\n",
    "                                              │\n",
    "                                              ▼\n",
    "                                   mart_location_performance\n",
    "```\n",
    "\n",
    "### 13.1 stg_yellow_trips.sql\n",
    "\n",
    "The main staging model. Since Flink already cleaned the data, this is a passthrough\n",
    "with safety-net null filtering and DuckDB type re-casting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/staging/stg_yellow_trips.sql\n",
    "{#\n",
    "    Staging model: Yellow taxi trip records (Iceberg pipeline variant)\n",
    "\n",
    "    This is a simple passthrough since Flink already performed the heavy lifting:\n",
    "      - Column renaming (VendorID -> vendor_id, etc.)\n",
    "      - Type casting\n",
    "      - Data quality filtering (nulls, negative fares, date range)\n",
    "      - Surrogate key generation (MD5 hash)\n",
    "      - Computed columns (duration, speed, cost, tip %)\n",
    "\n",
    "    The source reads the Silver Iceberg table via DuckDB iceberg_scan().\n",
    "#}\n",
    "\n",
    "with source as (\n",
    "    select * from {{ source('raw_nyc_taxi', 'raw_yellow_trips') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        -- Flink already generated the surrogate key\n",
    "        trip_id,\n",
    "\n",
    "        -- identifiers (already renamed and cast by Flink)\n",
    "        vendor_id,\n",
    "        rate_code_id,\n",
    "        pickup_location_id,\n",
    "        dropoff_location_id,\n",
    "        payment_type_id,\n",
    "\n",
    "        -- timestamps (already parsed by Flink)\n",
    "        cast(pickup_datetime as timestamp) as pickup_datetime,\n",
    "        cast(dropoff_datetime as timestamp) as dropoff_datetime,\n",
    "\n",
    "        -- trip info\n",
    "        passenger_count,\n",
    "        trip_distance_miles,\n",
    "        store_and_fwd_flag,\n",
    "\n",
    "        -- financials (already rounded by Flink)\n",
    "        round(cast(fare_amount as decimal(10, 2)), 2) as fare_amount,\n",
    "        round(cast(extra_amount as decimal(10, 2)), 2) as extra_amount,\n",
    "        round(cast(mta_tax as decimal(10, 2)), 2) as mta_tax,\n",
    "        round(cast(tip_amount as decimal(10, 2)), 2) as tip_amount,\n",
    "        round(cast(tolls_amount as decimal(10, 2)), 2) as tolls_amount,\n",
    "        round(cast(improvement_surcharge as decimal(10, 2)), 2) as improvement_surcharge,\n",
    "        round(cast(total_amount as decimal(10, 2)), 2) as total_amount,\n",
    "        round(cast(congestion_surcharge as decimal(10, 2)), 2) as congestion_surcharge,\n",
    "        round(cast(airport_fee as decimal(10, 2)), 2) as airport_fee\n",
    "\n",
    "    from source\n",
    "    -- Flink already applied quality filters; this is a safety net\n",
    "    where pickup_datetime is not null\n",
    "      and dropoff_datetime is not null\n",
    ")\n",
    "\n",
    "select * from final"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 stg_payment_types.sql"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/staging/stg_payment_types.sql\n",
    "/*\n",
    "    Staging model: Payment type lookup\n",
    "    Maps payment_type_id to human-readable names.\n",
    "*/\n",
    "\n",
    "with source as (\n",
    "    select * from {{ ref('payment_type_lookup') }}\n",
    "),\n",
    "\n",
    "renamed as (\n",
    "    select\n",
    "        payment_type_id,\n",
    "        payment_type_name\n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from renamed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 stg_rate_codes.sql"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/staging/stg_rate_codes.sql\n",
    "/*\n",
    "    Staging model: Rate code lookup\n",
    "    Maps rate_code_id to human-readable names.\n",
    "*/\n",
    "\n",
    "with source as (\n",
    "    select * from {{ ref('rate_code_lookup') }}\n",
    "),\n",
    "\n",
    "renamed as (\n",
    "    select\n",
    "        rate_code_id,\n",
    "        rate_code_name\n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from renamed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4 stg_taxi_zones.sql"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/staging/stg_taxi_zones.sql\n",
    "/*\n",
    "    Staging model: Taxi zone lookup\n",
    "    Maps LocationID to borough and zone name.\n",
    "*/\n",
    "\n",
    "with source as (\n",
    "    select * from {{ ref('taxi_zone_lookup') }}\n",
    "),\n",
    "\n",
    "renamed as (\n",
    "    select\n",
    "        cast(\"LocationID\" as integer) as location_id,\n",
    "        \"Borough\" as borough,\n",
    "        \"Zone\" as zone_name,\n",
    "        service_zone\n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from renamed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5 stg_vendors.sql"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/staging/stg_vendors.sql",
    "/*",
    "    Staging model: Vendor lookup",
    "    Maps vendor_id to vendor name and abbreviation.",
    "*/",
    "",
    "with source as (",
    "    select * from {{ ref('vendor_lookup') }}",
    "),",
    "",
    "renamed as (",
    "    select",
    "        vendor_id,",
    "        vendor_name,",
    "        vendor_abbr",
    "    from source",
    ")",
    "",
    "select * from renamed",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5 staging.yml (schema + tests)\n",
    "\n",
    "Defines **32 tests** across staging models: uniqueness, not-null, accepted values, relationships."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/staging/staging.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: stg_yellow_trips\n",
    "    description: \"Cleaned and renamed yellow taxi trip records from Flink Silver Iceberg table. Flink already applied quality filters and column renaming.\"\n",
    "    columns:\n",
    "      - name: trip_id\n",
    "        description: \"Surrogate key generated by Flink via MD5 hash of VendorID + timestamps + locations + fare/total amounts\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - unique\n",
    "      - name: vendor_id\n",
    "        description: \"TPEP provider: 1=Creative Mobile Technologies, 2=VeriFone Inc., 6=Unknown/Other\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              arguments:\n",
    "                values: [1, 2, 6]\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: rate_code_id\n",
    "        description: \"Rate code: 1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester, 5=Negotiated, 6=Group\"\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              arguments:\n",
    "                values: [1, 2, 3, 4, 5, 6, 99]\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: pickup_location_id\n",
    "        description: \"TLC Taxi Zone ID for pickup\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - relationships:\n",
    "              arguments:\n",
    "                to: ref('stg_taxi_zones')\n",
    "                field: location_id\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: dropoff_location_id\n",
    "        description: \"TLC Taxi Zone ID for dropoff\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - relationships:\n",
    "              arguments:\n",
    "                to: ref('stg_taxi_zones')\n",
    "                field: location_id\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: payment_type_id\n",
    "        description: \"Payment method\"\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              arguments:\n",
    "                values: [0, 1, 2, 3, 4, 5, 6]\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: pickup_datetime\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: dropoff_datetime\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: trip_distance_miles\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: fare_amount\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: total_amount\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: stg_taxi_zones\n",
    "    description: \"Taxi zone reference mapping location IDs to borough and zone names\"\n",
    "    columns:\n",
    "      - name: location_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: borough\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: zone_name\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: stg_payment_types\n",
    "    description: \"Payment type reference\"\n",
    "    columns:\n",
    "      - name: payment_type_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: payment_type_name\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: stg_rate_codes\n",
    "    description: \"Rate code reference\"\n",
    "    columns:\n",
    "      - name: rate_code_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: rate_code_name\n",
    "        tests:\n",
    "          - not_null"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. dbt Intermediate Models\n",
    "\n",
    "Intermediate models add **business logic** on top of staging. They compute metrics,\n",
    "aggregate data, and apply final quality filters.\n",
    "\n",
    "### 14.1 int_trip_metrics.sql\n",
    "\n",
    "Enriches each trip with calculated fields:\n",
    "- `trip_duration_minutes` (using adapter-dispatched macro)\n",
    "- `avg_speed_mph` (with division-by-zero protection)\n",
    "- `cost_per_mile`, `tip_percentage`\n",
    "- `pickup_day_of_week`, `is_weekend`\n",
    "\n",
    "Also applies final quality gates:\n",
    "- Duration between 1-720 minutes\n",
    "- Speed under 100 mph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/intermediate/int_trip_metrics.sql\n",
    "/*\n",
    "    Intermediate model: Trip-level enrichment with calculated metrics.\n",
    "    Uses adapter-dispatched macros for cross-dialect compatibility.\n",
    "*/\n",
    "\n",
    "with trips as (\n",
    "    select * from {{ ref('stg_yellow_trips') }}\n",
    "),\n",
    "\n",
    "enriched as (\n",
    "    select\n",
    "        trip_id,\n",
    "        vendor_id,\n",
    "        rate_code_id,\n",
    "        pickup_location_id,\n",
    "        dropoff_location_id,\n",
    "        payment_type_id,\n",
    "        pickup_datetime,\n",
    "        dropoff_datetime,\n",
    "        passenger_count,\n",
    "        trip_distance_miles,\n",
    "        store_and_fwd_flag,\n",
    "\n",
    "        -- calculated: duration in minutes\n",
    "        {{ duration_minutes('pickup_datetime', 'dropoff_datetime') }} as trip_duration_minutes,\n",
    "\n",
    "        -- calculated: average speed (avoid division by zero)\n",
    "        case\n",
    "            when {{ duration_minutes('pickup_datetime', 'dropoff_datetime') }} > 0\n",
    "            then round(\n",
    "                trip_distance_miles / ({{ duration_minutes('pickup_datetime', 'dropoff_datetime') }} / 60.0),\n",
    "                2\n",
    "            )\n",
    "            else null\n",
    "        end as avg_speed_mph,\n",
    "\n",
    "        -- calculated: cost per mile\n",
    "        case\n",
    "            when trip_distance_miles > 0\n",
    "            then round(fare_amount / trip_distance_miles, 2)\n",
    "            else null\n",
    "        end as cost_per_mile,\n",
    "\n",
    "        -- calculated: tip percentage\n",
    "        case\n",
    "            when fare_amount > 0\n",
    "            then round((tip_amount / fare_amount) * 100, 2)\n",
    "            else null\n",
    "        end as tip_percentage,\n",
    "\n",
    "        -- time dimensions (using adapter-dispatched macros)\n",
    "        date_trunc('day', pickup_datetime)::date as pickup_date,\n",
    "        extract(hour from pickup_datetime) as pickup_hour,\n",
    "        {{ dayname_compat('pickup_datetime') }} as pickup_day_of_week,\n",
    "        case\n",
    "            when extract(dow from pickup_datetime) in (0, 6) then true\n",
    "            else false\n",
    "        end as is_weekend,\n",
    "\n",
    "        -- financials passthrough\n",
    "        fare_amount,\n",
    "        extra_amount,\n",
    "        mta_tax,\n",
    "        tip_amount,\n",
    "        tolls_amount,\n",
    "        improvement_surcharge,\n",
    "        total_amount,\n",
    "        congestion_surcharge,\n",
    "        airport_fee\n",
    "\n",
    "    from trips\n",
    ")\n",
    "\n",
    "select *\n",
    "from enriched\n",
    "where trip_duration_minutes between 1 and 720\n",
    "  and (avg_speed_mph is null or avg_speed_mph < 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 int_daily_summary.sql\n",
    "\n",
    "One row per day with aggregated counts, averages, and revenue totals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/intermediate/int_daily_summary.sql\n",
    "/*\n",
    "    Intermediate model: Daily aggregated trip and revenue metrics.\n",
    "    One row per day with counts, averages, and revenue totals.\n",
    "*/\n",
    "\n",
    "with trip_metrics as (\n",
    "    select * from {{ ref('int_trip_metrics') }}\n",
    "),\n",
    "\n",
    "daily_agg as (\n",
    "    select\n",
    "        pickup_date,\n",
    "        pickup_day_of_week,\n",
    "        is_weekend,\n",
    "\n",
    "        count(*) as total_trips,\n",
    "        sum(passenger_count) as total_passengers,\n",
    "\n",
    "        round(avg(trip_distance_miles), 2) as avg_trip_distance,\n",
    "        round(avg(trip_duration_minutes), 2) as avg_trip_duration_min,\n",
    "        round(avg(avg_speed_mph), 2) as avg_speed_mph,\n",
    "\n",
    "        round(sum(fare_amount), 2) as total_fare_revenue,\n",
    "        round(sum(tip_amount), 2) as total_tip_revenue,\n",
    "        round(sum(total_amount), 2) as total_revenue,\n",
    "        round(avg(total_amount), 2) as avg_trip_revenue,\n",
    "        round(avg(tip_percentage), 2) as avg_tip_percentage,\n",
    "\n",
    "        count(case when payment_type_id = 1 then 1 end) as credit_card_trips,\n",
    "        count(case when payment_type_id = 2 then 1 end) as cash_trips\n",
    "\n",
    "    from trip_metrics\n",
    "    group by pickup_date, pickup_day_of_week, is_weekend\n",
    ")\n",
    "\n",
    "select * from daily_agg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 int_hourly_patterns.sql\n",
    "\n",
    "One row per date+hour combination for demand analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/intermediate/int_hourly_patterns.sql\n",
    "/*\n",
    "    Intermediate model: Hourly trip patterns by date.\n",
    "    One row per date + hour combination.\n",
    "*/\n",
    "\n",
    "with trip_metrics as (\n",
    "    select * from {{ ref('int_trip_metrics') }}\n",
    "),\n",
    "\n",
    "hourly_agg as (\n",
    "    select\n",
    "        pickup_date,\n",
    "        pickup_hour,\n",
    "        pickup_day_of_week,\n",
    "        is_weekend,\n",
    "\n",
    "        count(*) as total_trips,\n",
    "        round(avg(trip_distance_miles), 2) as avg_distance,\n",
    "        round(avg(trip_duration_minutes), 2) as avg_duration_min,\n",
    "        round(sum(total_amount), 2) as total_revenue,\n",
    "        round(avg(total_amount), 2) as avg_revenue\n",
    "\n",
    "    from trip_metrics\n",
    "    group by pickup_date, pickup_hour, pickup_day_of_week, is_weekend\n",
    ")\n",
    "\n",
    "select * from hourly_agg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4 intermediate.yml (schema + tests)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/intermediate/intermediate.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: int_trip_metrics\n",
    "    description: \"Trip records enriched with calculated metrics.\"\n",
    "    columns:\n",
    "      - name: trip_id\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: trip_duration_minutes\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 1\n",
    "                max_value: 720\n",
    "      - name: pickup_date\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: pickup_hour\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 0\n",
    "                max_value: 23\n",
    "      - name: is_weekend\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: int_daily_summary\n",
    "    description: \"Daily aggregated trip counts, revenue, and average metrics\"\n",
    "    columns:\n",
    "      - name: pickup_date\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: total_trips\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 0\n",
    "      - name: total_revenue\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: int_hourly_patterns\n",
    "    description: \"Hourly trip aggregations by date\"\n",
    "    columns:\n",
    "      - name: pickup_date\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: pickup_hour\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 0\n",
    "                max_value: 23\n",
    "      - name: total_trips\n",
    "        tests:\n",
    "          - not_null"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. dbt Core Marts (Gold Layer - Facts & Dimensions)\n",
    "\n",
    "The core marts form the **Gold layer** — the final, query-ready tables for analytics.\n",
    "\n",
    "### Star Schema Design\n",
    "\n",
    "```\n",
    "               dim_dates\n",
    "                  │\n",
    "                  │\n",
    "dim_locations ─── fct_trips ─── dim_payment_types\n",
    "                  │\n",
    "                  │\n",
    "           dim_locations (dropoff)\n",
    "```\n",
    "\n",
    "### 15.1 fct_trips.sql\n",
    "\n",
    "The central **fact table**. Joins trip metrics with location dimensions.\n",
    "Uses `incremental` materialization with `delete+insert` strategy for efficient rebuilds."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/core/fct_trips.sql\n",
    "/*\n",
    "    Fact table: Fully enriched trip records with location names.\n",
    "    Incremental with delete+insert strategy.\n",
    "*/\n",
    "\n",
    "{{\n",
    "  config(\n",
    "    materialized='incremental',\n",
    "    unique_key='trip_id',\n",
    "    incremental_strategy='delete+insert',\n",
    "    on_schema_change='fail'\n",
    "  )\n",
    "}}\n",
    "\n",
    "with trip_metrics as (\n",
    "    select * from {{ ref('int_trip_metrics') }}\n",
    "),\n",
    "\n",
    "pickup_locations as (\n",
    "    select * from {{ ref('dim_locations') }}\n",
    "),\n",
    "\n",
    "dropoff_locations as (\n",
    "    select * from {{ ref('dim_locations') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        t.trip_id,\n",
    "        t.vendor_id,\n",
    "        t.rate_code_id,\n",
    "        t.payment_type_id,\n",
    "        t.pickup_location_id,\n",
    "        t.dropoff_location_id,\n",
    "        t.pickup_datetime,\n",
    "        t.dropoff_datetime,\n",
    "        t.pickup_date,\n",
    "        t.pickup_hour,\n",
    "        t.pickup_day_of_week,\n",
    "        t.is_weekend,\n",
    "        t.passenger_count,\n",
    "        t.trip_distance_miles,\n",
    "        t.trip_duration_minutes,\n",
    "        t.avg_speed_mph,\n",
    "        t.cost_per_mile,\n",
    "        t.fare_amount,\n",
    "        t.extra_amount,\n",
    "        t.mta_tax,\n",
    "        t.tip_amount,\n",
    "        t.tip_percentage,\n",
    "        t.tolls_amount,\n",
    "        t.improvement_surcharge,\n",
    "        t.total_amount,\n",
    "        t.congestion_surcharge,\n",
    "        t.airport_fee,\n",
    "\n",
    "        -- enriched from dimensions\n",
    "        pu.borough as pickup_borough,\n",
    "        pu.zone_name as pickup_zone,\n",
    "        do_loc.borough as dropoff_borough,\n",
    "        do_loc.zone_name as dropoff_zone\n",
    "\n",
    "    from trip_metrics t\n",
    "    left join pickup_locations pu\n",
    "        on t.pickup_location_id = pu.location_id\n",
    "    left join dropoff_locations do_loc\n",
    "        on t.dropoff_location_id = do_loc.location_id\n",
    "\n",
    "    {% if is_incremental() %}\n",
    "    where t.pickup_datetime > (select max(pickup_datetime) from {{ this }})\n",
    "    {% endif %}\n",
    ")\n",
    "\n",
    "select * from final"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 dim_dates.sql\n",
    "\n",
    "Date dimension for January 2024. Uses `dbt_utils.date_spine()` to generate all dates,\n",
    "then enriches with day-of-week, month name, weekend/holiday flags."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/core/dim_dates.sql\n",
    "/*\n",
    "    Dimension table: Calendar dates for January 2024.\n",
    "    Uses adapter-dispatched macros for dayname/monthname.\n",
    "*/\n",
    "\n",
    "with date_spine as (\n",
    "    {{ dbt_utils.date_spine(\n",
    "        datepart=\"day\",\n",
    "        start_date=\"cast('2024-01-01' as date)\",\n",
    "        end_date=\"cast('2024-02-01' as date)\"\n",
    "    ) }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        cast(date_day as date) as date_key,\n",
    "        extract(year from date_day) as year,\n",
    "        extract(month from date_day) as month,\n",
    "        extract(day from date_day) as day_of_month,\n",
    "        extract(dow from date_day) as day_of_week_num,\n",
    "        {{ dayname_compat('date_day') }} as day_of_week_name,\n",
    "        {{ monthname_compat('date_day') }} as month_name,\n",
    "        extract(week from date_day) as week_of_year,\n",
    "        case\n",
    "            when extract(dow from date_day) in (0, 6) then true\n",
    "            else false\n",
    "        end as is_weekend,\n",
    "        case\n",
    "            when cast(date_day as date) in (\n",
    "                cast('2024-01-01' as date),\n",
    "                cast('2024-01-15' as date)\n",
    "            ) then true\n",
    "            else false\n",
    "        end as is_holiday\n",
    "\n",
    "    from date_spine\n",
    ")\n",
    "\n",
    "select * from final"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3 dim_locations.sql\n",
    "\n",
    "Taxi zone dimension from seed data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/core/dim_locations.sql\n",
    "/*\n",
    "    Dimension table: TLC Taxi Zone locations.\n",
    "*/\n",
    "\n",
    "with zones as (\n",
    "    select * from {{ ref('stg_taxi_zones') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        location_id,\n",
    "        borough,\n",
    "        zone_name,\n",
    "        service_zone\n",
    "    from zones\n",
    ")\n",
    "\n",
    "select * from final"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4 dim_payment_types.sql\n",
    "\n",
    "Payment method dimension from seed data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/core/dim_payment_types.sql\n",
    "/*\n",
    "    Dimension table: Payment type descriptions.\n",
    "*/\n",
    "\n",
    "with payment_types as (\n",
    "    select * from {{ ref('stg_payment_types') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        payment_type_id,\n",
    "        payment_type_name\n",
    "    from payment_types\n",
    ")\n",
    "\n",
    "select * from final"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5 dim_vendors.sql"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/core/dim_vendors.sql",
    "/*",
    "    Dimension table: Taxi vendor descriptions.",
    "    TPEP provider: 1=Creative Mobile Technologies (CMT), 2=VeriFone Inc. (VFI)",
    "*/",
    "",
    "with vendors as (",
    "    select * from {{ ref('stg_vendors') }}",
    "),",
    "",
    "final as (",
    "    select",
    "        vendor_id,",
    "        vendor_name,",
    "        vendor_abbr",
    "    from vendors",
    ")",
    "",
    "select * from final",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5 core.yml (contracts + tests)\n",
    "\n",
    "Enforces **data contracts** on all core models — every column has a declared `data_type`.\n",
    "This catches schema drift at build time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/core/core.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: fct_trips\n",
    "    description: \"Fact table with fully enriched trip records.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: trip_id\n",
    "        data_type: varchar\n",
    "        tests: [not_null, unique]\n",
    "      - name: vendor_id\n",
    "        data_type: integer\n",
    "      - name: rate_code_id\n",
    "        data_type: integer\n",
    "      - name: payment_type_id\n",
    "        data_type: integer\n",
    "      - name: pickup_location_id\n",
    "        data_type: integer\n",
    "      - name: dropoff_location_id\n",
    "        data_type: integer\n",
    "      - name: pickup_datetime\n",
    "        data_type: timestamp\n",
    "        tests: [not_null]\n",
    "      - name: dropoff_datetime\n",
    "        data_type: timestamp\n",
    "      - name: pickup_date\n",
    "        data_type: date\n",
    "      - name: pickup_hour\n",
    "        data_type: bigint\n",
    "      - name: pickup_day_of_week\n",
    "        data_type: varchar\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "      - name: passenger_count\n",
    "        data_type: integer\n",
    "      - name: trip_distance_miles\n",
    "        data_type: double\n",
    "      - name: trip_duration_minutes\n",
    "        data_type: bigint\n",
    "      - name: avg_speed_mph\n",
    "        data_type: double\n",
    "      - name: cost_per_mile\n",
    "        data_type: double\n",
    "      - name: fare_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: extra_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: mta_tax\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: tip_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: tip_percentage\n",
    "        data_type: double\n",
    "      - name: tolls_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: improvement_surcharge\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: total_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "        tests: [not_null]\n",
    "      - name: congestion_surcharge\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: airport_fee\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: pickup_borough\n",
    "        data_type: varchar\n",
    "      - name: pickup_zone\n",
    "        data_type: varchar\n",
    "      - name: dropoff_borough\n",
    "        data_type: varchar\n",
    "      - name: dropoff_zone\n",
    "        data_type: varchar\n",
    "\n",
    "  - name: dim_locations\n",
    "    description: \"Location dimension\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: location_id\n",
    "        data_type: integer\n",
    "        tests: [unique, not_null]\n",
    "      - name: borough\n",
    "        data_type: varchar\n",
    "        tests: [not_null]\n",
    "      - name: zone_name\n",
    "        data_type: varchar\n",
    "        tests: [not_null]\n",
    "      - name: service_zone\n",
    "        data_type: varchar\n",
    "\n",
    "  - name: dim_dates\n",
    "    description: \"Date dimension for January 2024\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: date_key\n",
    "        data_type: date\n",
    "        tests: [unique, not_null]\n",
    "      - name: year\n",
    "        data_type: bigint\n",
    "      - name: month\n",
    "        data_type: bigint\n",
    "      - name: day_of_month\n",
    "        data_type: bigint\n",
    "      - name: day_of_week_num\n",
    "        data_type: bigint\n",
    "      - name: day_of_week_name\n",
    "        data_type: varchar\n",
    "        tests: [not_null]\n",
    "      - name: month_name\n",
    "        data_type: varchar\n",
    "      - name: week_of_year\n",
    "        data_type: bigint\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "        tests: [not_null]\n",
    "      - name: is_holiday\n",
    "        data_type: boolean\n",
    "        tests: [not_null]\n",
    "\n",
    "  - name: dim_payment_types\n",
    "    description: \"Payment type dimension\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: payment_type_id\n",
    "        data_type: integer\n",
    "        tests: [unique, not_null]\n",
    "      - name: payment_type_name\n",
    "        data_type: varchar\n",
    "        tests: [not_null]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. dbt Analytics Marts (Gold Layer - Business KPIs)\n",
    "\n",
    "Analytics marts are purpose-built aggregations for specific business questions.\n",
    "\n",
    "### 16.1 mart_daily_revenue.sql\n",
    "\n",
    "Daily revenue metrics with **running totals** and **day-over-day change**.\n",
    "Joins with `dim_dates` for calendar context (weekends, holidays)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/analytics/mart_daily_revenue.sql\n",
    "/*\n",
    "    Analytics mart: Daily revenue metrics with running totals.\n",
    "*/\n",
    "\n",
    "with daily as (\n",
    "    select * from {{ ref('int_daily_summary') }}\n",
    "),\n",
    "\n",
    "dates as (\n",
    "    select * from {{ ref('dim_dates') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        d.date_key,\n",
    "        d.day_of_week_name,\n",
    "        d.is_weekend,\n",
    "        d.is_holiday,\n",
    "        d.week_of_year,\n",
    "\n",
    "        daily.total_trips,\n",
    "        daily.total_passengers,\n",
    "        daily.total_fare_revenue,\n",
    "        daily.total_tip_revenue,\n",
    "        daily.total_revenue,\n",
    "        daily.avg_trip_revenue,\n",
    "        daily.avg_tip_percentage,\n",
    "        daily.credit_card_trips,\n",
    "        daily.cash_trips,\n",
    "        daily.avg_trip_distance,\n",
    "        daily.avg_trip_duration_min,\n",
    "\n",
    "        -- running total\n",
    "        sum(daily.total_revenue) over (order by d.date_key) as cumulative_revenue,\n",
    "\n",
    "        -- day-over-day change\n",
    "        daily.total_revenue - lag(daily.total_revenue) over (order by d.date_key) as revenue_change_vs_prior_day\n",
    "\n",
    "    from daily\n",
    "    inner join dates d\n",
    "        on daily.pickup_date = d.date_key\n",
    ")\n",
    "\n",
    "select * from final"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.2 mart_hourly_demand.sql\n",
    "\n",
    "Hourly demand patterns aggregated across all days. Answers: \"What's the average trip count at 8 AM on weekdays vs weekends?\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/analytics/mart_hourly_demand.sql\n",
    "/*\n",
    "    Analytics mart: Hourly demand patterns.\n",
    "*/\n",
    "\n",
    "with hourly as (\n",
    "    select * from {{ ref('int_hourly_patterns') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        pickup_hour,\n",
    "        is_weekend,\n",
    "\n",
    "        count(*) as days_observed,\n",
    "        round(avg(total_trips), 0) as avg_trips_per_period,\n",
    "        round(avg(avg_distance), 2) as avg_distance,\n",
    "        round(avg(avg_duration_min), 2) as avg_duration_min,\n",
    "        round(avg(total_revenue), 2) as avg_revenue_per_period,\n",
    "        sum(total_trips) as total_trips_all_days\n",
    "\n",
    "    from hourly\n",
    "    group by pickup_hour, is_weekend\n",
    ")\n",
    "\n",
    "select * from final\n",
    "order by is_weekend, pickup_hour"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3 mart_location_performance.sql\n",
    "\n",
    "Per-zone performance summary. Includes `mode()` for most common dropoff destination and peak hour."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/analytics/mart_location_performance.sql\n",
    "/*\n",
    "    Analytics mart: Location-level performance summary.\n",
    "    Uses adapter-dispatched mode_compat() for cross-dialect support.\n",
    "*/\n",
    "\n",
    "with trips as (\n",
    "    select * from {{ ref('fct_trips') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        pickup_location_id,\n",
    "        pickup_borough,\n",
    "        pickup_zone,\n",
    "\n",
    "        count(*) as total_pickups,\n",
    "        round(avg(trip_distance_miles), 2) as avg_trip_distance,\n",
    "        round(avg(trip_duration_minutes), 2) as avg_trip_duration_min,\n",
    "        round(sum(total_amount), 2) as total_revenue,\n",
    "        round(avg(total_amount), 2) as avg_revenue_per_trip,\n",
    "        round(avg(tip_percentage), 2) as avg_tip_pct,\n",
    "        round(avg(passenger_count), 2) as avg_passengers,\n",
    "\n",
    "        -- most common dropoff destination\n",
    "        {{ mode_compat('dropoff_zone') }} as most_common_dropoff_zone,\n",
    "\n",
    "        -- busiest hour\n",
    "        {{ mode_compat('pickup_hour') }} as peak_pickup_hour\n",
    "\n",
    "    from trips\n",
    "    where pickup_zone is not null\n",
    "    group by pickup_location_id, pickup_borough, pickup_zone\n",
    ")\n",
    "\n",
    "select * from final\n",
    "order by total_pickups desc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.4 analytics.yml (contracts + tests)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/models/marts/analytics/analytics.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: mart_daily_revenue\n",
    "    description: \"Daily revenue metrics with running totals.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: date_key\n",
    "        data_type: date\n",
    "        tests: [unique, not_null]\n",
    "      - name: day_of_week_name\n",
    "        data_type: varchar\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "      - name: is_holiday\n",
    "        data_type: boolean\n",
    "      - name: week_of_year\n",
    "        data_type: bigint\n",
    "      - name: total_trips\n",
    "        data_type: bigint\n",
    "      - name: total_passengers\n",
    "        data_type: hugeint\n",
    "      - name: total_fare_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: total_tip_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: total_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "        tests: [not_null]\n",
    "      - name: avg_trip_revenue\n",
    "        data_type: double\n",
    "      - name: avg_tip_percentage\n",
    "        data_type: double\n",
    "      - name: credit_card_trips\n",
    "        data_type: bigint\n",
    "      - name: cash_trips\n",
    "        data_type: bigint\n",
    "      - name: avg_trip_distance\n",
    "        data_type: double\n",
    "      - name: avg_trip_duration_min\n",
    "        data_type: double\n",
    "      - name: cumulative_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: revenue_change_vs_prior_day\n",
    "        data_type: \"decimal(38,2)\"\n",
    "\n",
    "  - name: mart_location_performance\n",
    "    description: \"Per-zone performance summary.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: pickup_location_id\n",
    "        data_type: integer\n",
    "        tests: [unique, not_null]\n",
    "      - name: pickup_borough\n",
    "        data_type: varchar\n",
    "      - name: pickup_zone\n",
    "        data_type: varchar\n",
    "      - name: total_pickups\n",
    "        data_type: bigint\n",
    "        tests: [not_null]\n",
    "      - name: avg_trip_distance\n",
    "        data_type: double\n",
    "      - name: avg_trip_duration_min\n",
    "        data_type: double\n",
    "      - name: total_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: avg_revenue_per_trip\n",
    "        data_type: double\n",
    "      - name: avg_tip_pct\n",
    "        data_type: double\n",
    "      - name: avg_passengers\n",
    "        data_type: double\n",
    "      - name: most_common_dropoff_zone\n",
    "        data_type: varchar\n",
    "      - name: peak_pickup_hour\n",
    "        data_type: bigint\n",
    "\n",
    "  - name: mart_hourly_demand\n",
    "    description: \"Hourly demand patterns.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: pickup_hour\n",
    "        data_type: bigint\n",
    "        tests: [not_null]\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "        tests: [not_null]\n",
    "      - name: days_observed\n",
    "        data_type: bigint\n",
    "      - name: avg_trips_per_period\n",
    "        data_type: double\n",
    "      - name: avg_distance\n",
    "        data_type: double\n",
    "      - name: avg_duration_min\n",
    "        data_type: double\n",
    "      - name: avg_revenue_per_period\n",
    "        data_type: double\n",
    "      - name: total_trips_all_days\n",
    "        data_type: hugeint"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. dbt Tests: Data Quality Assertions\n",
    "\n",
    "dbt tests come in two flavors:\n",
    "1. **Generic tests** (in YAML files): `unique`, `not_null`, `accepted_values`, `relationships`, `accepted_range`\n",
    "2. **Singular tests** (SQL files): Custom queries that return rows **only if there's a problem**\n",
    "\n",
    "### Test Summary: 91 tests across all layers\n",
    "\n",
    "| Layer | Tests | What They Check |\n",
    "|-------|-------|-----------------|\n",
    "| Staging | 32 | Uniqueness, nulls, accepted values, referential integrity |\n",
    "| Intermediate | 15 | Ranges (duration 1-720 min, hour 0-23), totals > 0 |\n",
    "| Core Marts | 24 | Data contracts (column types), key uniqueness |\n",
    "| Analytics | 12 | Aggregation integrity, non-null results |\n",
    "| Singular | 2 | fare ≤ total, duration ≥ 0 |\n",
    "| Seeds | 6 | Reference data integrity |\n",
    "\n",
    "### 17.1 assert_fare_not_exceeds_total.sql\n",
    "\n",
    "Fare amount should never exceed total amount (which includes tips, taxes, surcharges)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/tests/assert_fare_not_exceeds_total.sql\n",
    "/*\n",
    "    Singular test: fare_amount should not exceed total_amount.\n",
    "*/\n",
    "\n",
    "select\n",
    "    trip_id,\n",
    "    fare_amount,\n",
    "    total_amount\n",
    "from {{ ref('stg_yellow_trips') }}\n",
    "where fare_amount > total_amount + 0.01\n",
    "  and total_amount > 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.2 assert_trip_duration_positive.sql\n",
    "\n",
    "No trip should have a negative duration (dropoff before pickup)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/dbt_project/tests/assert_trip_duration_positive.sql\n",
    "/*\n",
    "    Singular test: No trip should have negative duration.\n",
    "*/\n",
    "\n",
    "select\n",
    "    trip_id,\n",
    "    pickup_datetime,\n",
    "    dropoff_datetime,\n",
    "    trip_duration_minutes\n",
    "from {{ ref('int_trip_metrics') }}\n",
    "where trip_duration_minutes < 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Pipeline Makefile: One-Command Orchestration\n",
    "\n",
    "The Makefile orchestrates the entire pipeline lifecycle with `make` commands:\n",
    "\n",
    "| Command | What It Does |\n",
    "|---------|-------------|\n",
    "| `make up` | Start all infrastructure services |\n",
    "| `make down` | Stop and remove volumes (includes Lakekeeper profile) |\n",
    "| `make create-topics` | Create Kafka topics (raw + DLQ) |\n",
    "| `make generate` | Produce taxi events to Kafka (burst mode) |\n",
    "| `make generate-limited` | Produce 10k events for testing |\n",
    "| `make process` | Submit Bronze + Silver Flink SQL jobs |\n",
    "| `make dbt-build` | Run dbt build with full-refresh |\n",
    "| `make benchmark` | Full E2E benchmark (down → up → process → dbt → down) |\n",
    "| `make health` | Quick health check of all services |\n",
    "| `make check-lag` | Show Kafka consumer group lag |\n",
    "| `make up-lakekeeper` | Start with Lakekeeper REST catalog |\n",
    "| `make process-rest` | Run Flink SQL via REST catalog |\n",
    "\n",
    "> **Windows note:** `MSYS_NO_PATHCONV=1` prefix on Docker commands prevents Git Bash\n",
    "> from converting Linux paths to Windows paths."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/Makefile\n",
    "SHELL := bash\n",
    "# =============================================================================\n",
    "# Pipeline 01: Kafka + Flink + Iceberg (Production-Grade Template)\n",
    "# =============================================================================\n",
    "# Makefile for orchestrating the complete streaming pipeline lifecycle.\n",
    "# =============================================================================\n",
    "\n",
    "COMPOSE = docker compose\n",
    "FLINK_SQL_CLIENT = MSYS_NO_PATHCONV=1 $(COMPOSE) exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded\n",
    "\n",
    ".PHONY: help up down generate create-topics process process-bronze process-silver \\\n",
    "        dbt-build benchmark logs status clean ps restart check-lag health\n",
    "\n",
    "help: ## Show this help\n",
    "\t@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | \\\n",
    "\t\tawk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-20s\\033[0m %s\\n\", $$1, $$2}'\n",
    "\n",
    "# =============================================================================\n",
    "# Lifecycle\n",
    "# =============================================================================\n",
    "\n",
    "up: ## Start all infrastructure services\n",
    "\t$(COMPOSE) up -d\n",
    "\t@echo \"\"\n",
    "\t@echo \"=== Pipeline 01: Kafka + Flink + Iceberg ===\"\n",
    "\t@echo \"Kafka:            localhost:9092\"\n",
    "\t@echo \"Schema Registry:  http://localhost:8085\"\n",
    "\t@echo \"Flink Dashboard:  http://localhost:8083\"\n",
    "\t@echo \"MinIO Console:    http://localhost:9001  (minioadmin/minioadmin)\"\n",
    "\t@echo \"\"\n",
    "\t@echo \"Next steps:\"\n",
    "\t@echo \"  make create-topics   # Create Kafka topics\"\n",
    "\t@echo \"  make generate        # Produce taxi events to Kafka\"\n",
    "\t@echo \"  make process         # Submit Flink SQL jobs\"\n",
    "\t@echo \"  make dbt-build       # Run dbt transformations\"\n",
    "\n",
    "down: ## Stop all services and remove volumes\n",
    "\t$(COMPOSE) --profile generator --profile dbt --profile lakekeeper down -v\n",
    "\t@echo \"Pipeline 01 stopped and volumes removed.\"\n",
    "\n",
    "clean: ## Stop everything and prune all related resources\n",
    "\t$(COMPOSE) --profile generator --profile dbt --profile lakekeeper down -v --remove-orphans\n",
    "\tdocker network rm p01-pipeline-net 2>/dev/null || true\n",
    "\t@echo \"Pipeline 01 fully cleaned.\"\n",
    "\n",
    "restart: ## Restart all services\n",
    "\t$(MAKE) down\n",
    "\t$(MAKE) up\n",
    "\n",
    "# =============================================================================\n",
    "# Topic Management\n",
    "# =============================================================================\n",
    "\n",
    "create-topics: ## Create Kafka topics (raw + DLQ)\n",
    "\t$(COMPOSE) exec kafka /opt/kafka/bin/kafka-topics.sh \\\n",
    "\t\t--bootstrap-server localhost:9092 \\\n",
    "\t\t--create \\\n",
    "\t\t--topic taxi.raw_trips \\\n",
    "\t\t--partitions 3 \\\n",
    "\t\t--replication-factor 1 \\\n",
    "\t\t--if-not-exists\n",
    "\t@echo \"Topic taxi.raw_trips created (3 partitions).\"\n",
    "\t$(COMPOSE) exec kafka /opt/kafka/bin/kafka-topics.sh \\\n",
    "\t\t--bootstrap-server localhost:9092 \\\n",
    "\t\t--create \\\n",
    "\t\t--topic taxi.raw_trips.dlq \\\n",
    "\t\t--partitions 1 \\\n",
    "\t\t--replication-factor 1 \\\n",
    "\t\t--if-not-exists\n",
    "\t@echo \"Topic taxi.raw_trips.dlq created (1 partition, dead letter queue).\"\n",
    "\n",
    "# =============================================================================\n",
    "# Data Generation\n",
    "# =============================================================================\n",
    "\n",
    "generate: ## Produce taxi trip events to Kafka (burst mode)\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm data-generator\n",
    "\t@echo \"Data generation complete.\"\n",
    "\n",
    "generate-limited: ## Produce limited events for testing (10k)\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm -e MAX_EVENTS=10000 data-generator\n",
    "\t@echo \"Limited data generation complete (10k events).\"\n",
    "\n",
    "# =============================================================================\n",
    "# Flink SQL Processing\n",
    "# =============================================================================\n",
    "\n",
    "process: process-bronze process-silver ## Submit all Flink SQL jobs (Bronze + Silver)\n",
    "\t@echo \"All Flink SQL jobs complete.\"\n",
    "\n",
    "process-bronze: ## Submit Bronze layer Flink SQL jobs (batch mode)\n",
    "\t@echo \"=== Bronze: Kafka → Iceberg ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/05-bronze.sql\n",
    "\t@echo \"Bronze layer complete.\"\n",
    "\n",
    "process-silver: ## Submit Silver layer Flink SQL jobs (batch mode)\n",
    "\t@echo \"=== Silver: Bronze → Cleaned Iceberg ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/06-silver.sql\n",
    "\t@echo \"Silver layer complete.\"\n",
    "\n",
    "# =============================================================================\n",
    "# dbt Transformations\n",
    "# =============================================================================\n",
    "\n",
    "dbt-build: ## Run dbt build (full-refresh) on Iceberg Silver data\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm --entrypoint /bin/sh dbt -c \"dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir .\"\n",
    "\t@echo \"dbt build complete.\"\n",
    "\n",
    "dbt-test: ## Run dbt tests only\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm dbt test --profiles-dir .\n",
    "\n",
    "dbt-docs: ## Generate dbt documentation\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm dbt docs generate --profiles-dir .\n",
    "\n",
    "# =============================================================================\n",
    "# Benchmark (Full E2E)\n",
    "# =============================================================================\n",
    "\n",
    "benchmark: ## Full end-to-end benchmark: down -> up → topics → generate → process → dbt → down\n",
    "\t@echo \"============================================================\"\n",
    "\t@echo \"  Pipeline 01 Benchmark: Kafka + Flink + Iceberg\"\n",
    "\t@echo \"============================================================\"\n",
    "\t@START_TIME=$$(date +%s) && \\\n",
    "\t$(MAKE) down 2>/dev/null || true && \\\n",
    "\t$(MAKE) up && \\\n",
    "\techo \"Waiting for services to stabilize...\" && \\\n",
    "\tsleep 15 && \\\n",
    "\t$(MAKE) create-topics && \\\n",
    "\t$(MAKE) generate && \\\n",
    "\techo \"Waiting for Flink processing to catch up...\" && \\\n",
    "\tsleep 10 && \\\n",
    "\t$(MAKE) process && \\\n",
    "\techo \"Waiting for streaming jobs to process data...\" && \\\n",
    "\tsleep 30 && \\\n",
    "\t$(MAKE) dbt-build && \\\n",
    "\tEND_TIME=$$(date +%s) && \\\n",
    "\tELAPSED=$$((END_TIME - START_TIME)) && \\\n",
    "\techo \"\" && \\\n",
    "\techo \"============================================================\" && \\\n",
    "\techo \"  BENCHMARK COMPLETE\" && \\\n",
    "\techo \"  Total elapsed: $${ELAPSED}s\" && \\\n",
    "\techo \"============================================================\" && \\\n",
    "\techo \"{\\\"pipeline\\\": \\\"01-kafka-flink-iceberg\\\", \\\"elapsed_seconds\\\": $$ELAPSED, \\\"timestamp\\\": \\\"$$(date -Iseconds)\\\"}\" > benchmark_results/latest.json && \\\n",
    "\techo \"Results saved to benchmark_results/latest.json\" && \\\n",
    "\t$(MAKE) down\n",
    "\n",
    "# =============================================================================\n",
    "# Observability\n",
    "# =============================================================================\n",
    "\n",
    "logs: ## Tail logs from all services\n",
    "\t$(COMPOSE) logs -f --tail=100\n",
    "\n",
    "logs-kafka: ## Tail Kafka logs\n",
    "\t$(COMPOSE) logs -f kafka\n",
    "\n",
    "logs-flink: ## Tail Flink JobManager logs\n",
    "\t$(COMPOSE) logs -f flink-jobmanager\n",
    "\n",
    "logs-flink-tm: ## Tail Flink TaskManager logs\n",
    "\t$(COMPOSE) logs -f flink-taskmanager\n",
    "\n",
    "status: ## Show service status\n",
    "\t@echo \"=== Pipeline 01: Service Status ===\"\n",
    "\t$(COMPOSE) ps\n",
    "\t@echo \"\"\n",
    "\t@echo \"=== Kafka Topics ===\"\n",
    "\t$(COMPOSE) exec kafka /opt/kafka/bin/kafka-topics.sh \\\n",
    "\t\t--bootstrap-server localhost:9092 --list 2>/dev/null || echo \"(Kafka not running)\"\n",
    "\t@echo \"\"\n",
    "\t@echo \"=== Flink Jobs ===\"\n",
    "\t@curl -s http://localhost:8083/jobs/overview 2>/dev/null | python3 -m json.tool 2>/dev/null || echo \"(Flink not running)\"\n",
    "\n",
    "ps: ## Show running containers\n",
    "\t$(COMPOSE) ps\n",
    "\n",
    "# =============================================================================\n",
    "# Health & Diagnostics\n",
    "# =============================================================================\n",
    "\n",
    "health: ## Quick health check of all services\n",
    "\t@echo \"=== Pipeline 01: Health Check ===\"\n",
    "\t@echo -n \"Kafka:           \" && $(COMPOSE) exec -T kafka /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 > /dev/null 2>&1 && echo \"OK\" || echo \"FAIL\"\n",
    "\t@echo -n \"Schema Registry: \" && curl -sf http://localhost:8085/subjects > /dev/null 2>&1 && echo \"OK\" || echo \"FAIL\"\n",
    "\t@echo -n \"MinIO:           \" && curl -sf http://localhost:9000/minio/health/live > /dev/null 2>&1 && echo \"OK\" || echo \"FAIL\"\n",
    "\t@echo -n \"Flink Dashboard: \" && curl -sf http://localhost:8083/overview > /dev/null 2>&1 && echo \"OK\" || echo \"FAIL\"\n",
    "\n",
    "check-lag: ## Show Kafka consumer group lag\n",
    "\t$(COMPOSE) exec kafka /opt/kafka/bin/kafka-consumer-groups.sh \\\n",
    "\t\t--bootstrap-server localhost:9092 \\\n",
    "\t\t--describe --group flink-consumer 2>/dev/null || echo \"(No active consumer group 'flink-consumer')\"\n",
    "\n",
    "# =============================================================================\n",
    "# Lakekeeper REST Catalog (opt-in)\n",
    "# =============================================================================\n",
    "\n",
    "up-lakekeeper: ## Start with Lakekeeper REST catalog\n",
    "\t$(COMPOSE) --profile lakekeeper up -d\n",
    "\t@echo \"\"\n",
    "\t@echo \"=== Pipeline 01: Kafka + Flink + Iceberg (Lakekeeper REST Catalog) ===\"\n",
    "\t@echo \"Lakekeeper UI:    http://localhost:8181\"\n",
    "\t@echo \"Flink Dashboard:  http://localhost:8083\"\n",
    "\t@echo \"\"\n",
    "\t@echo \"Use 'make process-rest' to run Flink SQL with REST catalog\"\n",
    "\n",
    "process-rest: process-bronze-rest process-silver-rest ## Submit Flink SQL via REST catalog\n",
    "\t@echo \"All Flink SQL jobs complete (REST catalog).\"\n",
    "\n",
    "process-bronze-rest: ## Submit Bronze layer via REST catalog\n",
    "\t@echo \"=== Bronze: Kafka → Iceberg (REST catalog) ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init-rest.sql -f /opt/flink/sql/05-bronze.sql\n",
    "\t@echo \"Bronze layer complete.\"\n",
    "\n",
    "process-silver-rest: ## Submit Silver layer via REST catalog\n",
    "\t@echo \"=== Silver: Bronze → Cleaned Iceberg (REST catalog) ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init-rest.sql -f /opt/flink/sql/06-silver.sql\n",
    "\t@echo \"Silver layer complete.\"\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Airflow DAGs: Production Scheduling\n",
    "\n",
    "These DAGs implement the **control plane** — scheduling dbt runs and Iceberg maintenance.\n",
    "\n",
    "> **Note:** These are reference implementations. Pipeline 01 runs Airflow via the\n",
    "> optional docker-compose services. For a dedicated orchestrated pipeline, see\n",
    "> Pipeline 08 (Airflow/Astronomer).\n",
    "\n",
    "### 19.1 Pipeline DAG (every 10 minutes)\n",
    "\n",
    "```\n",
    "check_flink_health\n",
    "    ├─ healthy → run_dbt → run_dbt_tests\n",
    "    └─ unhealthy → alert_flink_down\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/airflow/dags/taxi_pipeline_dag.py\n",
    "\"\"\"NYC Taxi Pipeline DAG - Production Orchestration.\n",
    "\n",
    "Runs every 10 minutes:\n",
    "  1. Check Flink cluster health\n",
    "  2. Run dbt build (Silver → Gold)\n",
    "  3. Run dbt tests (91 data quality assertions)\n",
    "  4. Alert if Flink is down\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['alerts@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'nyc_taxi_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='NYC Taxi Real-Time Pipeline Orchestration',\n",
    "    schedule_interval='*/10 * * * *',  # Every 10 minutes\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=['production', 'nyc-taxi', 'real-time'],\n",
    ")\n",
    "\n",
    "\n",
    "def check_flink_health(**context):\n",
    "    \"\"\"Check if Flink cluster is healthy and jobs are running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get('http://flink-jobmanager:8081/jobs/overview')\n",
    "        response.raise_for_status()\n",
    "        jobs = response.json()['jobs']\n",
    "        running_jobs = [j for j in jobs if j['state'] == 'RUNNING']\n",
    "        if not running_jobs:\n",
    "            raise ValueError(\"No running Flink jobs found\")\n",
    "        print(f\"Flink healthy: {len(running_jobs)} jobs running\")\n",
    "        return 'run_dbt'\n",
    "    except Exception as e:\n",
    "        print(f\"Flink health check failed: {e}\")\n",
    "        return 'alert_flink_down'\n",
    "\n",
    "\n",
    "# Task 1: Health check (branch based on result)\n",
    "health_check = BranchPythonOperator(\n",
    "    task_id='check_flink_health',\n",
    "    python_callable=check_flink_health,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Run dbt build (Silver → Gold)\n",
    "run_dbt = BashOperator(\n",
    "    task_id='run_dbt',\n",
    "    bash_command='cd /opt/airflow/dbt && dbt build --profiles-dir . --target prod',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Run dbt tests\n",
    "run_dbt_tests = BashOperator(\n",
    "    task_id='run_dbt_tests',\n",
    "    bash_command='cd /opt/airflow/dbt && dbt test --profiles-dir . --target prod',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 4: Alert if Flink is unhealthy\n",
    "alert_flink_down = BashOperator(\n",
    "    task_id='alert_flink_down',\n",
    "    bash_command='echo \"ALERT: Flink cluster unhealthy\" && exit 1',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Dependencies\n",
    "health_check >> [run_dbt, alert_flink_down]\n",
    "run_dbt >> run_dbt_tests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.2 Maintenance DAG (daily at 2 AM)\n",
    "\n",
    "```\n",
    "compact_silver → expire_snapshots → remove_orphan_files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/airflow/dags/iceberg_maintenance_dag.py\n",
    "\"\"\"Iceberg Maintenance DAG - Daily at 2 AM.\n",
    "\n",
    "Operations:\n",
    "  1. Compact Silver table (merge small files for query performance)\n",
    "  2. Expire old snapshots (cleanup metadata, keep last 5)\n",
    "  3. Remove orphan files (reclaim unreferenced storage)\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'iceberg_maintenance',\n",
    "    default_args=default_args,\n",
    "    description='Iceberg table maintenance (compaction, expiration)',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    tags=['maintenance', 'iceberg'],\n",
    ")\n",
    "\n",
    "compact_silver = BashOperator(\n",
    "    task_id='compact_silver_table',\n",
    "    bash_command=\"\"\"\n",
    "    docker exec p01-flink-jobmanager /opt/flink/bin/sql-client.sh \\\n",
    "      -i /opt/flink/sql/00-init.sql \\\n",
    "      -e \"CALL iceberg_catalog.system.rewrite_data_files(\n",
    "            table => 'nyc_taxi.silver.cleaned_trips',\n",
    "            strategy => 'sort',\n",
    "            sort_order => 'pickup_date,pickup_hour'\n",
    "          );\"\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "expire_snapshots = BashOperator(\n",
    "    task_id='expire_old_snapshots',\n",
    "    bash_command=\"\"\"\n",
    "    docker exec p01-flink-jobmanager /opt/flink/bin/sql-client.sh \\\n",
    "      -i /opt/flink/sql/00-init.sql \\\n",
    "      -e \"CALL iceberg_catalog.system.expire_snapshots(\n",
    "            table => 'nyc_taxi.silver.cleaned_trips',\n",
    "            older_than => CURRENT_TIMESTAMP - INTERVAL '7' DAY,\n",
    "            retain_last => 5\n",
    "          );\"\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "remove_orphans = BashOperator(\n",
    "    task_id='remove_orphan_files',\n",
    "    bash_command=\"\"\"\n",
    "    docker exec p01-flink-jobmanager /opt/flink/bin/sql-client.sh \\\n",
    "      -i /opt/flink/sql/00-init.sql \\\n",
    "      -e \"CALL iceberg_catalog.system.remove_orphan_files(\n",
    "            table => 'nyc_taxi.silver.cleaned_trips',\n",
    "            older_than => CURRENT_TIMESTAMP - INTERVAL '7' DAY\n",
    "          );\"\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "compact_silver >> expire_snapshots >> remove_orphans"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Running the Pipeline\n",
    "\n",
    "### Quick Start (Full Benchmark)\n",
    "\n",
    "```bash\n",
    "cd pipelines/01-kafka-flink-iceberg\n",
    "make benchmark\n",
    "```\n",
    "\n",
    "This runs the complete E2E flow: `down → up → create-topics → generate → process → dbt-build → down`\n",
    "\n",
    "### Step-by-Step Execution\n",
    "\n",
    "```bash\n",
    "# 1. Start infrastructure (Kafka, Flink, MinIO, Schema Registry)\n",
    "make up\n",
    "# Wait ~15-30s for all health checks to pass\n",
    "\n",
    "# 2. Verify all services are healthy\n",
    "make health\n",
    "# Expected: Kafka=OK, Schema Registry=OK, MinIO=OK, Flink Dashboard=OK\n",
    "\n",
    "# 3. Create Kafka topics (raw + DLQ)\n",
    "make create-topics\n",
    "# Creates taxi.raw_trips (3 partitions) + taxi.raw_trips.dlq (1 partition)\n",
    "\n",
    "# 4. Produce events\n",
    "make generate\n",
    "# Sends 10k events in burst mode with idempotent delivery\n",
    "\n",
    "# 5. Wait for Kafka to be fully written\n",
    "sleep 10\n",
    "\n",
    "# 6. Process Bronze layer (Kafka → Iceberg)\n",
    "make process-bronze\n",
    "# Runs: sql-client.sh -i 00-init.sql -f 05-bronze.sql\n",
    "\n",
    "# 7. Process Silver layer (Bronze → Silver with dedup)\n",
    "make process-silver\n",
    "# Runs: sql-client.sh -i 00-init.sql -f 06-silver.sql\n",
    "\n",
    "# 8. Run dbt (Silver → Gold, 94 tests)\n",
    "make dbt-build\n",
    "# Runs: dbt deps && dbt build --full-refresh\n",
    "\n",
    "# 9. Check consumer lag\n",
    "make check-lag\n",
    "\n",
    "# 10. Clean up\n",
    "make down\n",
    "```\n",
    "\n",
    "### With Lakekeeper REST Catalog (Optional)\n",
    "\n",
    "```bash\n",
    "# Start with REST catalog services\n",
    "make up-lakekeeper\n",
    "# Additional: Lakekeeper UI at http://localhost:8181\n",
    "\n",
    "# Process using REST catalog (no S3 creds in SQL)\n",
    "make process-rest\n",
    "\n",
    "# Same dbt-build step\n",
    "make dbt-build\n",
    "```\n",
    "\n",
    "### Monitoring During Execution\n",
    "\n",
    "| Service | URL | What to Check |\n",
    "|---------|-----|---------------|\n",
    "| **Flink Dashboard** | http://localhost:8083 | Running jobs, task metrics, backpressure |\n",
    "| **MinIO Console** | http://localhost:9001 | Iceberg data files in `warehouse` bucket |\n",
    "| **Schema Registry** | http://localhost:8085 | Registered schemas |\n",
    "| **Lakekeeper** | http://localhost:8181 | REST catalog (if `--profile lakekeeper`) |\n",
    "| **Prometheus** | port 9249 on Flink containers | Metrics scraping endpoint |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Production Operations\n",
    "\n",
    "### Performance Summary (Flink 2.0.1 + Iceberg 1.10.1)\n",
    "\n",
    "| Phase | Duration | What Happens |\n",
    "|-------|----------|-------------|\n",
    "| **Infrastructure startup** | 15-30s | Services healthy, buckets created |\n",
    "| **Ingestion** | 0.3-0.7s | 10k events to Kafka (idempotent, acks=all) |\n",
    "| **Bronze processing** | ~10s | Kafka → Iceberg (with watermarks) |\n",
    "| **Silver processing** | ~14s | Bronze → Silver (ROW_NUMBER dedup + filters) |\n",
    "| **dbt build** | ~21s | Silver → Gold (15 models, 94 tests) |\n",
    "| **Total E2E** | **~75s** | First event → Gold tables ready |\n",
    "\n",
    "### Defense-in-Depth Monitoring\n",
    "\n",
    "| Layer | What to Monitor | Alert Threshold |\n",
    "|-------|----------------|-----------------|\n",
    "| **Kafka** | Consumer lag (`make check-lag`) | lag > 10,000 events for > 5 min |\n",
    "| **Kafka** | DLQ message count | Any message in DLQ |\n",
    "| **Flink** | Backpressure (Dashboard) | > 10% for > 10 min |\n",
    "| **Flink** | Checkpoint duration | > 60s |\n",
    "| **Flink** | Prometheus metrics (port 9249) | Task failures > 0 |\n",
    "| **Iceberg** | File count per table | > 1,000 (needs compaction) |\n",
    "| **dbt** | Test results (94 expected) | Any FAIL or ERROR |\n",
    "| **dbt** | Source freshness | Exceeds `warn_after` threshold |\n",
    "\n",
    "### Scaling Considerations\n",
    "\n",
    "| Component | Horizontal | Vertical |\n",
    "|-----------|-----------|----------|\n",
    "| **Kafka** | Add partitions (must match Flink parallelism) | Increase broker memory |\n",
    "| **Flink** | Add task managers | Increase task slots, memory |\n",
    "| **MinIO** | Add nodes (distributed mode) | Increase disk |\n",
    "| **dbt** | Increase threads in profiles.yml | Increase DuckDB memory_limit |\n",
    "\n",
    "### Batch vs Streaming Mode\n",
    "\n",
    "The same SQL works in both modes — only the runtime setting changes:\n",
    "\n",
    "```sql\n",
    "-- Batch mode (default, for catch-up/backfill)\n",
    "SET 'execution.runtime-mode' = 'batch';\n",
    "SET 'table.dml-sync' = 'true';\n",
    "\n",
    "-- Streaming mode (continuous processing)\n",
    "SET 'execution.runtime-mode' = 'streaming';\n",
    "-- Do NOT set table.dml-sync (would block forever)\n",
    "```\n",
    "\n",
    "Use `07-streaming-bronze.sql` for continuous streaming. Use `05-bronze.sql` for batch catch-up.\n",
    "\n",
    "### Backfill Pattern\n",
    "\n",
    "```bash\n",
    "# 1. Reset Kafka consumer offset\n",
    "kafka-consumer-groups.sh --bootstrap-server localhost:9092 \\\n",
    "  --group flink-consumer --topic taxi.raw_trips \\\n",
    "  --reset-offsets --to-earliest --execute\n",
    "\n",
    "# 2. Re-run Flink processing (batch mode)\n",
    "make process\n",
    "\n",
    "# 3. Rebuild dbt from scratch\n",
    "make dbt-build\n",
    "```\n",
    "\n",
    "### Version Matrix (Validated)\n",
    "\n",
    "| Component | Version | JAR/Image |\n",
    "|-----------|---------|-----------|\n",
    "| Flink | 2.0.1 | `flink:2.0.1-java17` |\n",
    "| Iceberg | 1.10.1 | `iceberg-flink-runtime-2.0-1.10.1.jar` |\n",
    "| Kafka Connector | 4.0.1-2.0 | `flink-sql-connector-kafka-4.0.1-2.0.jar` |\n",
    "| Kafka | 4.0.0 | `apache/kafka:4.0.0` |\n",
    "| Hadoop Client | 3.3.6 | `hadoop-client-api/runtime-3.3.6.jar` |\n",
    "| AWS SDK | 1.12.367 | `aws-java-sdk-bundle-1.12.367.jar` |\n",
    "| Lakekeeper | 0.11.2 | `quay.io/lakekeeper/catalog:v0.11.2` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}