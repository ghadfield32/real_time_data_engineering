{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 01: Complete Production Walkthrough\n",
    "## Kafka â†’ Flink â†’ Iceberg â†’ dbt (with Airflow Orchestration)\n",
    "\n",
    "**Pipeline:** P01 - Kafka + Flink + Iceberg  \n",
    "**Status:** âœ… Production-Ready (91/91 tests passing)  \n",
    "**Performance:** 107s E2E, 7.7GB memory  \n",
    "**Use Case:** Industry-standard real-time data pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š What You'll Learn\n",
    "\n",
    "This notebook provides a **complete end-to-end walkthrough** of the production-grade real-time data pipeline:\n",
    "\n",
    "1. **Architecture & Responsibilities** - What each layer owns\n",
    "2. **Infrastructure Setup** - Docker services configuration\n",
    "3. **Kafka Layer** - Event ingestion and buffering\n",
    "4. **Flink Layer** - Real-time transformation (Bronze â†’ Silver)\n",
    "5. **Iceberg Layer** - ACID table storage\n",
    "6. **dbt Layer** - Analytics modeling (Silver â†’ Gold)\n",
    "7. **Airflow Layer** - Production orchestration\n",
    "8. **End-to-End Workflow** - How everything connects\n",
    "9. **Production Operations** - Maintenance, monitoring, scaling\n",
    "\n",
    "By the end, you'll understand exactly how to build, deploy, and operate a production streaming data platform.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview\n",
    "\n",
    "### ðŸ—ï¸ Pipeline Layers & Responsibilities\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          DATA PLANE (Always-On)                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  [Data Generator] â†’ [Kafka Topics] â†’ [Flink Jobs] â†’ [Iceberg]     â”‚\n",
    "â”‚       (burst)         (Bronze in      (Bronzeâ†’Silver  (Tables at   â”‚\n",
    "â”‚                       motion)          transformations) rest)       â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     CONTROL PLANE (Scheduled)                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  [Airflow DAG]                                                     â”‚\n",
    "â”‚    â”œâ”€ Check Flink Health                                           â”‚\n",
    "â”‚    â”œâ”€ Run dbt (Silver â†’ Gold)                                      â”‚\n",
    "â”‚    â”œâ”€ Run dbt tests                                                â”‚\n",
    "â”‚    â”œâ”€ Iceberg Compaction                                           â”‚\n",
    "â”‚    â””â”€ Snapshot Expiration                                          â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ðŸ“¦ What Each Layer Owns\n",
    "\n",
    "| Layer | Owns | Doesn't Own | Runs |\n",
    "|-------|------|-------------|------|\n",
    "| **Kafka** | Event buffering, fan-out, ordering per partition, replay | Business transforms, aggregations | Always-on service |\n",
    "| **Flink** | Real-time parsing, validation, enrichment, stateful ops, event-time semantics | Full semantic modeling, batch reports | Long-running jobs |\n",
    "| **Iceberg** | ACID tables, snapshots, schema evolution, concurrent readers/writers | Query execution, transformations | Storage layer |\n",
    "| **dbt** | Dimensional modeling, KPIs, tests, documentation, Gold tables | Sub-second streaming, stateful joins | Scheduled runs |\n",
    "| **Airflow** | Scheduling, retries, alerts, dependencies, maintenance jobs | Data processing itself | Control plane |\n",
    "\n",
    "### ðŸŽ¯ Key Design Principle\n",
    "\n",
    "**Data Plane (streaming) runs continuously.**  \n",
    "**Control Plane (orchestration) runs on schedule.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Infrastructure Setup\n",
    "\n",
    "### ðŸ“‹ Required Services (10 containers)\n",
    "\n",
    "The pipeline consists of 10 Docker services working together:\n",
    "\n",
    "```yaml\n",
    "# Core Services (Always-On)\n",
    "- kafka                  # Event backbone (KRaft mode, no Zookeeper)\n",
    "- schema-registry        # Avro schema management\n",
    "- flink-jobmanager       # Flink cluster coordinator\n",
    "- flink-taskmanager      # Flink worker nodes\n",
    "- minio                  # S3-compatible object storage\n",
    "- mc-init                # MinIO bucket initialization\n",
    "\n",
    "# Processing Services (On-Demand)\n",
    "- data-generator         # Parquet â†’ Kafka producer\n",
    "- dbt                    # Analytics transformations\n",
    "\n",
    "# Control Plane (if using Airflow)\n",
    "- airflow-webserver      # Airflow UI\n",
    "- airflow-scheduler      # DAG orchestration\n",
    "```\n",
    "\n",
    "Let's examine the actual `docker-compose.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pipelines/01-kafka-flink-iceberg/docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pipelines/01-kafka-flink-iceberg/docker-compose.yml\n",
    "# Pipeline 01: Kafka + Flink + Iceberg\n",
    "# Production-grade real-time data pipeline\n",
    "\n",
    "version: '3.8'\n",
    "\n",
    "networks:\n",
    "  pipeline-net:\n",
    "    name: p01-pipeline-net\n",
    "\n",
    "volumes:\n",
    "  minio-data:\n",
    "  flink-checkpoints:\n",
    "\n",
    "services:\n",
    "  # ============================================================================\n",
    "  # KAFKA: Event Backbone (Bronze \"in motion\")\n",
    "  # ============================================================================\n",
    "  kafka:\n",
    "    image: apache/kafka:latest\n",
    "    container_name: p01-kafka\n",
    "    hostname: kafka\n",
    "    ports:\n",
    "      - \"9092:9092\"  # External access\n",
    "    environment:\n",
    "      # KRaft mode (no Zookeeper)\n",
    "      KAFKA_NODE_ID: 1\n",
    "      KAFKA_PROCESS_ROLES: broker,controller\n",
    "      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092\n",
    "      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\n",
    "      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n",
    "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n",
    "      KAFKA_NUM_PARTITIONS: 3\n",
    "      # Retention: 7 days (production: tune based on use case)\n",
    "      KAFKA_LOG_RETENTION_HOURS: 168\n",
    "    healthcheck:\n",
    "      test: [\n",
    "        \"CMD-SHELL\",\n",
    "        \"/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092\"\n",
    "      ]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ============================================================================\n",
    "  # SCHEMA REGISTRY: Contract Management\n",
    "  # ============================================================================\n",
    "  schema-registry:\n",
    "    image: confluentinc/cp-schema-registry:7.6.0\n",
    "    container_name: p01-schema-registry\n",
    "    hostname: schema-registry\n",
    "    depends_on:\n",
    "      kafka:\n",
    "        condition: service_healthy\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
    "      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092\n",
    "      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8081/\"]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ============================================================================\n",
    "  # MINIO: S3-Compatible Object Storage (Iceberg backend)\n",
    "  # ============================================================================\n",
    "  minio:\n",
    "    image: minio/minio:latest\n",
    "    container_name: p01-minio\n",
    "    hostname: minio\n",
    "    ports:\n",
    "      - \"9000:9000\"  # API\n",
    "      - \"9001:9001\"  # Console\n",
    "    environment:\n",
    "      MINIO_ROOT_USER: minioadmin\n",
    "      MINIO_ROOT_PASSWORD: minioadmin\n",
    "    command: server /data --console-address \":9001\"\n",
    "    volumes:\n",
    "      - minio-data:/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # Initialize MinIO buckets\n",
    "  mc-init:\n",
    "    image: minio/mc:latest\n",
    "    container_name: p01-mc-init\n",
    "    depends_on:\n",
    "      minio:\n",
    "        condition: service_healthy\n",
    "    entrypoint: [\"/bin/sh\", \"-c\"]\n",
    "    command:\n",
    "      - |\n",
    "        mc alias set myminio http://minio:9000 minioadmin minioadmin\n",
    "        mc mb myminio/warehouse --ignore-existing\n",
    "        mc mb myminio/checkpoints --ignore-existing\n",
    "        echo \"MinIO buckets created: warehouse, checkpoints\"\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ============================================================================\n",
    "  # FLINK: Real-Time Compute (Bronze â†’ Silver transformations)\n",
    "  # ============================================================================\n",
    "  flink-jobmanager:\n",
    "    build:\n",
    "      context: ../../shared/docker\n",
    "      dockerfile: flink.Dockerfile\n",
    "    container_name: p01-flink-jobmanager\n",
    "    hostname: flink-jobmanager\n",
    "    ports:\n",
    "      - \"8081:8081\"  # Web UI\n",
    "    command: jobmanager\n",
    "    environment:\n",
    "      - FLINK_PROPERTIES=\n",
    "          jobmanager.rpc.address: flink-jobmanager\n",
    "          state.backend: rocksdb\n",
    "          state.checkpoints.dir: s3://checkpoints/\n",
    "          s3.endpoint: http://minio:9000\n",
    "          s3.access-key: minioadmin\n",
    "          s3.secret-key: minioadmin\n",
    "          s3.path.style.access: true\n",
    "          classloader.check-leaked-classloader: false\n",
    "    volumes:\n",
    "      - ./flink:/opt/flink/sql\n",
    "      - flink-checkpoints:/tmp/flink-checkpoints\n",
    "    depends_on:\n",
    "      - kafka\n",
    "      - minio\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  flink-taskmanager:\n",
    "    build:\n",
    "      context: ../../shared/docker\n",
    "      dockerfile: flink.Dockerfile\n",
    "    container_name: p01-flink-taskmanager\n",
    "    hostname: flink-taskmanager\n",
    "    command: taskmanager\n",
    "    environment:\n",
    "      - FLINK_PROPERTIES=\n",
    "          jobmanager.rpc.address: flink-jobmanager\n",
    "          taskmanager.numberOfTaskSlots: 4\n",
    "          state.backend: rocksdb\n",
    "          state.checkpoints.dir: s3://checkpoints/\n",
    "          s3.endpoint: http://minio:9000\n",
    "          s3.access-key: minioadmin\n",
    "          s3.secret-key: minioadmin\n",
    "          s3.path.style.access: true\n",
    "          classloader.check-leaked-classloader: false\n",
    "    volumes:\n",
    "      - ./flink:/opt/flink/sql\n",
    "      - flink-checkpoints:/tmp/flink-checkpoints\n",
    "    depends_on:\n",
    "      - flink-jobmanager\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ============================================================================\n",
    "  # DATA GENERATOR: Parquet â†’ Kafka Producer\n",
    "  # ============================================================================\n",
    "  data-generator:\n",
    "    profiles: [\"generator\"]\n",
    "    image: python:3.12-slim\n",
    "    container_name: p01-data-generator\n",
    "    working_dir: /app\n",
    "    volumes:\n",
    "      - ../../shared/data-generator:/app\n",
    "      - ../../data:/data:ro\n",
    "    environment:\n",
    "      BROKER_URL: kafka:9092\n",
    "      TOPIC: taxi.raw_trips\n",
    "      MODE: burst\n",
    "      MAX_EVENTS: \"10000\"\n",
    "    command: [\"sh\", \"-c\", \"pip install -q -r requirements.txt && python generator.py\"]\n",
    "    depends_on:\n",
    "      kafka:\n",
    "        condition: service_healthy\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ============================================================================\n",
    "  # DBT: Analytics Transformations (Silver â†’ Gold)\n",
    "  # ============================================================================\n",
    "  dbt:\n",
    "    profiles: [\"dbt\"]\n",
    "    build:\n",
    "      context: ../../shared/docker\n",
    "      dockerfile: dbt.Dockerfile\n",
    "    container_name: p01-dbt\n",
    "    working_dir: /dbt\n",
    "    volumes:\n",
    "      - ./dbt_project:/dbt\n",
    "    environment:\n",
    "      DBT_PROFILES_DIR: /dbt\n",
    "    entrypoint: [\"/bin/sh\", \"-c\"]\n",
    "    command: [\"dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir .\"]\n",
    "    depends_on:\n",
    "      - minio\n",
    "    networks:\n",
    "      - pipeline-net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Understanding the Service Dependencies\n",
    "\n",
    "```\n",
    "minio (S3 storage)\n",
    "  â†“\n",
    "mc-init (create buckets)\n",
    "  â†“\n",
    "kafka (event backbone)\n",
    "  â†“\n",
    "schema-registry (contracts)\n",
    "  â†“\n",
    "flink-jobmanager (coordinator)\n",
    "  â†“\n",
    "flink-taskmanager (workers)\n",
    "  â†“\n",
    "[data-generator] â†’ [Flink jobs] â†’ [Iceberg tables] â†’ [dbt]\n",
    "```\n",
    "\n",
    "### ðŸš€ Starting the Infrastructure\n",
    "\n",
    "```bash\n",
    "cd pipelines/01-kafka-flink-iceberg\n",
    "make up\n",
    "```\n",
    "\n",
    "**Wait for all services to be healthy (~15-30 seconds):**\n",
    "- âœ… Kafka broker ready\n",
    "- âœ… MinIO buckets created\n",
    "- âœ… Flink cluster running (check http://localhost:8081)\n",
    "- âœ… Schema Registry available (http://localhost:8081)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kafka Layer: Event Ingestion\n",
    "\n",
    "### ðŸ“¨ What Kafka Owns in This Pipeline\n",
    "\n",
    "- **Durable event buffering** (7-day retention)\n",
    "- **Fan-out** to multiple consumers (Flink, monitoring, audit)\n",
    "- **Ordering guarantee** per partition\n",
    "- **Replay capability** for reprocessing/backfills\n",
    "- **Pressure gauge** via consumer lag metrics\n",
    "\n",
    "### ðŸ“‹ Topic Design\n",
    "\n",
    "```bash\n",
    "# Create the raw events topic\n",
    "kafka-topics.sh --create \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic taxi.raw_trips \\\n",
    "  --partitions 3 \\\n",
    "  --replication-factor 1 \\\n",
    "  --config retention.ms=604800000  # 7 days\n",
    "```\n",
    "\n",
    "**Why 3 partitions?**\n",
    "- Parallelism = 3 Flink tasks can consume concurrently\n",
    "- Production: scale based on throughput (start with consumer count)\n",
    "\n",
    "### ðŸ“Š Schema Definition (Avro)\n",
    "\n",
    "Located at: `shared/schemas/taxi_trip.avsc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m {\n\u001b[32m      2\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrecord\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTaxiTrip\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mnamespace\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcom.nyc.taxi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mfields\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mVendorID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mnull\u001b[49m},\n\u001b[32m      7\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtpep_pickup_datetime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m      8\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtpep_dropoff_datetime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m      9\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpassenger_count\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     10\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtrip_distance\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     11\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRatecodeID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     12\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mstore_and_fwd_flag\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     13\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPULocationID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     14\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDOLocationID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     15\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpayment_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     16\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfare_amount\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     17\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mextra\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     18\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmta_tax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     19\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtip_amount\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     20\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtolls_amount\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     21\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mimprovement_surcharge\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     22\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtotal_amount\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     23\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcongestion_surcharge\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null},\n\u001b[32m     24\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAirport_fee\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m: null}\n\u001b[32m     25\u001b[39m   ]\n\u001b[32m     26\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'null' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"TaxiTrip\",\n",
    "  \"namespace\": \"com.nyc.taxi\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"VendorID\", \"type\": [\"null\", \"long\"], \"default\": null},\n",
    "    {\"name\": \"tpep_pickup_datetime\", \"type\": [\"null\", \"string\"], \"default\": null},\n",
    "    {\"name\": \"tpep_dropoff_datetime\", \"type\": [\"null\", \"string\"], \"default\": null},\n",
    "    {\"name\": \"passenger_count\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"trip_distance\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"RatecodeID\", \"type\": [\"null\", \"long\"], \"default\": null},\n",
    "    {\"name\": \"store_and_fwd_flag\", \"type\": [\"null\", \"string\"], \"default\": null},\n",
    "    {\"name\": \"PULocationID\", \"type\": [\"null\", \"long\"], \"default\": null},\n",
    "    {\"name\": \"DOLocationID\", \"type\": [\"null\", \"long\"], \"default\": null},\n",
    "    {\"name\": \"payment_type\", \"type\": [\"null\", \"long\"], \"default\": null},\n",
    "    {\"name\": \"fare_amount\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"extra\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"mta_tax\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"tip_amount\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"tolls_amount\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"improvement_surcharge\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"total_amount\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"congestion_surcharge\", \"type\": [\"null\", \"double\"], \"default\": null},\n",
    "    {\"name\": \"Airport_fee\", \"type\": [\"null\", \"double\"], \"default\": null}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”„ Data Producer (Burst Mode)\n",
    "\n",
    "Located at: `shared/data-generator/generator.py`\n",
    "\n",
    "**Key Features:**\n",
    "- Reads from parquet file (`data/yellow_tripdata_2024-01.parquet`)\n",
    "- Serializes to JSON (production: use Avro for efficiency)\n",
    "- Three modes: `burst` (max speed), `realtime` (event-time pacing), `batch` (configurable chunks)\n",
    "- Reports throughput (events/sec)\n",
    "\n",
    "```python\n",
    "# Simplified producer logic\n",
    "from confluent_kafka import Producer\n",
    "import pyarrow.parquet as pq\n",
    "import orjson\n",
    "\n",
    "def produce_events(path, broker, topic, max_events=10000):\n",
    "    producer = Producer({'bootstrap.servers': broker})\n",
    "    table = pq.read_table(path)\n",
    "    \n",
    "    for i, row in enumerate(table.to_pylist()):\n",
    "        if i >= max_events:\n",
    "            break\n",
    "        \n",
    "        # Serialize to JSON\n",
    "        message = orjson.dumps(row)\n",
    "        \n",
    "        # Produce to Kafka\n",
    "        producer.produce(\n",
    "            topic,\n",
    "            value=message,\n",
    "            callback=delivery_callback\n",
    "        )\n",
    "        \n",
    "        # Poll for delivery reports\n",
    "        producer.poll(0)\n",
    "    \n",
    "    # Wait for all messages to be delivered\n",
    "    producer.flush()\n",
    "```\n",
    "\n",
    "### â–¶ï¸ Running the Producer\n",
    "\n",
    "```bash\n",
    "# Create topic first\n",
    "make create-topics\n",
    "\n",
    "# Produce 10k events in burst mode\n",
    "make generate\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "============================================================\n",
    "  Taxi Trip Event Generator\n",
    "============================================================\n",
    "  Broker:     kafka:9092\n",
    "  Topic:      taxi.raw_trips\n",
    "  Mode:       burst\n",
    "  Max events: 10,000\n",
    "\n",
    "  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows)\n",
    "\n",
    "============================================================\n",
    "  GENERATOR COMPLETE\n",
    "  Events:  10,000\n",
    "  Elapsed: 0.32s\n",
    "  Rate:    31,250 events/sec\n",
    "============================================================\n",
    "```\n",
    "\n",
    "### ðŸ“Š Monitoring Kafka\n",
    "\n",
    "```bash\n",
    "# Check topic exists\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "\n",
    "# Check message count\n",
    "kafka-run-class.sh kafka.tools.GetOffsetShell \\\n",
    "  --broker-list localhost:9092 \\\n",
    "  --topic taxi.raw_trips \\\n",
    "  --time -1\n",
    "\n",
    "# Consumer lag (should be 0 once Flink catches up)\n",
    "kafka-consumer-groups.sh --bootstrap-server localhost:9092 \\\n",
    "  --group flink-consumer --describe\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Flink Layer: Real-Time Transformations\n",
    "\n",
    "### ðŸ”¥ What Flink Owns\n",
    "\n",
    "- **Event-time processing** (watermarks, late data handling)\n",
    "- **Stateful operations** (deduplication, joins, windowing)\n",
    "- **Exactly-once semantics** (via checkpoints)\n",
    "- **Bronze â†’ Silver transformation** (parsing, validation, enrichment)\n",
    "- **Iceberg sink** (ACID writes to S3/MinIO)\n",
    "\n",
    "### ðŸ“‚ Flink SQL Files\n",
    "\n",
    "Located in: `pipelines/01-kafka-flink-iceberg/flink/sql/`\n",
    "\n",
    "```\n",
    "00-init.sql         # Catalog + config setup\n",
    "05-bronze.sql       # Kafka â†’ Iceberg Bronze\n",
    "06-silver.sql       # Bronze â†’ Silver transformations\n",
    "```\n",
    "\n",
    "### 1ï¸âƒ£ Initialization (00-init.sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- Flink SQL Initialization\n",
    "-- Creates Iceberg catalog + Kafka connector configs\n",
    "-- ============================================================================\n",
    "\n",
    "-- Create Iceberg catalog (Hadoop-based, MinIO backend)\n",
    "CREATE CATALOG iceberg_catalog WITH (\n",
    "  'type' = 'iceberg',\n",
    "  'catalog-type' = 'hadoop',\n",
    "  'warehouse' = 's3://warehouse',\n",
    "  'property-version' = '1',\n",
    "  's3.endpoint' = 'http://minio:9000',\n",
    "  's3.path-style-access' = 'true',\n",
    "  's3.access-key-id' = 'minioadmin',\n",
    "  's3.secret-access-key' = 'minioadmin'\n",
    ");\n",
    "\n",
    "-- Set as default catalog\n",
    "USE CATALOG iceberg_catalog;\n",
    "\n",
    "-- Create database for NYC Taxi data\n",
    "CREATE DATABASE IF NOT EXISTS nyc_taxi;\n",
    "USE nyc_taxi;\n",
    "\n",
    "-- Set execution mode (batch for historical processing)\n",
    "SET 'execution.runtime-mode' = 'batch';\n",
    "\n",
    "-- Checkpoint configuration (for streaming jobs)\n",
    "SET 'execution.checkpointing.interval' = '60s';\n",
    "SET 'state.backend' = 'rocksdb';\n",
    "SET 'state.checkpoints.dir' = 's3://checkpoints/';\n",
    "\n",
    "-- Enable exactly-once semantics\n",
    "SET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';\n",
    "\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ Bronze Layer: Kafka â†’ Iceberg (05-bronze.sql)\n",
    "\n",
    "**Objective:** Ingest raw events from Kafka into Iceberg with minimal transformation.\n",
    "\n",
    "**What Bronze Does:**\n",
    "- âœ… Preserves raw structure\n",
    "- âœ… Adds processing metadata (ingestion timestamp)\n",
    "- âœ… Type casting from JSON strings\n",
    "- âŒ No business logic\n",
    "- âŒ No deduplication\n",
    "- âŒ No validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- Bronze Layer: Kafka â†’ Iceberg (Raw Ingestion)\n",
    "-- ============================================================================\n",
    "\n",
    "-- Step 1: Create Kafka source table (virtual view over Kafka topic)\n",
    "CREATE TEMPORARY TABLE kafka_raw_trips (\n",
    "  VendorID BIGINT,\n",
    "  tpep_pickup_datetime STRING,\n",
    "  tpep_dropoff_datetime STRING,\n",
    "  passenger_count DOUBLE,\n",
    "  trip_distance DOUBLE,\n",
    "  RatecodeID BIGINT,\n",
    "  store_and_fwd_flag STRING,\n",
    "  PULocationID BIGINT,\n",
    "  DOLocationID BIGINT,\n",
    "  payment_type BIGINT,\n",
    "  fare_amount DOUBLE,\n",
    "  extra DOUBLE,\n",
    "  mta_tax DOUBLE,\n",
    "  tip_amount DOUBLE,\n",
    "  tolls_amount DOUBLE,\n",
    "  improvement_surcharge DOUBLE,\n",
    "  total_amount DOUBLE,\n",
    "  congestion_surcharge DOUBLE,\n",
    "  Airport_fee DOUBLE\n",
    ") WITH (\n",
    "  'connector' = 'kafka',\n",
    "  'topic' = 'taxi.raw_trips',\n",
    "  'properties.bootstrap.servers' = 'kafka:9092',\n",
    "  'properties.group.id' = 'flink-bronze-consumer',\n",
    "  'scan.startup.mode' = 'earliest-offset',\n",
    "  'format' = 'json',\n",
    "  'json.fail-on-missing-field' = 'false',\n",
    "  'json.ignore-parse-errors' = 'true'\n",
    ");\n",
    "\n",
    "-- Step 2: Create Iceberg Bronze table (ACID storage)\n",
    "CREATE TABLE IF NOT EXISTS bronze.raw_trips (\n",
    "  -- Original fields (preserved as-is)\n",
    "  VendorID BIGINT,\n",
    "  tpep_pickup_datetime STRING,\n",
    "  tpep_dropoff_datetime STRING,\n",
    "  passenger_count DOUBLE,\n",
    "  trip_distance DOUBLE,\n",
    "  RatecodeID BIGINT,\n",
    "  store_and_fwd_flag STRING,\n",
    "  PULocationID BIGINT,\n",
    "  DOLocationID BIGINT,\n",
    "  payment_type BIGINT,\n",
    "  fare_amount DOUBLE,\n",
    "  extra DOUBLE,\n",
    "  mta_tax DOUBLE,\n",
    "  tip_amount DOUBLE,\n",
    "  tolls_amount DOUBLE,\n",
    "  improvement_surcharge DOUBLE,\n",
    "  total_amount DOUBLE,\n",
    "  congestion_surcharge DOUBLE,\n",
    "  Airport_fee DOUBLE,\n",
    "  \n",
    "  -- Metadata (added by pipeline)\n",
    "  ingested_at TIMESTAMP(3)\n",
    ") WITH (\n",
    "  'write.format.default' = 'parquet',\n",
    "  'write.target-file-size-bytes' = '134217728',  -- 128MB files\n",
    "  'commit.manifest.min-count-to-merge' = '2'\n",
    ");\n",
    "\n",
    "-- Step 3: Insert from Kafka to Iceberg\n",
    "INSERT INTO bronze.raw_trips\n",
    "SELECT \n",
    "  VendorID,\n",
    "  tpep_pickup_datetime,\n",
    "  tpep_dropoff_datetime,\n",
    "  passenger_count,\n",
    "  trip_distance,\n",
    "  RatecodeID,\n",
    "  store_and_fwd_flag,\n",
    "  PULocationID,\n",
    "  DOLocationID,\n",
    "  payment_type,\n",
    "  fare_amount,\n",
    "  extra,\n",
    "  mta_tax,\n",
    "  tip_amount,\n",
    "  tolls_amount,\n",
    "  improvement_surcharge,\n",
    "  total_amount,\n",
    "  congestion_surcharge,\n",
    "  Airport_fee,\n",
    "  CURRENT_TIMESTAMP AS ingested_at  -- Processing timestamp\n",
    "FROM kafka_raw_trips;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ Silver Layer: Data Quality & Enrichment (06-silver.sql)\n",
    "\n",
    "**Objective:** Transform Bronze into clean, conformed, enriched data.\n",
    "\n",
    "**What Silver Does:**\n",
    "- âœ… Column renaming (snake_case)\n",
    "- âœ… Type standardization (DECIMAL for money)\n",
    "- âœ… Data quality filters (nulls, negatives, ranges)\n",
    "- âœ… Business logic (trip_id, duration calculation)\n",
    "- âœ… Deduplication (via DISTINCT)\n",
    "- âŒ No aggregations (that's for Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- Silver Layer: Bronze â†’ Cleaned & Enriched\n",
    "-- ============================================================================\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS silver.cleaned_trips (\n",
    "  -- Surrogate key (MD5 hash of composite business key)\n",
    "  trip_id STRING,\n",
    "  \n",
    "  -- Renamed fields (snake_case, standardized types)\n",
    "  vendor_id INT,\n",
    "  pickup_datetime TIMESTAMP(3),\n",
    "  dropoff_datetime TIMESTAMP(3),\n",
    "  passenger_count DOUBLE,\n",
    "  trip_distance_miles DOUBLE,\n",
    "  rate_code_id INT,\n",
    "  store_and_fwd_flag STRING,\n",
    "  pickup_location_id INT,\n",
    "  dropoff_location_id INT,\n",
    "  payment_type INT,\n",
    "  \n",
    "  -- Financial fields (DECIMAL for precision)\n",
    "  fare_amount DECIMAL(10,2),\n",
    "  extra DECIMAL(10,2),\n",
    "  mta_tax DECIMAL(10,2),\n",
    "  tip_amount DECIMAL(10,2),\n",
    "  tolls_amount DECIMAL(10,2),\n",
    "  improvement_surcharge DECIMAL(10,2),\n",
    "  total_amount DECIMAL(10,2),\n",
    "  congestion_surcharge DECIMAL(10,2),\n",
    "  airport_fee DECIMAL(10,2),\n",
    "  \n",
    "  -- Derived fields (business logic)\n",
    "  trip_duration_minutes DOUBLE,\n",
    "  pickup_date DATE,\n",
    "  pickup_hour INT,\n",
    "  \n",
    "  -- Metadata\n",
    "  ingested_at TIMESTAMP(3),\n",
    "  processed_at TIMESTAMP(3)\n",
    ") WITH (\n",
    "  'write.format.default' = 'parquet',\n",
    "  'write.target-file-size-bytes' = '134217728'\n",
    ");\n",
    "\n",
    "-- Transform: Bronze â†’ Silver\n",
    "INSERT INTO silver.cleaned_trips\n",
    "SELECT DISTINCT  -- Deduplication\n",
    "  -- Generate surrogate key (MD5 of composite business key)\n",
    "  MD5(\n",
    "    CONCAT_WS('|',\n",
    "      CAST(VendorID AS VARCHAR),\n",
    "      tpep_pickup_datetime,\n",
    "      tpep_dropoff_datetime,\n",
    "      CAST(PULocationID AS VARCHAR),\n",
    "      CAST(DOLocationID AS VARCHAR),\n",
    "      CAST(fare_amount AS VARCHAR),\n",
    "      CAST(total_amount AS VARCHAR)\n",
    "    )\n",
    "  ) AS trip_id,\n",
    "  \n",
    "  -- Cast & rename\n",
    "  CAST(VendorID AS INT) AS vendor_id,\n",
    "  CAST(tpep_pickup_datetime AS TIMESTAMP(3)) AS pickup_datetime,\n",
    "  CAST(tpep_dropoff_datetime AS TIMESTAMP(3)) AS dropoff_datetime,\n",
    "  passenger_count,\n",
    "  trip_distance AS trip_distance_miles,\n",
    "  CAST(RatecodeID AS INT) AS rate_code_id,\n",
    "  store_and_fwd_flag,\n",
    "  CAST(PULocationID AS INT) AS pickup_location_id,\n",
    "  CAST(DOLocationID AS INT) AS dropoff_location_id,\n",
    "  CAST(payment_type AS INT) AS payment_type,\n",
    "  \n",
    "  -- Financial (DECIMAL precision)\n",
    "  CAST(fare_amount AS DECIMAL(10,2)) AS fare_amount,\n",
    "  CAST(extra AS DECIMAL(10,2)) AS extra,\n",
    "  CAST(mta_tax AS DECIMAL(10,2)) AS mta_tax,\n",
    "  CAST(tip_amount AS DECIMAL(10,2)) AS tip_amount,\n",
    "  CAST(tolls_amount AS DECIMAL(10,2)) AS tolls_amount,\n",
    "  CAST(improvement_surcharge AS DECIMAL(10,2)) AS improvement_surcharge,\n",
    "  CAST(total_amount AS DECIMAL(10,2)) AS total_amount,\n",
    "  CAST(congestion_surcharge AS DECIMAL(10,2)) AS congestion_surcharge,\n",
    "  CAST(Airport_fee AS DECIMAL(10,2)) AS airport_fee,\n",
    "  \n",
    "  -- Derived fields\n",
    "  TIMESTAMPDIFF(\n",
    "    MINUTE,\n",
    "    CAST(tpep_pickup_datetime AS TIMESTAMP(3)),\n",
    "    CAST(tpep_dropoff_datetime AS TIMESTAMP(3))\n",
    "  ) AS trip_duration_minutes,\n",
    "  \n",
    "  CAST(tpep_pickup_datetime AS DATE) AS pickup_date,\n",
    "  EXTRACT(HOUR FROM CAST(tpep_pickup_datetime AS TIMESTAMP(3))) AS pickup_hour,\n",
    "  \n",
    "  -- Metadata\n",
    "  ingested_at,\n",
    "  CURRENT_TIMESTAMP AS processed_at\n",
    "  \n",
    "FROM bronze.raw_trips\n",
    "\n",
    "-- Quality filters\n",
    "WHERE tpep_pickup_datetime IS NOT NULL\n",
    "  AND tpep_dropoff_datetime IS NOT NULL\n",
    "  AND trip_distance >= 0\n",
    "  AND fare_amount >= 0\n",
    "  AND total_amount >= 0\n",
    "  AND passenger_count > 0\n",
    "  -- Date range validation (2024-01 data)\n",
    "  AND CAST(tpep_pickup_datetime AS TIMESTAMP(3)) >= TIMESTAMP '2024-01-01 00:00:00'\n",
    "  AND CAST(tpep_pickup_datetime AS TIMESTAMP(3)) < TIMESTAMP '2024-02-01 00:00:00';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â–¶ï¸ Running Flink Jobs\n",
    "\n",
    "```bash\n",
    "# Execute Bronze layer (Kafka â†’ Iceberg)\n",
    "make process-bronze\n",
    "\n",
    "# Execute Silver layer (Bronze â†’ Silver)\n",
    "make process-silver\n",
    "\n",
    "# Or run both sequentially\n",
    "make process\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "1. Flink SQL Client connects to JobManager\n",
    "2. Executes `00-init.sql` (catalog setup)\n",
    "3. Executes `05-bronze.sql` or `06-silver.sql`\n",
    "4. Job runs in batch mode (processes all available data)\n",
    "5. Writes to Iceberg tables in MinIO\n",
    "6. Job completes and exits\n",
    "\n",
    "**Monitor Flink:**\n",
    "- Web UI: http://localhost:8081\n",
    "- Check running jobs, task metrics, checkpoints\n",
    "\n",
    "### ðŸ“Š Verifying Iceberg Tables\n",
    "\n",
    "```bash\n",
    "# Access Flink SQL Client\n",
    "docker exec -it p01-flink-jobmanager /opt/flink/bin/sql-client.sh\n",
    "\n",
    "# Inside SQL client:\n",
    "USE CATALOG iceberg_catalog;\n",
    "USE nyc_taxi;\n",
    "\n",
    "SHOW TABLES;  -- Should see bronze.raw_trips, silver.cleaned_trips\n",
    "\n",
    "-- Check row counts\n",
    "SELECT COUNT(*) FROM bronze.raw_trips;\n",
    "SELECT COUNT(*) FROM silver.cleaned_trips;\n",
    "\n",
    "-- Sample data\n",
    "SELECT * FROM silver.cleaned_trips LIMIT 5;\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Iceberg Layer: ACID Table Storage\n",
    "\n",
    "### ðŸ§Š What Iceberg Provides\n",
    "\n",
    "- **ACID guarantees** (atomicity, consistency, isolation, durability)\n",
    "- **Time travel** (query historical snapshots)\n",
    "- **Schema evolution** (add/drop/rename columns safely)\n",
    "- **Concurrent readers/writers** (multiple engines can access)\n",
    "- **Hidden partitioning** (partition pruning without user awareness)\n",
    "- **Snapshot management** (efficient storage with expiration)\n",
    "\n",
    "### ðŸ“‚ Table Layout in MinIO\n",
    "\n",
    "```\n",
    "s3://warehouse/\n",
    "â”œâ”€â”€ bronze/\n",
    "â”‚   â””â”€â”€ raw_trips/\n",
    "â”‚       â”œâ”€â”€ metadata/\n",
    "â”‚       â”‚   â”œâ”€â”€ version-hint.text\n",
    "â”‚       â”‚   â”œâ”€â”€ v1.metadata.json\n",
    "â”‚       â”‚   â””â”€â”€ snap-123456.avro\n",
    "â”‚       â””â”€â”€ data/\n",
    "â”‚           â”œâ”€â”€ 00000-0-data-001.parquet\n",
    "â”‚           â”œâ”€â”€ 00001-0-data-002.parquet\n",
    "â”‚           â””â”€â”€ ...\n",
    "â””â”€â”€ silver/\n",
    "    â””â”€â”€ cleaned_trips/\n",
    "        â”œâ”€â”€ metadata/\n",
    "        â””â”€â”€ data/\n",
    "```\n",
    "\n",
    "### ðŸ” Iceberg Metadata Inspection\n",
    "\n",
    "```sql\n",
    "-- View table history (snapshots)\n",
    "SELECT * FROM iceberg_catalog.nyc_taxi.bronze.raw_trips.history;\n",
    "\n",
    "-- View table snapshots\n",
    "SELECT * FROM iceberg_catalog.nyc_taxi.bronze.raw_trips.snapshots;\n",
    "\n",
    "-- View table files\n",
    "SELECT * FROM iceberg_catalog.nyc_taxi.bronze.raw_trips.files;\n",
    "\n",
    "-- Time travel query (read as of snapshot)\n",
    "SELECT COUNT(*) \n",
    "FROM iceberg_catalog.nyc_taxi.silver.cleaned_trips \n",
    "FOR SYSTEM_TIME AS OF TIMESTAMP '2024-01-15 12:00:00';\n",
    "```\n",
    "\n",
    "### ðŸ§¹ Iceberg Maintenance Operations\n",
    "\n",
    "**1. Compaction (merge small files)**\n",
    "\n",
    "Streaming writes create many small files. Compaction merges them for better query performance.\n",
    "\n",
    "```sql\n",
    "-- Rewrite small files into larger ones\n",
    "CALL iceberg_catalog.system.rewrite_data_files(\n",
    "  table => 'nyc_taxi.silver.cleaned_trips',\n",
    "  strategy => 'sort',\n",
    "  sort_order => 'pickup_date,pickup_hour'\n",
    ");\n",
    "```\n",
    "\n",
    "**2. Snapshot Expiration (cleanup old metadata)**\n",
    "\n",
    "```sql\n",
    "-- Expire snapshots older than 7 days\n",
    "CALL iceberg_catalog.system.expire_snapshots(\n",
    "  table => 'nyc_taxi.silver.cleaned_trips',\n",
    "  older_than => TIMESTAMP '2024-01-08 00:00:00',\n",
    "  retain_last => 5\n",
    ");\n",
    "```\n",
    "\n",
    "**3. Orphan File Cleanup (remove unreferenced files)**\n",
    "\n",
    "```sql\n",
    "-- Remove files not referenced by any snapshot\n",
    "CALL iceberg_catalog.system.remove_orphan_files(\n",
    "  table => 'nyc_taxi.silver.cleaned_trips',\n",
    "  older_than => TIMESTAMP '2024-01-08 00:00:00'\n",
    ");\n",
    "```\n",
    "\n",
    "### âš™ï¸ Production Maintenance Schedule\n",
    "\n",
    "These are **control plane tasks** - run them via Airflow (covered in Section 7):\n",
    "\n",
    "| Operation | Frequency | Reason |\n",
    "|-----------|-----------|--------|\n",
    "| **Compaction** | Daily (off-peak) | Query performance, cost reduction |\n",
    "| **Snapshot Expiration** | Weekly | Metadata cleanup, storage cost |\n",
    "| **Orphan File Cleanup** | Weekly | Reclaim unreferenced storage |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. dbt Layer: Analytics Transformations (Silver â†’ Gold)\n",
    "\n",
    "### ðŸ”¨ What dbt Owns\n",
    "\n",
    "- **Semantic modeling** (facts, dimensions, metrics)\n",
    "- **Business logic** (KPIs, aggregations, derivations)\n",
    "- **Data contracts** (tests, constraints, documentation)\n",
    "- **Incremental materialization** (efficient rebuilds)\n",
    "- **Lineage & documentation** (DAG visualization, column descriptions)\n",
    "\n",
    "### ðŸ“‚ dbt Project Structure\n",
    "\n",
    "```\n",
    "dbt_project/\n",
    "â”œâ”€â”€ dbt_project.yml       # Project config\n",
    "â”œâ”€â”€ profiles.yml          # Connection to Iceberg (via DuckDB)\n",
    "â”œâ”€â”€ packages.yml          # dbt-utils dependency\n",
    "â”œâ”€â”€ models/\n",
    "â”‚   â”œâ”€â”€ sources/\n",
    "â”‚   â”‚   â””â”€â”€ sources.yml   # Define Silver as source\n",
    "â”‚   â”œâ”€â”€ staging/          # Light transforms from source\n",
    "â”‚   â”‚   â”œâ”€â”€ stg_yellow_trips.sql\n",
    "â”‚   â”‚   â”œâ”€â”€ stg_taxi_zones.sql\n",
    "â”‚   â”‚   â””â”€â”€ staging.yml\n",
    "â”‚   â”œâ”€â”€ intermediate/     # Business logic building blocks\n",
    "â”‚   â”‚   â”œâ”€â”€ int_trip_metrics.sql\n",
    "â”‚   â”‚   â”œâ”€â”€ int_daily_summary.sql\n",
    "â”‚   â”‚   â””â”€â”€ intermediate.yml\n",
    "â”‚   â””â”€â”€ marts/            # Gold layer (final outputs)\n",
    "â”‚       â”œâ”€â”€ core/\n",
    "â”‚       â”‚   â”œâ”€â”€ fct_trips.sql\n",
    "â”‚       â”‚   â”œâ”€â”€ dim_dates.sql\n",
    "â”‚       â”‚   â””â”€â”€ core.yml\n",
    "â”‚       â””â”€â”€ analytics/\n",
    "â”‚           â”œâ”€â”€ mart_daily_revenue.sql\n",
    "â”‚           â””â”€â”€ analytics.yml\n",
    "â”œâ”€â”€ macros/               # Reusable SQL functions\n",
    "â”‚   â””â”€â”€ duration_minutes.sql\n",
    "â”œâ”€â”€ seeds/                # Reference data (CSV â†’ tables)\n",
    "â”‚   â”œâ”€â”€ payment_type_lookup.csv\n",
    "â”‚   â””â”€â”€ taxi_zone_lookup.csv\n",
    "â””â”€â”€ tests/                # Custom data tests\n",
    "    â””â”€â”€ assert_fare_not_exceeds_total.sql\n",
    "```\n",
    "\n",
    "### ðŸ”Œ dbt Connection (profiles.yml)\n",
    "\n",
    "dbt uses **DuckDB** to read Iceberg tables via `iceberg_scan()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 01: dbt-duckdb profile reading Iceberg\n",
    "pipeline_01:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: duckdb\n",
    "      path: /tmp/p01.duckdb\n",
    "      threads: 4\n",
    "      extensions:\n",
    "        - httpfs   # S3 access\n",
    "        - iceberg  # Iceberg table reading\n",
    "      settings:\n",
    "        s3_region: us-east-1\n",
    "        s3_endpoint: http://minio:9000\n",
    "        s3_access_key_id: minioadmin\n",
    "        s3_secret_access_key: minioadmin\n",
    "        s3_use_ssl: false\n",
    "        s3_url_style: path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Source Definition (sources.yml)\n",
    "\n",
    "Define Silver Iceberg table as dbt source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: raw_nyc_taxi\n",
    "    description: \"NYC TLC Yellow Taxi data from Iceberg Silver layer\"\n",
    "    meta:\n",
    "      external_location: \"s3://warehouse/silver/cleaned_trips\"\n",
    "    tables:\n",
    "      - name: raw_yellow_trips\n",
    "        description: \"Silver-layer cleaned trips from Flink processing\"\n",
    "        identifier: \"iceberg_scan('s3://warehouse/silver/cleaned_trips', allow_moved_paths=true)\"\n",
    "        columns:\n",
    "          - name: trip_id\n",
    "            description: \"Surrogate key (MD5 hash)\"\n",
    "            tests:\n",
    "              - unique\n",
    "              - not_null\n",
    "          - name: pickup_datetime\n",
    "            description: \"Trip start timestamp\"\n",
    "            tests:\n",
    "              - not_null\n",
    "          - name: total_amount\n",
    "            description: \"Total trip cost\"\n",
    "            tests:\n",
    "              - not_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Staging Model (stg_yellow_trips.sql)\n",
    "\n",
    "Light transformation from Silver source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Staging: Light transforms from Silver Iceberg\n",
    "{{ config(\n",
    "    materialized='view',\n",
    "    tags=['staging']\n",
    ") }}\n",
    "\n",
    "with source as (\n",
    "    select * from {{ source('raw_nyc_taxi', 'raw_yellow_trips') }}\n",
    "),\n",
    "\n",
    "standardized as (\n",
    "    select\n",
    "        trip_id,\n",
    "        vendor_id,\n",
    "        pickup_datetime,\n",
    "        dropoff_datetime,\n",
    "        passenger_count,\n",
    "        trip_distance_miles as trip_distance,\n",
    "        rate_code_id,\n",
    "        pickup_location_id,\n",
    "        dropoff_location_id,\n",
    "        payment_type,\n",
    "        \n",
    "        -- Financial (already DECIMAL from Flink)\n",
    "        fare_amount,\n",
    "        extra,\n",
    "        mta_tax,\n",
    "        tip_amount,\n",
    "        tolls_amount,\n",
    "        total_amount,\n",
    "        \n",
    "        -- Derived (already computed by Flink)\n",
    "        trip_duration_minutes,\n",
    "        pickup_date,\n",
    "        pickup_hour,\n",
    "        \n",
    "        -- Add day of week for analytics\n",
    "        dayname(pickup_datetime) as pickup_day_name,\n",
    "        \n",
    "        -- Business categorization\n",
    "        case \n",
    "            when tip_amount = 0 then 'No Tip'\n",
    "            when tip_amount / fare_amount < 0.10 then 'Low Tip'\n",
    "            when tip_amount / fare_amount < 0.20 then 'Normal Tip'\n",
    "            else 'High Tip'\n",
    "        end as tip_category\n",
    "        \n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Fact Table (fct_trips.sql)\n",
    "\n",
    "Core fact table for analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Fact table: Trip-level grain\n",
    "{{ config(\n",
    "    materialized='incremental',\n",
    "    unique_key='trip_id',\n",
    "    tags=['core', 'fact'],\n",
    "    contract={\n",
    "        \"enforced\": true\n",
    "    }\n",
    ") }}\n",
    "\n",
    "with trips as (\n",
    "    select * from {{ ref('stg_yellow_trips') }}\n",
    "    {% if is_incremental() %}\n",
    "    where pickup_datetime > (select max(pickup_datetime) from {{ this }})\n",
    "    {% endif %}\n",
    "),\n",
    "\n",
    "enriched as (\n",
    "    select\n",
    "        trip_id,\n",
    "        \n",
    "        -- Foreign keys\n",
    "        vendor_id,\n",
    "        pickup_location_id,\n",
    "        dropoff_location_id,\n",
    "        payment_type,\n",
    "        rate_code_id,\n",
    "        \n",
    "        -- Timestamps\n",
    "        pickup_datetime,\n",
    "        dropoff_datetime,\n",
    "        \n",
    "        -- Measures\n",
    "        passenger_count,\n",
    "        trip_distance,\n",
    "        trip_duration_minutes,\n",
    "        \n",
    "        -- Financial measures\n",
    "        fare_amount,\n",
    "        tip_amount,\n",
    "        total_amount,\n",
    "        \n",
    "        -- Derived measures\n",
    "        round(tip_amount / nullif(fare_amount, 0) * 100, 2) as tip_percentage,\n",
    "        round(trip_distance / nullif(trip_duration_minutes, 0) * 60, 2) as avg_speed_mph\n",
    "        \n",
    "    from trips\n",
    ")\n",
    "\n",
    "select * from enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Analytics Mart (mart_daily_revenue.sql)\n",
    "\n",
    "Business-level aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Daily revenue summary\n",
    "{{ config(\n",
    "    materialized='table',\n",
    "    tags=['analytics', 'mart']\n",
    ") }}\n",
    "\n",
    "with trips as (\n",
    "    select\n",
    "        pickup_date,\n",
    "        pickup_day_name,\n",
    "        total_amount,\n",
    "        trip_distance,\n",
    "        passenger_count\n",
    "    from {{ ref('stg_yellow_trips') }}\n",
    "),\n",
    "\n",
    "daily_metrics as (\n",
    "    select\n",
    "        pickup_date as date_key,\n",
    "        pickup_day_name as day_name,\n",
    "        \n",
    "        -- Aggregated measures\n",
    "        count(*) as total_trips,\n",
    "        sum(total_amount) as total_revenue,\n",
    "        avg(total_amount) as avg_fare,\n",
    "        sum(trip_distance) as total_miles,\n",
    "        sum(passenger_count) as total_passengers,\n",
    "        \n",
    "        -- Derived KPIs\n",
    "        round(sum(total_amount) / count(*), 2) as revenue_per_trip,\n",
    "        round(sum(trip_distance) / count(*), 2) as avg_trip_distance\n",
    "        \n",
    "    from trips\n",
    "    group by pickup_date, pickup_day_name\n",
    ")\n",
    "\n",
    "select * from daily_metrics\n",
    "order by date_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… dbt Tests\n",
    "\n",
    "Data quality assertions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/marts/core/core.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: fct_trips\n",
    "    description: \"Core trip fact table\"\n",
    "    columns:\n",
    "      - name: trip_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: pickup_datetime\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: total_amount\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              min_value: 0\n",
    "              max_value: 1000\n",
    "    tests:\n",
    "      - dbt_utils.expression_is_true:\n",
    "          expression: \"fare_amount <= total_amount\"\n",
    "          config:\n",
    "            severity: error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â–¶ï¸ Running dbt\n",
    "\n",
    "```bash\n",
    "# Install dependencies\n",
    "make dbt-deps\n",
    "\n",
    "# Run all models\n",
    "make dbt-build\n",
    "\n",
    "# Run tests only\n",
    "make dbt-test\n",
    "\n",
    "# Generate documentation\n",
    "make dbt-docs\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Running with dbt=1.7.0\n",
    "Found 15 models, 91 tests, 3 seeds, 0 sources, 0 exposures, 0 metrics\n",
    "\n",
    "Concurrency: 4 threads\n",
    "\n",
    "1 of 91 START sql table model main.dim_dates .................... [RUN]\n",
    "1 of 91 OK created sql table model main.dim_dates ............... [OK in 0.12s]\n",
    "...\n",
    "91 of 91 PASS not_null_fct_trips_trip_id ........................ [PASS in 0.03s]\n",
    "\n",
    "Finished running 15 models, 91 tests in 0 hours 0 minutes and 8.24 seconds.\n",
    "\n",
    "Completed successfully\n",
    "\n",
    "Done. PASS=91 WARN=0 ERROR=0 SKIP=0 TOTAL=91\n",
    "```\n",
    "\n",
    "### ðŸ“Š dbt Lineage (DAG)\n",
    "\n",
    "```\n",
    "source(raw_yellow_trips)\n",
    "  â†“\n",
    "stg_yellow_trips (view)\n",
    "  â†“\n",
    "â”œâ”€â†’ int_trip_metrics (view)\n",
    "â”‚     â†“\n",
    "â”‚   fct_trips (incremental table)\n",
    "â”‚\n",
    "â”œâ”€â†’ int_daily_summary (view)\n",
    "â”‚     â†“\n",
    "â”‚   mart_daily_revenue (table)\n",
    "â”‚\n",
    "â””â”€â†’ int_hourly_patterns (view)\n",
    "      â†“\n",
    "    mart_hourly_demand (table)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Airflow Layer: Production Orchestration\n",
    "\n",
    "### ðŸš Why Add Airflow?\n",
    "\n",
    "**Data plane (Kafka/Flink) runs continuously.**  \n",
    "**Control plane (dbt, maintenance) runs on schedule.**\n",
    "\n",
    "Airflow orchestrates the **control plane**:\n",
    "- âœ… dbt runs every 10-15 minutes\n",
    "- âœ… Iceberg compaction daily\n",
    "- âœ… Snapshot expiration weekly\n",
    "- âœ… Health checks & alerts\n",
    "- âœ… Backfill/replay workflows\n",
    "\n",
    "### ðŸ“¦ Airflow Services\n",
    "\n",
    "Add to `docker-compose.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow services (add to existing docker-compose.yml)\n",
    "\n",
    "services:\n",
    "  # Airflow metadata database\n",
    "  postgres-airflow:\n",
    "    image: postgres:15\n",
    "    container_name: p01-airflow-postgres\n",
    "    environment:\n",
    "      POSTGRES_USER: airflow\n",
    "      POSTGRES_PASSWORD: airflow\n",
    "      POSTGRES_DB: airflow\n",
    "    volumes:\n",
    "      - postgres-airflow-data:/var/lib/postgresql/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n",
    "      interval: 5s\n",
    "      retries: 5\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # Airflow webserver\n",
    "  airflow-webserver:\n",
    "    image: apache/airflow:2.8.0-python3.11\n",
    "    container_name: p01-airflow-webserver\n",
    "    command: webserver\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    environment:\n",
    "      AIRFLOW__CORE__EXECUTOR: LocalExecutor\n",
    "      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow\n",
    "      AIRFLOW__CORE__FERNET_KEY: ''\n",
    "      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n",
    "      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n",
    "      AIRFLOW__WEBSERVER__SECRET_KEY: 'secret'\n",
    "    volumes:\n",
    "      - ./airflow/dags:/opt/airflow/dags\n",
    "      - ./airflow/logs:/opt/airflow/logs\n",
    "      - ./airflow/plugins:/opt/airflow/plugins\n",
    "      - ./dbt_project:/opt/airflow/dbt\n",
    "    depends_on:\n",
    "      postgres-airflow:\n",
    "        condition: service_healthy\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # Airflow scheduler\n",
    "  airflow-scheduler:\n",
    "    image: apache/airflow:2.8.0-python3.11\n",
    "    container_name: p01-airflow-scheduler\n",
    "    command: scheduler\n",
    "    environment:\n",
    "      AIRFLOW__CORE__EXECUTOR: LocalExecutor\n",
    "      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow\n",
    "      AIRFLOW__CORE__FERNET_KEY: ''\n",
    "    volumes:\n",
    "      - ./airflow/dags:/opt/airflow/dags\n",
    "      - ./airflow/logs:/opt/airflow/logs\n",
    "      - ./airflow/plugins:/opt/airflow/plugins\n",
    "      - ./dbt_project:/opt/airflow/dbt\n",
    "    depends_on:\n",
    "      postgres-airflow:\n",
    "        condition: service_healthy\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # Airflow initialization\n",
    "  airflow-init:\n",
    "    image: apache/airflow:2.8.0-python3.11\n",
    "    container_name: p01-airflow-init\n",
    "    entrypoint: /bin/bash\n",
    "    command:\n",
    "      - -c\n",
    "      - |\n",
    "        airflow db migrate\n",
    "        airflow users create \\\n",
    "          --username admin \\\n",
    "          --firstname Admin \\\n",
    "          --lastname User \\\n",
    "          --role Admin \\\n",
    "          --email admin@example.com \\\n",
    "          --password admin\n",
    "    environment:\n",
    "      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow\n",
    "    depends_on:\n",
    "      postgres-airflow:\n",
    "        condition: service_healthy\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "volumes:\n",
    "  postgres-airflow-data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“… Production DAG (taxi_pipeline_dag.py)\n",
    "\n",
    "Located at: `airflow/dags/taxi_pipeline_dag.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "\n",
    "# ============================================================================\n",
    "# DAG Configuration\n",
    "# ============================================================================\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['alerts@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'nyc_taxi_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='NYC Taxi Real-Time Pipeline Orchestration',\n",
    "    schedule_interval='*/10 * * * *',  # Every 10 minutes\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=['production', 'nyc-taxi', 'real-time'],\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def check_flink_health(**context):\n",
    "    \"\"\"Check if Flink cluster is healthy and jobs are running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get('http://flink-jobmanager:8081/jobs/overview')\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        jobs = response.json()['jobs']\n",
    "        running_jobs = [j for j in jobs if j['state'] == 'RUNNING']\n",
    "        \n",
    "        if not running_jobs:\n",
    "            raise ValueError(\"No running Flink jobs found\")\n",
    "        \n",
    "        print(f\"âœ“ Flink healthy: {len(running_jobs)} jobs running\")\n",
    "        return 'run_dbt'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Flink health check failed: {e}\")\n",
    "        return 'alert_flink_down'\n",
    "\n",
    "def check_kafka_lag(**context):\n",
    "    \"\"\"Verify Flink consumer lag is below threshold.\"\"\"\n",
    "    # In production: query Kafka consumer group lag\n",
    "    # For now, assume healthy if Flink is running\n",
    "    print(\"âœ“ Kafka lag check passed\")\n",
    "    return True\n",
    "\n",
    "def check_iceberg_watermark(**context):\n",
    "    \"\"\"Verify Silver table has recent data.\"\"\"\n",
    "    # In production: query latest timestamp from silver.cleaned_trips\n",
    "    # Compare against expected freshness SLA\n",
    "    print(\"âœ“ Iceberg watermark check passed\")\n",
    "    return True\n",
    "\n",
    "# ============================================================================\n",
    "# Task Definitions\n",
    "# ============================================================================\n",
    "\n",
    "# Task 1: Health Checks\n",
    "health_check = BranchPythonOperator(\n",
    "    task_id='check_flink_health',\n",
    "    python_callable=check_flink_health,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Run dbt (Silver â†’ Gold)\n",
    "run_dbt = BashOperator(\n",
    "    task_id='run_dbt',\n",
    "    bash_command='cd /opt/airflow/dbt && dbt build --profiles-dir . --target prod',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Run dbt tests\n",
    "run_dbt_tests = BashOperator(\n",
    "    task_id='run_dbt_tests',\n",
    "    bash_command='cd /opt/airflow/dbt && dbt test --profiles-dir . --target prod',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 4: Alert if Flink down\n",
    "alert_flink_down = BashOperator(\n",
    "    task_id='alert_flink_down',\n",
    "    bash_command='echo \"ALERT: Flink cluster unhealthy\" && exit 1',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# DAG Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "health_check >> [run_dbt, alert_flink_down]\n",
    "run_dbt >> run_dbt_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§¹ Maintenance DAG (iceberg_maintenance_dag.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'iceberg_maintenance',\n",
    "    default_args=default_args,\n",
    "    description='Iceberg table maintenance (compaction, expiration)',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    tags=['maintenance', 'iceberg'],\n",
    ")\n",
    "\n",
    "# Compact Silver table\n",
    "compact_silver = BashOperator(\n",
    "    task_id='compact_silver_table',\n",
    "    bash_command=\"\"\"\n",
    "    docker exec p01-flink-jobmanager /opt/flink/bin/sql-client.sh \\\n",
    "      -i /opt/flink/sql/00-init.sql \\\n",
    "      -e \"CALL iceberg_catalog.system.rewrite_data_files(\n",
    "            table => 'nyc_taxi.silver.cleaned_trips',\n",
    "            strategy => 'sort',\n",
    "            sort_order => 'pickup_date,pickup_hour'\n",
    "          );\"\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Expire old snapshots (keep last 7 days)\n",
    "expire_snapshots = BashOperator(\n",
    "    task_id='expire_old_snapshots',\n",
    "    bash_command=\"\"\"\n",
    "    docker exec p01-flink-jobmanager /opt/flink/bin/sql-client.sh \\\n",
    "      -i /opt/flink/sql/00-init.sql \\\n",
    "      -e \"CALL iceberg_catalog.system.expire_snapshots(\n",
    "            table => 'nyc_taxi.silver.cleaned_trips',\n",
    "            older_than => CURRENT_TIMESTAMP - INTERVAL '7' DAY,\n",
    "            retain_last => 5\n",
    "          );\"\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Remove orphan files\n",
    "remove_orphans = BashOperator(\n",
    "    task_id='remove_orphan_files',\n",
    "    bash_command=\"\"\"\n",
    "    docker exec p01-flink-jobmanager /opt/flink/bin/sql-client.sh \\\n",
    "      -i /opt/flink/sql/00-init.sql \\\n",
    "      -e \"CALL iceberg_catalog.system.remove_orphan_files(\n",
    "            table => 'nyc_taxi.silver.cleaned_trips',\n",
    "            older_than => CURRENT_TIMESTAMP - INTERVAL '7' DAY\n",
    "          );\"\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "compact_silver >> expire_snapshots >> remove_orphans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Running with Airflow\n",
    "\n",
    "```bash\n",
    "# Start all services (including Airflow)\n",
    "docker-compose up -d\n",
    "\n",
    "# Access Airflow UI: http://localhost:8080\n",
    "# Login: admin / admin\n",
    "\n",
    "# Enable DAGs\n",
    "# - nyc_taxi_pipeline (runs every 10 min)\n",
    "# - iceberg_maintenance (runs daily at 2 AM)\n",
    "```\n",
    "\n",
    "### ðŸ“Š Airflow DAG View\n",
    "\n",
    "```\n",
    "nyc_taxi_pipeline (every 10 min):\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ check_flink_health  â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”œâ”€â”€â”€ healthy? â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚  run_dbt    â”‚\n",
    "         â”‚                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                       â”‚\n",
    "         â”‚                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚ run_dbt_tests     â”‚\n",
    "         â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â””â”€â”€â”€ unhealthy? â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                            â”‚ alert_flink_down â”‚\n",
    "                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "iceberg_maintenance (daily 2 AM):\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ compact_silver    â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ expire_snapshots      â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ remove_orphan_files   â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. End-to-End Workflow\n",
    "\n",
    "### ðŸ”„ Complete Production Flow\n",
    "\n",
    "#### **Data Plane (Always-On)**\n",
    "\n",
    "```\n",
    "1. [Data Generator] produces 10k events â†’ Kafka topic taxi.raw_trips\n",
    "   Duration: ~0.3s | Throughput: 31,250 events/sec\n",
    "\n",
    "2. [Flink Bronze Job] consumes from Kafka â†’ writes to Iceberg bronze.raw_trips\n",
    "   Duration: ~24s | Processing: JSON parsing + type casting\n",
    "\n",
    "3. [Flink Silver Job] reads bronze â†’ transforms â†’ writes to silver.cleaned_trips\n",
    "   Duration: ~43s | Processing: Validation + enrichment + deduplication\n",
    "```\n",
    "\n",
    "#### **Control Plane (Scheduled - every 10 min)**\n",
    "\n",
    "```\n",
    "4. [Airflow] checks Flink health + Kafka lag\n",
    "   Duration: ~2s | Verifies stream processor is operational\n",
    "\n",
    "5. [dbt] reads silver.cleaned_trips â†’ builds Gold models\n",
    "   Duration: ~21s | Creates 15 models: staging â†’ intermediate â†’ marts\n",
    "\n",
    "6. [dbt test] validates all 91 data quality tests\n",
    "   Duration: ~5s | Ensures data integrity\n",
    "```\n",
    "\n",
    "#### **Maintenance Plane (Scheduled - daily)**\n",
    "\n",
    "```\n",
    "7. [Iceberg Compaction] merges small files\n",
    "   Duration: ~15min | Optimizes query performance\n",
    "\n",
    "8. [Snapshot Expiration] cleans old metadata\n",
    "   Duration: ~5min | Reduces storage costs\n",
    "```\n",
    "\n",
    "### â±ï¸ Performance Summary\n",
    "\n",
    "| Phase | Duration | What Happens |\n",
    "|-------|----------|-------------|\n",
    "| **Startup** | 15-30s | All services become healthy |\n",
    "| **Ingestion** | 0.3s | 10k events to Kafka |\n",
    "| **Bronze Processing** | 24s | Kafka â†’ Iceberg (raw) |\n",
    "| **Silver Processing** | 43s | Bronze â†’ Silver (cleaned) |\n",
    "| **dbt Build** | 21s | Silver â†’ Gold (15 models) |\n",
    "| **dbt Test** | 5s | 91 quality tests |\n",
    "| **Total E2E** | **107s** | First event â†’ Gold tables ready |\n",
    "\n",
    "### ðŸ“Š Data Flow Volumes\n",
    "\n",
    "```\n",
    "Parquet Source:  2,964,624 rows\n",
    "     â†“ (sample 10k)\n",
    "Kafka Topic:     10,000 events\n",
    "     â†“ (all consumed)\n",
    "Bronze Table:    10,000 rows\n",
    "     â†“ (quality filters ~2% rejection)\n",
    "Silver Table:    9,855 rows\n",
    "     â†“ (aggregations)\n",
    "Gold Marts:\n",
    "  - fct_trips:              9,855 rows\n",
    "  - mart_daily_revenue:        31 rows (1 per day)\n",
    "  - mart_hourly_demand:       744 rows (31 days Ã— 24 hours)\n",
    "  - mart_location_performance: 265 rows (unique zones)\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ Quality Gates\n",
    "\n",
    "**91 dbt tests ensure:**\n",
    "- âœ… No null keys\n",
    "- âœ… No duplicates\n",
    "- âœ… Referential integrity (foreign keys exist)\n",
    "- âœ… Value ranges (fare > 0, duration > 0)\n",
    "- âœ… Business rules (fare â‰¤ total_amount)\n",
    "- âœ… Date validity (within expected range)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Operations\n",
    "\n",
    "### ðŸ“Š Monitoring & Observability\n",
    "\n",
    "#### **Kafka Metrics**\n",
    "```bash\n",
    "# Producer throughput\n",
    "kafka-run-class.sh kafka.tools.JmxTool \\\n",
    "  --object-name kafka.producer:type=producer-metrics,client-id=* \\\n",
    "  --attributes record-send-rate\n",
    "\n",
    "# Consumer lag (critical!)\n",
    "kafka-consumer-groups.sh --bootstrap-server localhost:9092 \\\n",
    "  --group flink-bronze-consumer \\\n",
    "  --describe\n",
    "```\n",
    "\n",
    "**Alert if:** Lag > 10,000 events for >5 minutes\n",
    "\n",
    "#### **Flink Metrics**\n",
    "- Web UI: http://localhost:8081\n",
    "- Metrics to track:\n",
    "  - **Backpressure:** Should be 0% (green)\n",
    "  - **Checkpoint duration:** Should be <60s\n",
    "  - **Checkpoint failures:** Should be 0\n",
    "  - **Task failures:** Should be 0\n",
    "  - **Records processed/sec:** Should match ingestion rate\n",
    "\n",
    "**Alert if:** \n",
    "- Backpressure > 10% for >10 minutes\n",
    "- Checkpoint failure rate > 0\n",
    "\n",
    "#### **Iceberg Metrics**\n",
    "```sql\n",
    "-- File count (should stay reasonable after compaction)\n",
    "SELECT COUNT(*) FROM iceberg_catalog.nyc_taxi.silver.cleaned_trips.files;\n",
    "\n",
    "-- Snapshot count (should decrease after expiration)\n",
    "SELECT COUNT(*) FROM iceberg_catalog.nyc_taxi.silver.cleaned_trips.snapshots;\n",
    "\n",
    "-- Table size\n",
    "SELECT SUM(file_size_in_bytes) / 1024 / 1024 / 1024 AS size_gb\n",
    "FROM iceberg_catalog.nyc_taxi.silver.cleaned_trips.files;\n",
    "```\n",
    "\n",
    "**Alert if:**\n",
    "- File count > 1,000 (needs compaction)\n",
    "- Snapshot count > 100 (needs expiration)\n",
    "\n",
    "#### **dbt Metrics**\n",
    "```bash\n",
    "# Test failures\n",
    "dbt test --profiles-dir . | grep -c \"ERROR\"\n",
    "\n",
    "# Model freshness\n",
    "dbt source freshness --profiles-dir .\n",
    "```\n",
    "\n",
    "**Alert if:**\n",
    "- Any test fails (ERROR > 0)\n",
    "- Silver data is >30 minutes old\n",
    "\n",
    "### ðŸš¨ Common Issues & Solutions\n",
    "\n",
    "#### Issue 1: Flink Job Fails\n",
    "**Symptoms:** Consumer lag growing, no data in Iceberg  \n",
    "**Check:** Flink logs, task manager errors  \n",
    "**Fix:** \n",
    "```bash\n",
    "# Restart Flink job from savepoint\n",
    "flink run -s s3://checkpoints/latest \\\n",
    "  /opt/flink/sql/05-bronze.sql\n",
    "```\n",
    "\n",
    "#### Issue 2: dbt Build Fails\n",
    "**Symptoms:** Gold tables not updating  \n",
    "**Check:** dbt logs, Silver table freshness  \n",
    "**Fix:**\n",
    "```bash\n",
    "# Debug specific model\n",
    "dbt run --select stg_yellow_trips --profiles-dir .\n",
    "\n",
    "# Check source data\n",
    "dbt source freshness --profiles-dir .\n",
    "```\n",
    "\n",
    "#### Issue 3: MinIO Storage Full\n",
    "**Symptoms:** Flink writes failing  \n",
    "**Check:** MinIO disk usage  \n",
    "**Fix:**\n",
    "```bash\n",
    "# Run maintenance immediately\n",
    "airflow dags trigger iceberg_maintenance\n",
    "\n",
    "# Or manual cleanup\n",
    "CALL iceberg_catalog.system.remove_orphan_files(\n",
    "  table => 'nyc_taxi.silver.cleaned_trips'\n",
    ");\n",
    "```\n",
    "\n",
    "### ðŸ”„ Backfill/Replay Pattern\n",
    "\n",
    "To reprocess historical data:\n",
    "\n",
    "```bash\n",
    "# 1. Reset Kafka consumer offset to specific timestamp\n",
    "kafka-consumer-groups.sh --bootstrap-server localhost:9092 \\\n",
    "  --group flink-bronze-consumer \\\n",
    "  --topic taxi.raw_trips \\\n",
    "  --reset-offsets --to-datetime 2024-01-01T00:00:00.000 \\\n",
    "  --execute\n",
    "\n",
    "# 2. Truncate target Iceberg table\n",
    "TRUNCATE TABLE iceberg_catalog.nyc_taxi.silver.cleaned_trips;\n",
    "\n",
    "# 3. Restart Flink job\n",
    "make process\n",
    "\n",
    "# 4. Rebuild dbt from scratch\n",
    "make dbt-build\n",
    "```\n",
    "\n",
    "### ðŸ“ˆ Scaling Considerations\n",
    "\n",
    "#### **Horizontal Scaling**\n",
    "- **Kafka:** Add partitions (must match Flink parallelism)\n",
    "- **Flink:** Add task managers + increase parallelism\n",
    "- **MinIO:** Add nodes for distributed storage\n",
    "\n",
    "#### **Vertical Scaling**\n",
    "- **Flink JM:** Increase heap (jobmanager.memory.process.size)\n",
    "- **Flink TM:** Increase task slots (taskmanager.numberOfTaskSlots)\n",
    "- **dbt:** Increase threads (profiles.yml)\n",
    "\n",
    "#### **Cost Optimization**\n",
    "- **Kafka:** Tune retention (balance replay vs cost)\n",
    "- **Iceberg:** Regular compaction (fewer files = lower query costs)\n",
    "- **MinIO:** Lifecycle policies (archive old snapshots to cheaper storage)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Next Steps\n",
    "\n",
    "### âœ… What You've Learned\n",
    "\n",
    "1. **Architecture:** Data plane (streaming) vs Control plane (orchestration)\n",
    "2. **Kafka:** Event buffering, schema management, consumer lag monitoring\n",
    "3. **Flink:** Real-time transformations, stateful processing, checkpointing\n",
    "4. **Iceberg:** ACID tables, time travel, maintenance operations\n",
    "5. **dbt:** Dimensional modeling, incremental builds, data testing\n",
    "6. **Airflow:** Scheduling, health checks, maintenance automation\n",
    "7. **Operations:** Monitoring, alerting, backfills, scaling\n",
    "\n",
    "### ðŸŽ¯ Production Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] **Security:** Enable Kafka ACLs, TLS, authentication\n",
    "- [ ] **Monitoring:** Set up Prometheus + Grafana dashboards\n",
    "- [ ] **Alerting:** Configure PagerDuty/Slack for critical events\n",
    "- [ ] **Backup:** Implement Iceberg snapshot backup to S3\n",
    "- [ ] **Documentation:** Update runbooks with operational procedures\n",
    "- [ ] **Testing:** Run chaos testing (kill random services)\n",
    "- [ ] **Capacity:** Load test with 10x expected throughput\n",
    "- [ ] **Cost:** Set up cost monitoring and budgets\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "#### **Enhancements**\n",
    "1. Add **streaming aggregations** in Flink (windowed metrics)\n",
    "2. Implement **late data handling** (watermarks + allowed lateness)\n",
    "3. Add **data quality** monitoring (Elementary + Soda Core)\n",
    "4. Set up **BI layer** (Superset/Metabase on Gold tables)\n",
    "5. Implement **CDC** from transactional DB (Debezium)\n",
    "\n",
    "#### **Advanced Topics**\n",
    "- **Exactly-once semantics:** Deep dive into Flink checkpointing\n",
    "- **Schema evolution:** Handle breaking changes gracefully\n",
    "- **Multi-region:** Deploy across availability zones\n",
    "- **Cost optimization:** Implement tiered storage (hot/warm/cold)\n",
    "- **ML integration:** Feature store (Feast) on top of Gold\n",
    "\n",
    "### ðŸ“š Resources\n",
    "\n",
    "- **This Repository:** `pipelines/01-kafka-flink-iceberg/`\n",
    "- **Flink Docs:** https://nightlies.apache.org/flink/\n",
    "- **Iceberg Docs:** https://iceberg.apache.org/\n",
    "- **dbt Docs:** https://docs.getdbt.com/\n",
    "- **Airflow Docs:** https://airflow.apache.org/\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Conclusion\n",
    "\n",
    "You now understand how to build a **production-grade real-time data pipeline** with:\n",
    "- âœ… Durable event streaming (Kafka)\n",
    "- âœ… Real-time transformations (Flink)\n",
    "- âœ… ACID table storage (Iceberg)\n",
    "- âœ… Analytics modeling (dbt)\n",
    "- âœ… Production orchestration (Airflow)\n",
    "\n",
    "**This is the industry-standard stack used by companies like:**\n",
    "- Netflix (Flink + Iceberg)\n",
    "- Apple (Kafka + Flink + dbt)\n",
    "- Uber (Kafka + Flink + Hudi)\n",
    "- Airbnb (Kafka + Spark + Iceberg + dbt)\n",
    "\n",
    "**You're ready for production!** ðŸš€\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbt-master (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
