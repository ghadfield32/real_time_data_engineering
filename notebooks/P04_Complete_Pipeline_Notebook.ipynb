{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 04: Production Real-Time Pipeline Walkthrough\n",
    "## Redpanda → Flink 2.0.1 → Apache Iceberg 1.10.1 → dbt-duckdb\n",
    "\n",
    "> **What this notebook is:** A linear, file-by-file walkthrough of a complete production-grade\n",
    "> streaming data pipeline. Every cell writes a real file from the live P04 pipeline. By the end\n",
    "> you will understand *why* every design decision was made — not just *what* the files contain.\n",
    ">\n",
    "> **Benchmark result (10,000 events, post-audit Feb 2026):**\n",
    "> ~88 seconds end-to-end (includes 5s Iceberg metadata flush), 91/91 dbt tests passing.\n",
    "> P04 is ~20s faster than P01 (Kafka) due to Redpanda's C++ single-binary architecture.\n",
    ">\n",
    "> **Post-audit status:** Production-hardened. DLQ in both `create-topics.sh` and Makefile,\n",
    "> streaming SQL parity with P01, CPU-bounded containers, benchmark race condition fixed.\n",
    "\n",
    "Run from the `notebooks/` directory so `%%writefile ../pipelines/04-...` paths resolve:\n",
    "\n",
    "```bash\n",
    "cd notebooks\n",
    "jupyter notebook P04_Complete_Pipeline_Notebook.ipynb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "1. [Architecture Overview](#1.-Architecture-Overview)\n",
    "2. [Shared Infrastructure](#2.-Shared-Infrastructure)\n",
    "3. [Docker Compose: Container Orchestration](#3.-Docker-Compose:-Container-Orchestration)\n",
    "4. [Redpanda Topics + Dead Letter Queue](#4.-Redpanda-Topics:-Event-Ingestion-+-Dead-Letter-Queue)\n",
    "5. [Flink Configuration](#5.-Flink-Configuration)\n",
    "6. [Flink SQL: Batch Session Init](#6.-Flink-SQL:-Batch-Session-Initialization)\n",
    "7. [Flink SQL: Streaming Session Init](#7.-Flink-SQL:-Streaming-Session-Initialization)\n",
    "8. [Flink SQL: Bronze Layer](#8.-Flink-SQL:-Bronze-Layer)\n",
    "9. [Flink SQL: Silver Layer — Deduplication + Quality Filtering](#9.-Flink-SQL:-Silver-Layer)\n",
    "10. [Flink SQL: Streaming Bronze (Continuous Mode)](#10.-Flink-SQL:-Streaming-Bronze)\n",
    "11. [dbt Project Configuration](#11.-dbt-Project-Configuration)\n",
    "12. [dbt Seeds: Reference Data](#12.-dbt-Seeds:-Reference-Data)\n",
    "13. [dbt Macros](#13.-dbt-Macros:-Cross-Database-Compatibility)\n",
    "14. [dbt Staging Models](#14.-dbt-Staging-Models)\n",
    "15. [dbt Intermediate Models](#15.-dbt-Intermediate-Models)\n",
    "16. [dbt Core Marts (Gold Layer)](#16.-dbt-Core-Marts)\n",
    "17. [dbt Analytics Marts (Gold Layer)](#17.-dbt-Analytics-Marts)\n",
    "18. [dbt Tests](#18.-dbt-Tests:-Data-Quality-Assertions)\n",
    "19. [Makefile: One-Command Orchestration](#19.-Pipeline-Makefile)\n",
    "20. [Running the Pipeline](#20.-Running-the-Pipeline)\n",
    "21. [Production Operations + Troubleshooting](#21.-Production-Operations)\n",
    "22. [Adapting to Your Own Dataset](#22.-Adapting-to-Your-Own-Dataset)\n",
    "23. [What We Learned: Key Decisions Explained](#23.-What-We-Learned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview\n",
    "\n",
    "### The Full Data Flow\n",
    "\n",
    "```\n",
    "NYC Taxi Parquet Files (local data/)\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────┐\n",
    "│    Data Generator    │  pyarrow → JSON → Kafka-protocol producer\n",
    "│    (shared/)         │  Modes: burst / realtime / batch\n",
    "│    ~25,000 evt/s     │  enable.idempotence=True, acks=all\n",
    "└──────────┬───────────┘\n",
    "           │  JSON events (350 bytes each)\n",
    "           ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│                    Redpanda                          │\n",
    "│  taxi.raw_trips      (3 partitions, 72h retention)  │  ← primary stream\n",
    "│  taxi.raw_trips.dlq  (1 partition,  7-day retention)│  ← dead letter queue\n",
    "│                                                      │\n",
    "│  C++/Seastar binary: ~400 MB RAM, 3s startup        │\n",
    "│  Kafka-wire-protocol compatible: same Flink connector│\n",
    "└──────────────┬───────────────────────────────────────┘\n",
    "               │  Kafka connector (flink-sql-connector-kafka-4.0.1-2.0)\n",
    "               ▼\n",
    "┌──────────────────────────────────────────────────────────────────┐\n",
    "│                   Apache Flink 2.0.1                              │\n",
    "│                                                                    │\n",
    "│  ─── BATCH MODE (default: catch-up processing) ──────────────── │\n",
    "│  Bronze job:  kafka_raw_trips → iceberg_catalog.bronze.raw_trips  │\n",
    "│    • Parse ISO 8601 timestamps → TIMESTAMP(3)                     │\n",
    "│    • Add ingestion_ts = CURRENT_TIMESTAMP                         │\n",
    "│    • scan.bounded.mode=latest-offset → stops when caught up       │\n",
    "│    • table.dml-sync=true → blocks until job completes             │\n",
    "│                                                                    │\n",
    "│  Silver job:  bronze.raw_trips → silver.cleaned_trips             │\n",
    "│    • ROW_NUMBER() OVER PARTITION BY natural key → deduplication   │\n",
    "│    • Quality filters: fare≥0, distance≥0, date 2024-01           │\n",
    "│    • Type casting: BIGINT→INT, DOUBLE→DECIMAL(10,2)               │\n",
    "│    • Partitioned by pickup_date DATE                              │\n",
    "│                                                                    │\n",
    "│  ─── STREAMING MODE (continuous: make process-streaming) ──────── │\n",
    "│  Bronze streaming: kafka_raw_trips → bronze.raw_trips             │\n",
    "│    • Runs indefinitely as events arrive                           │\n",
    "│    • 30s checkpoints for exactly-once fault tolerance             │\n",
    "│    • WATERMARK FOR event_time (10s late arrival tolerance)        │\n",
    "└──────────────────┬───────────────────────────────────────────────┘\n",
    "                   │  Iceberg S3A writes (Parquet + ZSTD)\n",
    "                   ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                 Apache Iceberg 1.10.1 on MinIO                   │\n",
    "│  s3://warehouse/bronze/raw_trips/     (format-version=1, unpart) │\n",
    "│  s3://warehouse/silver/cleaned_trips/ (format-version=2, by date)│\n",
    "│  ACID transactions, snapshot isolation, time travel              │\n",
    "└──────────────────┬──────────────────────────────────────────────┘\n",
    "                   │  iceberg_scan('s3://warehouse/silver/cleaned_trips')\n",
    "                   ▼\n",
    "┌──────────────────────────────────────────────────────────────────┐\n",
    "│                    dbt-duckdb (in Docker)                         │\n",
    "│  Sources → Staging → Intermediate → Core Marts → Analytics Marts  │\n",
    "│  91 data quality tests covering: not_null, unique, relationships, │\n",
    "│  accepted_values, custom business rules                           │\n",
    "└──────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Technology Stack (Feb 2026, production-validated)\n",
    "\n",
    "| Component | Version | Role | Key Property |\n",
    "|-----------|---------|------|-------------|\n",
    "| **Redpanda** | v25.3.7 | Message broker | Kafka-compatible, C++, 400 MB RAM |\n",
    "| **Apache Flink** | 2.0.1 (Java 17) | Stream/batch processor | config.yaml (not flink-conf.yaml) |\n",
    "| **Iceberg Flink runtime** | 1.10.1 for Flink 2.0 | Table format integration | iceberg-flink-runtime-2.0-1.10.1.jar |\n",
    "| **Kafka connector** | 4.0.1-2.0 | Flink↔Redpanda bridge | Same JAR works with Kafka and Redpanda |\n",
    "| **Apache Iceberg** | 1.10.1 | Open table format | ACID, time travel, deletion vectors |\n",
    "| **MinIO** | RELEASE.2025-04-22 | Object storage | S3-compatible, local dev |\n",
    "| **dbt-duckdb** | ≥1.9 | Analytics transformation | iceberg_scan() reads Iceberg directly |\n",
    "| **DuckDB** | ≥1.1 | In-process query engine | 500 MB/s Parquet reads |\n",
    "\n",
    "### P04 vs P01: Accurate Post-Audit Comparison\n",
    "\n",
    "| Dimension | P01 (Kafka 4.0.0) | P04 (Redpanda v25.3.7) | Notes |\n",
    "|-----------|-------------------|------------------------|-------|\n",
    "| Broker runtime | JVM (Java 17) | Native binary (C++/Seastar) | |\n",
    "| Broker startup | ~30s (KRaft negotiation) | ~3s (single binary) | ~27s savings |\n",
    "| Broker memory | ~1.5 GB JVM heap | ~400 MB | ~1.1 GB savings |\n",
    "| Total peak memory | ~5 GB | ~4.2 GB | ~800 MB savings |\n",
    "| Services (always-on) | 5 | 5 | Same (kafka≈redpanda) |\n",
    "| dbt tests | 94/94 (incl. vendor dim) | 91/91 | P01 has 3 extra vendor tests |\n",
    "| Streaming SQL | ✅ 00-init-streaming + 07-streaming | ✅ Same (added Feb 2026) | Now identical |\n",
    "| Dead Letter Queue | ✅ create-topics.sh + Makefile | ✅ Same (fixed Feb 2026) | Now identical |\n",
    "| CPU limits | ✅ JM 1.0, TM 2.0 | ✅ Same (added Feb 2026) | Now identical |\n",
    "| Lakekeeper REST catalog | ✅ opt-in `--profile lakekeeper` | Not included | P01 only, opt-in |\n",
    "| Flink SQL | Identical (`kafka:9092`) | Identical (`redpanda:9092`) | 1 line different |\n",
    "| dbt models | Identical | Identical | Same SQL |\n",
    "\n",
    "> **Key insight:** Everything you learn in P04 transfers 1:1 to P01 and vice versa.\n",
    "> The only code difference is one line in `00-init.sql`: `bootstrap.servers = 'redpanda:9092'`.\n",
    "> All Flink SQL, dbt models, Makefile targets, and Iceberg table definitions are identical.\n",
    "\n",
    "### Medallion Architecture (Bronze → Silver → Gold)\n",
    "\n",
    "```\n",
    "BRONZE  (raw landing)           SILVER  (trusted)              GOLD  (analytics)\n",
    "──────────────────────          ─────────────────────          ─────────────────\n",
    "• All raw events                • Quality-filtered             • fct_trips (star)\n",
    "• Original column names         • Deduplicated                 • dim_dates\n",
    "• Timestamps parsed             • Type-cast (INT, DECIMAL)     • dim_locations\n",
    "• ingestion_ts added            • Partitioned by date          • dim_payment_types\n",
    "• ~10,000 rows (all)            • ~9,855 rows (98.5%)          • dim_vendors\n",
    "• format-version=1              • format-version=2             • mart_daily_revenue\n",
    "• Unpartitioned                 • PARTITIONED BY pickup_date   • mart_hourly_demand\n",
    "                                                               • mart_location_perf\n",
    "Flink owns Bronze+Silver ──────────────────────────────► dbt owns Gold\n",
    "```\n",
    "\n",
    "**Separation of concerns:** Flink does what Flink is uniquely good at (ordering, dedup, type\n",
    "coercion at stream speed). Business logic (trip duration, tip %, speed) lives in dbt where\n",
    "it is version-controlled, tested, and documented in SQL that analysts can read.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shared Infrastructure\n",
    "\n",
    "The `shared/` directory is reused across all 24 pipelines. These files are **identical** in P01\n",
    "and P04 — the only difference is the `BROKER_URL` environment variable in Docker Compose.\n",
    "\n",
    "### 2.1 Flink Dockerfile\n",
    "\n",
    "Pre-installs 7 JARs at image build time so container startup is fast.\n",
    "\n",
    "**Why 7 JARs?**\n",
    "\n",
    "| JAR | Purpose |\n",
    "|-----|---------|\n",
    "| `iceberg-flink-runtime-2.0-1.10.1.jar` | Iceberg table format sink + source |\n",
    "| `flink-sql-connector-kafka-4.0.1-2.0.jar` | Kafka/Redpanda source connector |\n",
    "| `hadoop-aws-3.3.4.jar` | S3A filesystem for MinIO |\n",
    "| `hadoop-common-3.3.4.jar` | Core Hadoop (S3A depends on it) |\n",
    "| `aws-java-sdk-bundle-1.12.367.jar` | AWS SDK (S3A credential management) |\n",
    "| `iceberg-aws-bundle-1.10.1.jar` | Iceberg AWS utilities + S3FileIO |\n",
    "| *(Flink base)* | JobManager, TaskManager, SQL client |\n",
    "\n",
    "> **Flink 2.0 breaking change:** Config file renamed from `flink-conf.yaml` → `config.yaml`.\n",
    "> The Dockerfile copies `config.yaml` — any pipeline still using `flink-conf.yaml` will silently\n",
    "> use default settings, causing mysterious failures. All 10 Flink pipelines were migrated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../shared/docker/flink.Dockerfile\n",
    "# =============================================================================\n",
    "# Shared Flink Image with Kafka + Iceberg Connectors\n",
    "# =============================================================================\n",
    "# Base: Flink 2.0.1 (Java 17)\n",
    "# Adds: Kafka SQL connector, Iceberg Flink runtime, AWS S3 bundle\n",
    "# Used by: Pipelines 01, 04, 07-09, 11-12, 16-18, 21, 23\n",
    "# =============================================================================\n",
    "\n",
    "FROM flink:2.0.1-java17\n",
    "\n",
    "# Connector versions (Flink 2.0 requires new connector builds)\n",
    "ARG FLINK_KAFKA_CONNECTOR_VERSION=4.0.1-2.0\n",
    "ARG ICEBERG_VERSION=1.10.1\n",
    "ARG FLINK_MAJOR_MINOR=2.0\n",
    "\n",
    "# Download Kafka SQL connector (fat jar)\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/${FLINK_KAFKA_CONNECTOR_VERSION}/flink-sql-connector-kafka-${FLINK_KAFKA_CONNECTOR_VERSION}.jar\" \\\n",
    "    && echo \"Kafka SQL connector downloaded\"\n",
    "\n",
    "# Download Iceberg Flink runtime\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-flink-runtime-${FLINK_MAJOR_MINOR}/${ICEBERG_VERSION}/iceberg-flink-runtime-${FLINK_MAJOR_MINOR}-${ICEBERG_VERSION}.jar\" \\\n",
    "    && echo \"Iceberg Flink runtime downloaded\"\n",
    "\n",
    "# Download Iceberg AWS bundle (for S3FileIO with MinIO)\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/${ICEBERG_VERSION}/iceberg-aws-bundle-${ICEBERG_VERSION}.jar\" \\\n",
    "    && echo \"Iceberg AWS bundle downloaded\"\n",
    "\n",
    "# Download Hadoop client (required for Iceberg Hadoop catalog)\n",
    "ARG HADOOP_VERSION=3.3.6\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/${HADOOP_VERSION}/hadoop-client-api-${HADOOP_VERSION}.jar\" \\\n",
    "    && wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/${HADOOP_VERSION}/hadoop-client-runtime-${HADOOP_VERSION}.jar\" \\\n",
    "    && echo \"Hadoop client jars downloaded\"\n",
    "\n",
    "# Download Hadoop AWS module (for S3A filesystem in Iceberg Hadoop catalog)\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar\" \\\n",
    "    && echo \"Hadoop AWS jar downloaded\"\n",
    "\n",
    "# Download AWS SDK v1 bundle (required by hadoop-aws)\n",
    "ARG AWS_SDK_VERSION=1.12.367\n",
    "RUN wget -q -P /opt/flink/lib/ \\\n",
    "    \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_SDK_VERSION}/aws-java-sdk-bundle-${AWS_SDK_VERSION}.jar\" \\\n",
    "    && echo \"AWS SDK bundle downloaded\"\n",
    "\n",
    "# Enable S3 filesystem plugin (for Flink checkpoints on S3)\n",
    "RUN mkdir -p /opt/flink/plugins/s3-fs-hadoop \\\n",
    "    && cp /opt/flink/opt/flink-s3-fs-hadoop-*.jar /opt/flink/plugins/s3-fs-hadoop/ 2>/dev/null || true\n",
    "\n",
    "# Verify all JARs are present\n",
    "RUN ls -la /opt/flink/lib/flink-sql-connector-kafka*.jar \\\n",
    "           /opt/flink/lib/iceberg-flink-runtime*.jar \\\n",
    "           /opt/flink/lib/iceberg-aws-bundle*.jar \\\n",
    "           /opt/flink/lib/hadoop-client-*.jar \\\n",
    "           /opt/flink/lib/hadoop-aws-*.jar \\\n",
    "           /opt/flink/lib/aws-java-sdk-bundle-*.jar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 dbt Dockerfile\n",
    "\n",
    "Slim Python image with dbt-core, dbt-duckdb, and the DuckDB Iceberg/httpfs extensions.\n",
    "\n",
    "> **Design choice:** `dbt deps` runs at container *startup* (entrypoint), not at *build time*.\n",
    "> This ensures packages.yml is always respected and allows running without rebuilding the image.\n",
    "> The trade-off: first run takes ~10s longer. For production CI, pre-bake deps into the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../shared/docker/dbt.Dockerfile\n",
    "FROM python:3.12-slim\n",
    "\n",
    "# Build argument to select the dbt adapter\n",
    "ARG DBT_ADAPTER=dbt-duckdb\n",
    "ARG DBT_ADAPTER_VERSION=\">=1.8\"\n",
    "\n",
    "WORKDIR /dbt\n",
    "\n",
    "# Install dbt with the specified adapter\n",
    "RUN pip install --no-cache-dir \\\n",
    "    \"dbt-core>=1.8\" \\\n",
    "    \"${DBT_ADAPTER}${DBT_ADAPTER_VERSION}\" \\\n",
    "    pyarrow \\\n",
    "    pandas\n",
    "\n",
    "# For dbt-duckdb with Iceberg support\n",
    "RUN if [ \"$DBT_ADAPTER\" = \"dbt-duckdb\" ]; then \\\n",
    "    pip install --no-cache-dir duckdb; \\\n",
    "    fi\n",
    "\n",
    "# Copy dbt project (mounted or copied at build time)\n",
    "COPY dbt_project/ /dbt/\n",
    "\n",
    "ENTRYPOINT [\"dbt\"]\n",
    "CMD [\"build\", \"--profiles-dir\", \".\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Generator\n",
    "\n",
    "Reads NYC Yellow Taxi parquet files and produces JSON events to Redpanda (or Kafka).\n",
    "Key design decisions:\n",
    "\n",
    "| Decision | Implementation | Why |\n",
    "|----------|---------------|-----|\n",
    "| **Idempotent producer** | `enable.idempotence=True, acks=all` | Prevents duplicate events on retry |\n",
    "| **pyarrow** for parquet | `pyarrow.parquet.ParquetFile` | Columnar reads, lazy (only loads requested rows) |\n",
    "| **NaN → null** | `replace({float('nan'): None})` | JSON doesn't have NaN; Flink expects null |\n",
    "| **confluent_kafka** | Works with Kafka AND Redpanda | Same wire protocol |\n",
    "| **Metrics** | Every 1000 events: throughput + p95/p99 latency | Observability at the source |\n",
    "| **Modes** | `burst` / `realtime` / `batch` | benchmark / demo / CI respectively |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../shared/data-generator/requirements.txt\n",
    "pyarrow>=14.0.0\n",
    "confluent-kafka>=2.3.0\n",
    "orjson>=3.9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../shared/data-generator/generator.py\n",
    "\"\"\"Taxi trip event generator.\n",
    "\n",
    "Reads NYC Yellow Taxi parquet data and produces events to a Kafka-compatible\n",
    "broker (Kafka or Redpanda). Supports three modes:\n",
    "  - burst:    As fast as possible (benchmarking)\n",
    "  - realtime: Simulates actual event-time spacing\n",
    "  - batch:    Sends events in configurable batch sizes with delays\n",
    "\n",
    "Configuration via environment variables:\n",
    "  BROKER_URL    Kafka/Redpanda bootstrap servers  (default: localhost:9092)\n",
    "  TOPIC         Target topic name                  (default: taxi.raw_trips)\n",
    "  MODE          burst | realtime | batch           (default: burst)\n",
    "  RATE_LIMIT    Max events/sec in burst mode, 0=unlimited (default: 0)\n",
    "  BATCH_SIZE    Events per batch in batch mode     (default: 1000)\n",
    "  BATCH_DELAY   Seconds between batches            (default: 1.0)\n",
    "  DATA_PATH     Path to parquet file               (default: /data/yellow_tripdata_2024-01.parquet)\n",
    "  MAX_EVENTS    Stop after N events, 0=all         (default: 0)\n",
    "\n",
    "Usage:\n",
    "    python generator.py\n",
    "    python generator.py --mode burst --broker localhost:9092\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import orjson\n",
    "import pyarrow.parquet as pq\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "\n",
    "def delivery_callback(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"  [ERROR] Delivery failed: {err}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def read_parquet(path: str, max_events: int = 0):\n",
    "    \"\"\"Yield rows from parquet file as dicts.\"\"\"\n",
    "    table = pq.read_table(path)\n",
    "    total = table.num_rows if max_events == 0 else min(max_events, table.num_rows)\n",
    "    print(f\"  Source: {path} ({table.num_rows:,} rows, sending {total:,})\")\n",
    "\n",
    "    batches = table.to_batches(max_chunksize=10_000)\n",
    "    sent = 0\n",
    "    for batch in batches:\n",
    "        for row in batch.to_pylist():\n",
    "            if sent >= total:\n",
    "                return\n",
    "            # Convert timestamps to ISO strings for JSON serialization\n",
    "            for key, val in row.items():\n",
    "                if isinstance(val, datetime):\n",
    "                    row[key] = val.isoformat()\n",
    "            yield row\n",
    "            sent += 1\n",
    "\n",
    "\n",
    "def create_producer(broker_url: str) -> Producer:\n",
    "    conf = {\n",
    "        \"bootstrap.servers\": broker_url,\n",
    "        \"enable.idempotence\": True,\n",
    "        \"acks\": \"all\",\n",
    "        \"linger.ms\": 5,\n",
    "        \"batch.num.messages\": 10000,\n",
    "        \"queue.buffering.max.messages\": 500000,\n",
    "        \"queue.buffering.max.kbytes\": 1048576,\n",
    "        \"compression.type\": \"lz4\",\n",
    "    }\n",
    "    return Producer(conf)\n",
    "\n",
    "\n",
    "def produce_burst(producer: Producer, topic: str, rows, rate_limit: int):\n",
    "    \"\"\"Produce as fast as possible, optionally rate-limited.\"\"\"\n",
    "    count = 0\n",
    "    start = time.perf_counter()\n",
    "    last_report = start\n",
    "\n",
    "    for row in rows:\n",
    "        key = str(row.get(\"PULocationID\", \"\")).encode(\"utf-8\")\n",
    "        value = orjson.dumps(row)\n",
    "        producer.produce(topic, value=value, key=key, callback=delivery_callback)\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            producer.poll(0)\n",
    "            now = time.perf_counter()\n",
    "            if now - last_report >= 5.0:\n",
    "                elapsed = now - start\n",
    "                rate = count / elapsed\n",
    "                print(f\"  Produced {count:,} events ({rate:,.0f} evt/s)\")\n",
    "                last_report = now\n",
    "\n",
    "        # Rate limiting\n",
    "        if rate_limit > 0 and count % rate_limit == 0:\n",
    "            elapsed = time.perf_counter() - start\n",
    "            expected = count / rate_limit\n",
    "            if elapsed < expected:\n",
    "                time.sleep(expected - elapsed)\n",
    "\n",
    "    producer.flush(timeout=30)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    rate = count / elapsed if elapsed > 0 else 0\n",
    "    return count, elapsed, rate\n",
    "\n",
    "\n",
    "def produce_batch(producer: Producer, topic: str, rows, batch_size: int, batch_delay: float):\n",
    "    \"\"\"Produce in fixed-size batches with delays between them.\"\"\"\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_buffer = []\n",
    "    for row in rows:\n",
    "        batch_buffer.append(row)\n",
    "        if len(batch_buffer) >= batch_size:\n",
    "            for r in batch_buffer:\n",
    "                key = str(r.get(\"PULocationID\", \"\")).encode(\"utf-8\")\n",
    "                value = orjson.dumps(r)\n",
    "                producer.produce(topic, value=value, key=key, callback=delivery_callback)\n",
    "                count += 1\n",
    "            producer.flush(timeout=30)\n",
    "            batch_count += 1\n",
    "            elapsed = time.perf_counter() - start\n",
    "            rate = count / elapsed if elapsed > 0 else 0\n",
    "            print(f\"  Batch {batch_count}: {count:,} total ({rate:,.0f} evt/s)\")\n",
    "            batch_buffer = []\n",
    "            time.sleep(batch_delay)\n",
    "\n",
    "    # Final partial batch\n",
    "    if batch_buffer:\n",
    "        for r in batch_buffer:\n",
    "            key = str(r.get(\"PULocationID\", \"\")).encode(\"utf-8\")\n",
    "            value = orjson.dumps(r)\n",
    "            producer.produce(topic, value=value, key=key, callback=delivery_callback)\n",
    "            count += 1\n",
    "        producer.flush(timeout=30)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    rate = count / elapsed if elapsed > 0 else 0\n",
    "    return count, elapsed, rate\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Taxi trip event generator\")\n",
    "    parser.add_argument(\"--broker\", default=os.environ.get(\"BROKER_URL\", \"localhost:9092\"))\n",
    "    parser.add_argument(\"--topic\", default=os.environ.get(\"TOPIC\", \"taxi.raw_trips\"))\n",
    "    parser.add_argument(\"--mode\", default=os.environ.get(\"MODE\", \"burst\"),\n",
    "                        choices=[\"burst\", \"realtime\", \"batch\"])\n",
    "    parser.add_argument(\"--rate-limit\", type=int,\n",
    "                        default=int(os.environ.get(\"RATE_LIMIT\", \"0\")))\n",
    "    parser.add_argument(\"--batch-size\", type=int,\n",
    "                        default=int(os.environ.get(\"BATCH_SIZE\", \"1000\")))\n",
    "    parser.add_argument(\"--batch-delay\", type=float,\n",
    "                        default=float(os.environ.get(\"BATCH_DELAY\", \"1.0\")))\n",
    "    parser.add_argument(\"--data-path\",\n",
    "                        default=os.environ.get(\"DATA_PATH\", \"/data/yellow_tripdata_2024-01.parquet\"))\n",
    "    parser.add_argument(\"--max-events\", type=int,\n",
    "                        default=int(os.environ.get(\"MAX_EVENTS\", \"0\")))\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Taxi Trip Event Generator\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Broker:     {args.broker}\")\n",
    "    print(f\"  Topic:      {args.topic}\")\n",
    "    print(f\"  Mode:       {args.mode}\")\n",
    "    print(f\"  Data:       {args.data_path}\")\n",
    "    max_events_str = \"all\" if args.max_events == 0 else f\"{args.max_events:,}\"\n",
    "    print(f\"  Max events: {max_events_str}\")\n",
    "    print()\n",
    "\n",
    "    producer = create_producer(args.broker)\n",
    "    rows = read_parquet(args.data_path, args.max_events)\n",
    "\n",
    "    if args.mode == \"burst\":\n",
    "        count, elapsed, rate = produce_burst(producer, args.topic, rows, args.rate_limit)\n",
    "    elif args.mode == \"batch\":\n",
    "        count, elapsed, rate = produce_batch(\n",
    "            producer, args.topic, rows, args.batch_size, args.batch_delay\n",
    "        )\n",
    "    else:\n",
    "        # realtime mode: use burst with rate limiting to approximate real-time\n",
    "        count, elapsed, rate = produce_burst(producer, args.topic, rows, rate_limit=5000)\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  GENERATOR COMPLETE\")\n",
    "    print(f\"  Events:  {count:,}\")\n",
    "    print(f\"  Elapsed: {elapsed:.2f}s\")\n",
    "    print(f\"  Rate:    {rate:,.0f} events/sec\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Write metrics for benchmark collection\n",
    "    metrics_path = os.environ.get(\"METRICS_PATH\", \"/tmp/generator_metrics.json\")\n",
    "    metrics = {\n",
    "        \"events\": count,\n",
    "        \"elapsed_seconds\": round(elapsed, 3),\n",
    "        \"events_per_second\": round(rate, 1),\n",
    "        \"mode\": args.mode,\n",
    "        \"broker\": args.broker,\n",
    "        \"topic\": args.topic,\n",
    "    }\n",
    "    with open(metrics_path, \"wb\") as f:\n",
    "        f.write(orjson.dumps(metrics))\n",
    "    print(f\"  Metrics written to {metrics_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Event Schema (JSON Schema)\n",
    "\n",
    "Documents the taxi trip event contract. Not enforced at ingestion (no Schema Registry) —\n",
    "enforcement happens via Flink SQL column definitions and dbt tests downstream.\n",
    "\n",
    "> **Why no Schema Registry?** For JSON-over-Kafka at 10k–1M events/day, the overhead of\n",
    "> schema registration (REST API call per producer start, Avro serialization) adds complexity\n",
    "> without proportional benefit. Schema Registry makes sense for Avro/Protobuf at 10M+/day\n",
    "> or when multiple teams need a formal contract registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../shared/schemas/taxi_trip.json\n",
    "{\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"title\": \"TaxiTrip\",\n",
    "  \"description\": \"NYC Yellow Taxi trip record. Field names match the raw parquet source exactly.\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"VendorID\": {\"type\": [\"integer\", \"null\"], \"description\": \"TPEP provider: 1=Creative Mobile Technologies, 2=VeriFone Inc.\"},\n",
    "    \"tpep_pickup_datetime\": {\"type\": \"string\", \"format\": \"date-time\", \"description\": \"Meter engaged timestamp (ISO 8601)\"},\n",
    "    \"tpep_dropoff_datetime\": {\"type\": \"string\", \"format\": \"date-time\", \"description\": \"Meter disengaged timestamp (ISO 8601)\"},\n",
    "    \"passenger_count\": {\"type\": [\"integer\", \"null\"], \"description\": \"Number of passengers (driver-entered)\"},\n",
    "    \"trip_distance\": {\"type\": [\"number\", \"null\"], \"description\": \"Trip distance in miles from taximeter\"},\n",
    "    \"RatecodeID\": {\"type\": [\"integer\", \"null\"], \"description\": \"Rate code: 1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester, 5=Negotiated, 6=Group\"},\n",
    "    \"store_and_fwd_flag\": {\"type\": [\"string\", \"null\"], \"description\": \"Y=stored then forwarded, N=not a store-and-forward trip\"},\n",
    "    \"PULocationID\": {\"type\": [\"integer\", \"null\"], \"description\": \"TLC Taxi Zone pickup location ID\"},\n",
    "    \"DOLocationID\": {\"type\": [\"integer\", \"null\"], \"description\": \"TLC Taxi Zone dropoff location ID\"},\n",
    "    \"payment_type\": {\"type\": [\"integer\", \"null\"], \"description\": \"Payment method: 1=Credit, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided\"},\n",
    "    \"fare_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Time-and-distance fare in dollars\"},\n",
    "    \"extra\": {\"type\": [\"number\", \"null\"], \"description\": \"Misc extras and surcharges\"},\n",
    "    \"mta_tax\": {\"type\": [\"number\", \"null\"], \"description\": \"MTA tax\"},\n",
    "    \"tip_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Tip amount\"},\n",
    "    \"tolls_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Total tolls paid during trip\"},\n",
    "    \"improvement_surcharge\": {\"type\": [\"number\", \"null\"], \"description\": \"$0.30 improvement surcharge\"},\n",
    "    \"total_amount\": {\"type\": [\"number\", \"null\"], \"description\": \"Total amount charged to passengers\"},\n",
    "    \"congestion_surcharge\": {\"type\": [\"number\", \"null\"], \"description\": \"NYC congestion surcharge\"},\n",
    "    \"Airport_fee\": {\"type\": [\"number\", \"null\"], \"description\": \"$1.25 for pickups at LaGuardia and JFK\"}\n",
    "  },\n",
    "  \"required\": [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docker Compose: Container Orchestration\n",
    "\n",
    "P04 runs **6 always-on containers** plus 2 profile containers:\n",
    "\n",
    "| Container | Image | CPU limit | Memory limit | Role |\n",
    "|-----------|-------|-----------|-------------|------|\n",
    "| `p04-redpanda` | `redpandadata/redpanda:v25.3.7` | (Redpanda manages via --smp 1) | 1.5 GB | Kafka-compatible broker |\n",
    "| `p04-redpanda-console` | `redpandadata/console:v3.2.2` | — | — | Web UI: topic browser, consumer lag |\n",
    "| `p04-minio` | `minio/minio:RELEASE.2025-04-22...` | — | 1 GB | S3-compatible object store |\n",
    "| `p04-mc-init` | `minio/mc:RELEASE.2025-05-21...` | — | — | One-shot: creates `warehouse` bucket |\n",
    "| `p04-flink-jobmanager` | flink-custom (shared Dockerfile) | **1.0 CPU** | 2 GB | Flink coordinator |\n",
    "| `p04-flink-taskmanager` | flink-custom (shared Dockerfile) | **2.0 CPU** | 2.5 GB | Flink worker (executes SQL) |\n",
    "| `p04-dbt` (profile: dbt) | dbt-custom (shared Dockerfile) | — | — | dbt build + tests |\n",
    "| `p04-data-generator` (profile: generator) | data-generator | — | — | Taxi event producer |\n",
    "\n",
    "**CPU limits (added Feb 2026 audit):** Without `cpus` limits, all containers compete for\n",
    "Docker Desktop's shared CPU pool. During Flink processing, the TaskManager can consume all\n",
    "cores, starving MinIO and causing S3A write timeouts. Capping TM at 2 CPUs prevents this.\n",
    "\n",
    "### Network topology\n",
    "- All services: `p04-pipeline-net` (bridge, isolated from host)\n",
    "- Redpanda Kafka API inside network: `redpanda:9092`\n",
    "- Redpanda Kafka API external (for local tools): `localhost:19092`\n",
    "- MinIO S3 API inside network: `minio:9000`\n",
    "- Flink job API: `flink-jobmanager:8081` (internal), `localhost:8081` (external)\n",
    "\n",
    "### Healthcheck chain\n",
    "```\n",
    "minio healthy → mc-init completes → (Flink services start)\n",
    "redpanda healthy → flink-jobmanager starts → flink-taskmanager starts\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/docker-compose.yml\n",
    "# =============================================================================\n",
    "# Pipeline 04: Redpanda + Flink + Iceberg\n",
    "# =============================================================================\n",
    "# Architecture: Redpanda → Flink SQL → Iceberg (on MinIO) → dbt (DuckDB)\n",
    "# Fork of Pipeline 01 with Redpanda replacing Kafka + Schema Registry.\n",
    "# =============================================================================\n",
    "\n",
    "x-flink-common: &flink-common\n",
    "  build:\n",
    "    context: .\n",
    "    dockerfile: ../../shared/docker/flink.Dockerfile\n",
    "  environment: &flink-env\n",
    "    FLINK_PROPERTIES: |\n",
    "      jobmanager.rpc.address: flink-jobmanager\n",
    "      taskmanager.numberOfTaskSlots: 4\n",
    "      parallelism.default: 2\n",
    "      state.backend: hashmap\n",
    "      state.checkpoints.dir: file:///tmp/flink-checkpoints\n",
    "      execution.checkpointing.interval: 30s\n",
    "      rest.flamegraph.enabled: true\n",
    "      classloader.check-leaked-classloader: false\n",
    "  networks:\n",
    "    - pipeline-net\n",
    "\n",
    "services:\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Redpanda (single node - replaces Kafka + Schema Registry)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  redpanda:\n",
    "    image: redpandadata/redpanda:v25.3.7\n",
    "    container_name: p04-redpanda\n",
    "    hostname: redpanda\n",
    "    restart: unless-stopped\n",
    "    command:\n",
    "      - redpanda start\n",
    "      - --smp 1\n",
    "      - --memory 1G\n",
    "      - --overprovisioned\n",
    "      - --node-id 0\n",
    "      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092\n",
    "      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:19092\n",
    "      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082\n",
    "      - --advertise-pandaproxy-addr internal://redpanda:8082,external://localhost:18082\n",
    "    ports:\n",
    "      - \"19092:19092\"  # Kafka API\n",
    "      - \"18082:18082\"  # Pandaproxy\n",
    "      - \"9644:9644\"    # Admin API\n",
    "    volumes:\n",
    "      - redpanda-data:/var/lib/redpanda/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"rpk cluster health | grep -E 'Healthy:.+true' || exit 1\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 15\n",
    "      start_period: 20s\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 1.5G\n",
    "        reservations:\n",
    "          memory: 512m\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Redpanda Console (web UI for topic browsing and consumer lag)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  redpanda-console:\n",
    "    image: redpandadata/console:v3.2.2\n",
    "    container_name: p04-redpanda-console\n",
    "    hostname: redpanda-console\n",
    "    restart: unless-stopped\n",
    "    ports:\n",
    "      - \"8085:8080\"\n",
    "    environment:\n",
    "      KAFKA_BROKERS: redpanda:9092\n",
    "    depends_on:\n",
    "      redpanda:\n",
    "        condition: service_healthy\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # MinIO (S3-compatible object storage for Iceberg warehouse)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  minio:\n",
    "    image: minio/minio:RELEASE.2025-04-22T22-12-26Z\n",
    "    container_name: p04-minio\n",
    "    hostname: minio\n",
    "    restart: unless-stopped\n",
    "    ports:\n",
    "      - \"9000:9000\"\n",
    "      - \"9001:9001\"\n",
    "    environment:\n",
    "      MINIO_ROOT_USER: minioadmin\n",
    "      MINIO_ROOT_PASSWORD: minioadmin\n",
    "    command: server /data --console-address \":9001\"\n",
    "    healthcheck:\n",
    "      test: mc ready local || exit 1\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 10\n",
    "      start_period: 10s\n",
    "    volumes:\n",
    "      - minio-data:/data\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 1G\n",
    "        reservations:\n",
    "          memory: 256m\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # MinIO Client Init (create warehouse bucket)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  mc-init:\n",
    "    image: minio/mc:RELEASE.2025-05-21T01-59-54Z\n",
    "    container_name: p04-mc-init\n",
    "    depends_on:\n",
    "      minio:\n",
    "        condition: service_healthy\n",
    "    entrypoint: >\n",
    "      /bin/sh -c \"\n",
    "      mc alias set myminio http://minio:9000 minioadmin minioadmin &&\n",
    "      mc mb myminio/warehouse --ignore-existing &&\n",
    "      mc anonymous set download myminio/warehouse &&\n",
    "      echo 'Bucket warehouse created successfully'\n",
    "      \"\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Flink JobManager\n",
    "  # ---------------------------------------------------------------------------\n",
    "  flink-jobmanager:\n",
    "    <<: *flink-common\n",
    "    container_name: p04-flink-jobmanager\n",
    "    hostname: flink-jobmanager\n",
    "    restart: unless-stopped\n",
    "    command: jobmanager\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "      - \"9249:9249\"  # Prometheus metrics\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 2G\n",
    "          cpus: '1.0'\n",
    "        reservations:\n",
    "          memory: 1G\n",
    "    volumes:\n",
    "      - ./flink/sql:/opt/flink/sql:ro\n",
    "      - ./flink/conf/config.yaml:/opt/flink/conf/config.yaml:ro\n",
    "      - ./flink/conf/core-site.xml:/opt/hadoop/conf/core-site.xml:ro\n",
    "      - flink-checkpoints:/tmp/flink-checkpoints\n",
    "    depends_on:\n",
    "      redpanda:\n",
    "        condition: service_healthy\n",
    "      mc-init:\n",
    "        condition: service_completed_successfully\n",
    "    healthcheck:\n",
    "      test: curl -f http://localhost:8081/overview || exit 1\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 15\n",
    "      start_period: 30s\n",
    "    environment:\n",
    "      <<: *flink-env\n",
    "      AWS_ACCESS_KEY_ID: minioadmin\n",
    "      AWS_SECRET_ACCESS_KEY: minioadmin\n",
    "      AWS_REGION: us-east-1\n",
    "      HADOOP_CONF_DIR: /opt/hadoop/conf\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Flink TaskManager\n",
    "  # ---------------------------------------------------------------------------\n",
    "  flink-taskmanager:\n",
    "    <<: *flink-common\n",
    "    container_name: p04-flink-taskmanager\n",
    "    hostname: flink-taskmanager\n",
    "    restart: unless-stopped\n",
    "    command: taskmanager\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 2.5G\n",
    "          cpus: '2.0'\n",
    "        reservations:\n",
    "          memory: 2G\n",
    "    volumes:\n",
    "      - ./flink/conf/core-site.xml:/opt/hadoop/conf/core-site.xml:ro\n",
    "      - flink-checkpoints:/tmp/flink-checkpoints\n",
    "    depends_on:\n",
    "      flink-jobmanager:\n",
    "        condition: service_healthy\n",
    "    environment:\n",
    "      <<: *flink-env\n",
    "      AWS_ACCESS_KEY_ID: minioadmin\n",
    "      AWS_SECRET_ACCESS_KEY: minioadmin\n",
    "      AWS_REGION: us-east-1\n",
    "      HADOOP_CONF_DIR: /opt/hadoop/conf\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # dbt (DuckDB adapter - reads Iceberg tables from MinIO)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  dbt:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: ../../shared/docker/dbt.Dockerfile\n",
    "      args:\n",
    "        DBT_ADAPTER: dbt-duckdb\n",
    "    container_name: p04-dbt\n",
    "    volumes:\n",
    "      - ./dbt_project:/dbt\n",
    "    working_dir: /dbt\n",
    "    entrypoint: [\"/bin/sh\", \"-c\"]\n",
    "    command: [\"dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir .\"]\n",
    "    environment:\n",
    "      AWS_ACCESS_KEY_ID: minioadmin\n",
    "      AWS_SECRET_ACCESS_KEY: minioadmin\n",
    "      AWS_ENDPOINT_URL: http://minio:9000\n",
    "      AWS_REGION: us-east-1\n",
    "      DBT_PROFILES_DIR: /dbt\n",
    "    depends_on:\n",
    "      minio:\n",
    "        condition: service_healthy\n",
    "    profiles:\n",
    "      - dbt\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "  # ---------------------------------------------------------------------------\n",
    "  # Data Generator (reads parquet, produces to Redpanda)\n",
    "  # ---------------------------------------------------------------------------\n",
    "  data-generator:\n",
    "    build:\n",
    "      context: ../../shared/data-generator/\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: p04-data-generator\n",
    "    volumes:\n",
    "      - ../../data:/data:ro\n",
    "    environment:\n",
    "      BROKER_URL: redpanda:9092\n",
    "      TOPIC: taxi.raw_trips\n",
    "      MODE: burst\n",
    "      DATA_PATH: /data/yellow_tripdata_2024-01.parquet\n",
    "    depends_on:\n",
    "      redpanda:\n",
    "        condition: service_healthy\n",
    "    profiles:\n",
    "      - generator\n",
    "    networks:\n",
    "      - pipeline-net\n",
    "\n",
    "# =============================================================================\n",
    "# Volumes\n",
    "# =============================================================================\n",
    "volumes:\n",
    "  minio-data:\n",
    "    driver: local\n",
    "  flink-checkpoints:\n",
    "    driver: local\n",
    "  redpanda-data:\n",
    "    driver: local\n",
    "\n",
    "# =============================================================================\n",
    "# Networks\n",
    "# =============================================================================\n",
    "networks:\n",
    "  pipeline-net:\n",
    "    name: p04-pipeline-net\n",
    "    driver: bridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Redpanda Topics: Event Ingestion + Dead Letter Queue\n",
    "\n",
    "### Two-Topic Design: Primary + DLQ\n",
    "\n",
    "```\n",
    "Data Generator\n",
    "     │\n",
    "     │  valid JSON events\n",
    "     ▼\n",
    "taxi.raw_trips          ← 3 partitions, 72h retention, primary stream\n",
    "     │\n",
    "     │  (Flink reads this, processes into Bronze/Silver)\n",
    "     ▼\n",
    "[malformed events that fail Flink type parsing are sent to DLQ]\n",
    "     │\n",
    "     ▼\n",
    "taxi.raw_trips.dlq      ← 1 partition, 7-day retention, dead letter queue\n",
    "```\n",
    "\n",
    "### Why a Dead Letter Queue?\n",
    "\n",
    "Without a DLQ, a single malformed event (wrong timestamp format, null where not expected)\n",
    "can block an entire Kafka partition. Flink's JSON deserializer will throw on parse failure\n",
    "and stop consuming that partition. A DLQ gives you:\n",
    "- **Visibility:** You can see which events failed and why\n",
    "- **Replay:** After fixing the schema/parser, you can re-consume the DLQ\n",
    "- **Non-blocking:** Bad events don't stop good events from flowing\n",
    "\n",
    "### Topic Configuration\n",
    "\n",
    "| Setting | Primary | DLQ | Reason |\n",
    "|---------|---------|-----|--------|\n",
    "| Partitions | 3 | 1 | DLQ is low-volume, no need for parallelism |\n",
    "| `retention.ms` | 259,200,000 (72h) | 604,800,000 (7 days) | DLQ kept longer for investigation |\n",
    "| `cleanup.policy` | `delete` | `delete` | Time-based expiry (not compaction) |\n",
    "\n",
    "### Makefile vs create-topics.sh\n",
    "\n",
    "The Makefile's `create-topics` target creates both topics **inline** via `rpk`. The `create-topics.sh`\n",
    "shell script also creates both topics. Either works — the Makefile target is the standard entry point.\n",
    "\n",
    "> **Audit fix (Feb 2026):** The original Makefile `create-topics` only created the primary topic,\n",
    "> bypassing the DLQ. Both the shell script and Makefile now create both topics consistently.\n",
    "\n",
    "### rpk vs kafka-topics.sh\n",
    "\n",
    "```bash\n",
    "# Kafka (JVM, ~3s including JVM startup):\n",
    "kafka-topics.sh --bootstrap-server kafka:9092 --create --topic taxi.raw_trips   --partitions 3 --config retention.ms=259200000\n",
    "\n",
    "# Redpanda rpk (native Go binary, ~50ms):\n",
    "rpk topic create taxi.raw_trips --brokers redpanda:9092 --partitions 3   --topic-config retention.ms=259200000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/kafka/create-topics.sh\n",
    "#!/bin/bash\n",
    "# =============================================================================\n",
    "# Pipeline 04: Create Redpanda Topics\n",
    "# =============================================================================\n",
    "# Creates the required topics for the taxi trip streaming pipeline.\n",
    "# Uses rpk (Redpanda CLI) instead of kafka-topics.sh.\n",
    "#\n",
    "# Usage:\n",
    "#   docker compose exec redpanda rpk topic create ...\n",
    "#   -- or --\n",
    "#   make create-topics\n",
    "# =============================================================================\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"Creating topics via rpk...\"\n",
    "rpk topic create taxi.raw_trips \\\n",
    "    --brokers redpanda:9092 \\\n",
    "    --partitions 3 \\\n",
    "    --replicas 1 \\\n",
    "    --topic-config retention.ms=259200000 \\\n",
    "    --topic-config cleanup.policy=delete || true\n",
    "\n",
    "# Dead Letter Queue: for poison messages that fail processing\n",
    "rpk topic create taxi.raw_trips.dlq \\\n",
    "    --brokers redpanda:9092 \\\n",
    "    --partitions 1 \\\n",
    "    --replicas 1 \\\n",
    "    --topic-config retention.ms=604800000 \\\n",
    "    --topic-config cleanup.policy=delete || true\n",
    "\n",
    "rpk topic list --brokers redpanda:9092\n",
    "echo \"Topic creation complete.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flink Configuration\n",
    "\n",
    "### 5.1 Hadoop core-site.xml — S3A → MinIO Bridge\n",
    "\n",
    "Flink uses Apache Hadoop's S3A filesystem driver to read/write Iceberg files stored in MinIO.\n",
    "This XML file maps S3A URI scheme (`s3a://`) to MinIO's endpoint.\n",
    "\n",
    "```\n",
    "Flink SQL INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "     │   uses S3FileIO → s3a://warehouse/bronze/raw_trips/data/...\n",
    "     ▼\n",
    "Hadoop S3A FileSystem Driver\n",
    "     │   fs.s3a.endpoint = http://minio:9000\n",
    "     │   fs.s3a.path.style.access = true  (MinIO uses path-style, not virtual-hosted)\n",
    "     ▼\n",
    "MinIO HTTP server on port 9000\n",
    "```\n",
    "\n",
    "Without `path.style.access=true`, Hadoop would try to reach `warehouse.minio:9000` (DNS lookup\n",
    "fails in Docker) instead of `minio:9000/warehouse/` (correct Docker network address).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/conf/core-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <!-- MinIO (S3-compatible) Configuration for Hadoop S3A -->\n",
    "    <property>\n",
    "        <name>fs.s3a.endpoint</name>\n",
    "        <value>http://minio:9000</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.access.key</name>\n",
    "        <value>minioadmin</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.secret.key</name>\n",
    "        <value>minioadmin</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.path.style.access</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.impl</name>\n",
    "        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.connection.ssl.enabled</name>\n",
    "        <value>false</value>\n",
    "    </property>\n",
    "</configuration>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Flink Cluster Configuration (config.yaml)\n",
    "\n",
    "> **Flink 2.0 breaking change:** `flink-conf.yaml` → `config.yaml`. If you mount the old\n",
    "> filename, Flink silently uses defaults — no error. This caused mysterious TaskManager failures\n",
    "> until the rename was discovered. All pipelines migrated in the Feb 2026 upgrade.\n",
    "\n",
    "Key settings with explanations:\n",
    "\n",
    "| Setting | Value | Why it matters |\n",
    "|---------|-------|---------------|\n",
    "| `classloader.check-leaked-classloader` | `false` | Iceberg loads classes dynamically; without this Flink throws classloader leak warnings that abort jobs |\n",
    "| `taskmanager.memory.process.size` | `2048m` | Total TM memory (JVM heap + off-heap + metaspace). Must match docker-compose `memory: 2.5G` with headroom |\n",
    "| `parallelism.default` | `2` | 2 task slots for 3-partition topic; adequate for 10k benchmark |\n",
    "| `state.backend` | `hashmap` | In-memory state (fast for batch jobs). For streaming: change to `rocksdb` + configure `state.checkpoints.dir` |\n",
    "| `execution.checkpointing.interval` | `30s` | How often Flink snapshots operator state. Required for exactly-once in streaming mode |\n",
    "\n",
    "> **Streaming mode note:** The config.yaml settings for `state.backend = rocksdb` and\n",
    "> `state.checkpoints.dir = s3a://warehouse/checkpoints/` should be added before running\n",
    "> `make process-streaming` in production. Batch mode doesn't need checkpoint recovery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/conf/config.yaml\n",
    "# =============================================================================\n",
    "# Pipeline 04: Flink Configuration\n",
    "# =============================================================================\n",
    "# Configuration for Flink 2.0.1 with Iceberg + Redpanda connectors.\n",
    "# This file is mounted into the JobManager container.\n",
    "# =============================================================================\n",
    "\n",
    "# Cluster\n",
    "jobmanager.rpc.address: flink-jobmanager\n",
    "jobmanager.rpc.port: 6123\n",
    "jobmanager.bind-host: 0.0.0.0\n",
    "jobmanager.memory.process.size: 1600m\n",
    "\n",
    "taskmanager.bind-host: 0.0.0.0\n",
    "taskmanager.host: flink-taskmanager\n",
    "taskmanager.memory.process.size: 2048m\n",
    "taskmanager.numberOfTaskSlots: 4\n",
    "\n",
    "parallelism.default: 2\n",
    "\n",
    "# REST API (Flink Dashboard)\n",
    "rest.address: 0.0.0.0\n",
    "rest.bind-address: 0.0.0.0\n",
    "rest.port: 8081\n",
    "rest.flamegraph.enabled: true\n",
    "\n",
    "# Checkpointing\n",
    "execution.checkpointing.interval: 30s\n",
    "execution.checkpointing.mode: EXACTLY_ONCE\n",
    "execution.checkpointing.min-pause: 10s\n",
    "execution.checkpointing.timeout: 5min\n",
    "state.backend: rocksdb\n",
    "state.backend.incremental: true\n",
    "state.checkpoints.dir: s3a://warehouse/checkpoints\n",
    "state.savepoints.dir: s3a://warehouse/savepoints\n",
    "\n",
    "# Table / SQL Configuration\n",
    "table.exec.state.ttl: 0\n",
    "table.exec.sink.not-null-enforcer: DROP\n",
    "\n",
    "# Classloader (avoid Iceberg classloader leak with batch DML sync)\n",
    "classloader.check-leaked-classloader: false\n",
    "\n",
    "# S3 (MinIO) filesystem configuration\n",
    "s3.endpoint: http://minio:9000\n",
    "s3.access-key: minioadmin\n",
    "s3.secret-key: minioadmin\n",
    "s3.path.style.access: true\n",
    "\n",
    "# Logging\n",
    "env.log.max: 5\n",
    "env.log.dir: /opt/flink/log\n",
    "\n",
    "# Metrics: Prometheus reporter (scrape port 9249 on JobManager + TaskManager)\n",
    "metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory\n",
    "metrics.reporter.prom.port: 9249\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Flink SQL: Batch Session Initialization\n",
    "\n",
    "`00-init.sql` is the **most important file in the pipeline**. It defines the entire session\n",
    "state that all subsequent SQL files inherit.\n",
    "\n",
    "### How the Flink SQL CLI Session Works\n",
    "\n",
    "```bash\n",
    "# -i flag: \"initialize\" — runs before interactive or -f execution\n",
    "sql-client.sh embedded -i 00-init.sql -f 05-bronze.sql\n",
    "#                       ─────────────  ───────────────\n",
    "#                        init file      batch execute file\n",
    "#\n",
    "# Result: 00-init.sql creates tables and catalog in-session.\n",
    "#         05-bronze.sql can then reference kafka_raw_trips and iceberg_catalog.\n",
    "```\n",
    "\n",
    "The `-i` and `-f` flags share the same SQL session — like running two scripts in the same\n",
    "database connection. Without `-i`, the `-f` file would see an empty session with no tables.\n",
    "\n",
    "### Three Things 00-init.sql Does\n",
    "\n",
    "#### 1. Batch execution settings\n",
    "```sql\n",
    "SET 'execution.runtime-mode' = 'batch';       -- process bounded data and stop\n",
    "SET 'table.dml-sync' = 'true';               -- block after each INSERT until complete\n",
    "```\n",
    "Without `table.dml-sync=true`, the Silver job would start before Bronze finishes writing,\n",
    "resulting in 0 Silver rows (reading an empty or partially-written Bronze table).\n",
    "\n",
    "#### 2. Redpanda source table (virtual — reads from topic)\n",
    "The `kafka_raw_trips` table is never stored anywhere. It's a virtual table that maps\n",
    "Redpanda topic messages to SQL columns. Every row in the topic becomes a SQL row.\n",
    "\n",
    "Key option: `'scan.bounded.mode' = 'latest-offset'` — in batch mode, Flink stops reading\n",
    "at the offset that was \"latest\" when the job started. This makes the batch job finite.\n",
    "Without this, batch mode Flink would wait forever for new messages.\n",
    "\n",
    "#### 3. Iceberg catalog (where table metadata lives)\n",
    "The Hadoop catalog maps Iceberg table names to S3A paths:\n",
    "```\n",
    "iceberg_catalog.bronze.raw_trips\n",
    "    → s3a://warehouse/bronze/raw_trips/\n",
    "        ├── metadata/\n",
    "        │   ├── v1.metadata.json\n",
    "        │   └── snap-....avro\n",
    "        └── data/\n",
    "            └── 00000-0-....parquet\n",
    "```\n",
    "\n",
    "### Event-time Watermark (important for streaming)\n",
    "```sql\n",
    "event_time AS TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss'),\n",
    "WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND\n",
    "```\n",
    "In **batch mode**: watermark is parsed but has no effect. All records are processed without\n",
    "ordering concern. In **streaming mode**: watermark tells Flink to consider events more than\n",
    "10 seconds late as \"late arrivals\" — allows window functions to close properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/00-init.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Flink SQL - Session Initialization\n",
    "-- =============================================================================\n",
    "-- Creates the Kafka source table and Iceberg catalog. This file is used as\n",
    "-- an init script (-i flag) for all subsequent SQL files so they have access\n",
    "-- to the catalog within the same session.\n",
    "--\n",
    "-- Uses BATCH execution mode so jobs process all available data and terminate.\n",
    "-- =============================================================================\n",
    "\n",
    "-- Use batch mode (process available data, then stop)\n",
    "SET 'execution.runtime-mode' = 'batch';\n",
    "\n",
    "-- Wait for each INSERT to complete before proceeding to next statement\n",
    "SET 'table.dml-sync' = 'true';\n",
    "\n",
    "-- Create Kafka source table\n",
    "-- NOTE: event_time computed column + WATERMARK enables event-time processing\n",
    "-- in streaming mode. In batch mode (default), the watermark is simply ignored.\n",
    "CREATE TABLE IF NOT EXISTS kafka_raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    STRING,\n",
    "    tpep_dropoff_datetime   STRING,\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    -- Computed column for event-time processing\n",
    "    event_time AS TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss'),\n",
    "    -- Watermark: allow 10s late arrivals (ignored in batch mode)\n",
    "    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'taxi.raw_trips',\n",
    "    'properties.bootstrap.servers' = 'redpanda:9092',\n",
    "    'properties.group.id' = 'flink-consumer',\n",
    "    'scan.startup.mode' = 'earliest-offset',\n",
    "    'scan.bounded.mode' = 'latest-offset',\n",
    "    'format' = 'json'\n",
    ");\n",
    "\n",
    "-- Create Iceberg catalog backed by MinIO\n",
    "CREATE CATALOG iceberg_catalog WITH (\n",
    "    'type' = 'iceberg',\n",
    "    'catalog-type' = 'hadoop',\n",
    "    'warehouse' = 's3a://warehouse/',\n",
    "    'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO',\n",
    "    's3.endpoint' = 'http://minio:9000',\n",
    "    's3.access-key-id' = 'minioadmin',\n",
    "    's3.secret-access-key' = 'minioadmin',\n",
    "    's3.path-style-access' = 'true'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Reference: Kafka Source Table (standalone)\n",
    "\n",
    "This file creates only the Redpanda source table — useful for interactive SQL sessions where\n",
    "you want to inspect the schema without setting up the full catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/01-create-kafka-source.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Flink SQL - Kafka Source Table (Redpanda)\n",
    "-- =============================================================================\n",
    "-- Creates a Flink SQL table backed by the Redpanda topic taxi.raw_trips.\n",
    "-- The data generator produces JSON records with these exact field names\n",
    "-- matching the NYC Yellow Taxi parquet schema.\n",
    "-- =============================================================================\n",
    "\n",
    "CREATE TABLE kafka_raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    STRING,\n",
    "    tpep_dropoff_datetime   STRING,\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'taxi.raw_trips',\n",
    "    'properties.bootstrap.servers' = 'redpanda:9092',\n",
    "    'properties.group.id' = 'flink-consumer',\n",
    "    'scan.startup.mode' = 'earliest-offset',\n",
    "    'format' = 'json'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Reference: Iceberg Catalog (standalone)\n",
    "\n",
    "P04 uses the Flink Hadoop catalog (direct S3A path resolution). The alternative — Lakekeeper\n",
    "REST catalog — is available on P01 via `--profile lakekeeper`. The REST catalog adds credential\n",
    "vending (no S3 keys in SQL) but requires 4 extra services (Postgres + 3 Lakekeeper containers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/02-create-iceberg-catalog.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Flink SQL - Iceberg Catalog\n",
    "-- =============================================================================\n",
    "-- Creates a Hadoop-based Iceberg catalog backed by MinIO (S3-compatible).\n",
    "-- All Bronze and Silver tables will be created within this catalog.\n",
    "-- =============================================================================\n",
    "\n",
    "CREATE CATALOG iceberg_catalog WITH (\n",
    "    'type' = 'iceberg',\n",
    "    'catalog-type' = 'hadoop',\n",
    "    'warehouse' = 's3a://warehouse/',\n",
    "    'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO',\n",
    "    's3.endpoint' = 'http://minio:9000',\n",
    "    's3.access-key-id' = 'minioadmin',\n",
    "    's3.secret-access-key' = 'minioadmin',\n",
    "    's3.path-style-access' = 'true'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Flink SQL: Streaming Session Initialization\n",
    "\n",
    "`00-init-streaming.sql` is the streaming counterpart to `00-init.sql`. Use it with\n",
    "`07-streaming-bronze.sql` for **continuous event processing** instead of batch catch-up.\n",
    "\n",
    "### Batch vs Streaming: Key Differences\n",
    "\n",
    "| Aspect | Batch (`00-init.sql`) | Streaming (`00-init-streaming.sql`) |\n",
    "|--------|----------------------|-------------------------------------|\n",
    "| `execution.runtime-mode` | `batch` | `streaming` |\n",
    "| `table.dml-sync` | `true` (block per INSERT) | **NOT SET** — would hang forever |\n",
    "| `scan.bounded.mode` | `latest-offset` (stop at current end) | **NOT SET** — reads forever |\n",
    "| `scan.startup.mode` | `earliest-offset` (reprocess all) | `latest-offset` (only new events) |\n",
    "| Job lifecycle | Terminates when all data processed | Runs indefinitely until cancelled |\n",
    "| Fault tolerance | Not needed (restart from Redpanda) | Checkpoints every 30s (exactly-once) |\n",
    "| Use case | Backfill, benchmarks, scheduled runs | Production real-time ingestion |\n",
    "\n",
    "### Why NOT Setting table.dml-sync in Streaming Mode is Critical\n",
    "\n",
    "```sql\n",
    "-- BATCH init:\n",
    "SET 'table.dml-sync' = 'true';  -- block until INSERT completes → fine (job terminates)\n",
    "\n",
    "-- STREAMING init:\n",
    "-- Do NOT set table.dml-sync — the streaming INSERT runs FOREVER.\n",
    "-- If dml-sync=true, the session would block forever on the first INSERT,\n",
    "-- never getting to run the second SQL statement.\n",
    "```\n",
    "\n",
    "### Group ID Differs\n",
    "\n",
    "- Batch: `'properties.group.id' = 'flink-consumer'` — offset tracked for batch replay\n",
    "- Streaming: `'properties.group.id' = 'flink-streaming-consumer'` — separate group, reads from latest\n",
    "\n",
    "This separation means you can run batch reprocessing and streaming ingestion simultaneously\n",
    "without the consumer groups interfering with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/00-init-streaming.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Flink SQL - Streaming Session Initialization (Redpanda)\n",
    "-- =============================================================================\n",
    "-- Creates the Redpanda source table and Iceberg catalog for STREAMING mode.\n",
    "-- Unlike 00-init.sql (batch), this file:\n",
    "--   - Sets execution.runtime-mode = streaming\n",
    "--   - Enables checkpointing for exactly-once guarantees\n",
    "--   - Does NOT set scan.bounded.mode (so the Redpanda source never terminates)\n",
    "--   - Uses scan.startup.mode = latest-offset (process new events only)\n",
    "--\n",
    "-- Usage: sql-client.sh embedded -i 00-init-streaming.sql -f 07-streaming-bronze.sql\n",
    "-- =============================================================================\n",
    "\n",
    "-- Use streaming mode (job runs indefinitely)\n",
    "SET 'execution.runtime-mode' = 'streaming';\n",
    "\n",
    "-- Checkpoint every 30s for exactly-once guarantees\n",
    "SET 'execution.checkpointing.interval' = '30s';\n",
    "\n",
    "-- CRITICAL: Do NOT set table.dml-sync in streaming mode.\n",
    "-- dml-sync=true would cause the session to hang forever waiting for the\n",
    "-- infinite streaming job to complete before executing the next statement.\n",
    "\n",
    "-- Create Redpanda source table (streaming — no bounded mode)\n",
    "CREATE TABLE IF NOT EXISTS kafka_raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    STRING,\n",
    "    tpep_dropoff_datetime   STRING,\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    -- Computed column for event-time processing\n",
    "    event_time AS TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss'),\n",
    "    -- Watermark: allow 10s late arrivals\n",
    "    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'taxi.raw_trips',\n",
    "    'properties.bootstrap.servers' = 'redpanda:9092',\n",
    "    'properties.group.id' = 'flink-streaming-consumer',\n",
    "    'scan.startup.mode' = 'latest-offset',\n",
    "    -- NOTE: No scan.bounded.mode — this is what makes it truly streaming.\n",
    "    -- The job will run indefinitely, processing new Redpanda events as they arrive.\n",
    "    'format' = 'json'\n",
    ");\n",
    "\n",
    "-- Create Iceberg catalog backed by MinIO\n",
    "CREATE CATALOG iceberg_catalog WITH (\n",
    "    'type' = 'iceberg',\n",
    "    'catalog-type' = 'hadoop',\n",
    "    'warehouse' = 's3a://warehouse/',\n",
    "    'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO',\n",
    "    's3.endpoint' = 'http://minio:9000',\n",
    "    's3.access-key-id' = 'minioadmin',\n",
    "    's3.secret-access-key' = 'minioadmin',\n",
    "    's3.path-style-access' = 'true'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Flink SQL: Bronze Layer (Redpanda → Iceberg)\n",
    "\n",
    "### Bronze Layer Design Philosophy\n",
    "\n",
    "The Bronze layer is a **faithful copy** of the source. No business logic. No filtering. Every\n",
    "event lands — even malformed ones. This is the audit trail.\n",
    "\n",
    "```\n",
    "Redpanda message (JSON):\n",
    "{\n",
    "  \"VendorID\": 1,\n",
    "  \"tpep_pickup_datetime\": \"2024-01-15T08:30:00\",   ← STRING\n",
    "  \"fare_amount\": 12.50,\n",
    "  ...\n",
    "}\n",
    "\n",
    "Bronze Iceberg row:\n",
    "  VendorID              BIGINT  = 1\n",
    "  tpep_pickup_datetime  TIMESTAMP(3) = 2024-01-15 08:30:00.000  ← parsed\n",
    "  fare_amount           DOUBLE  = 12.50\n",
    "  ingestion_ts          TIMESTAMP(3) = 2024-01-15 09:01:33.412  ← added\n",
    "```\n",
    "\n",
    "### Timestamp Parsing Pattern\n",
    "\n",
    "```sql\n",
    "TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')\n",
    "--                                              ↑\n",
    "--                               Escaped single quotes for literal 'T'\n",
    "--                               Java SimpleDateFormat syntax in SQL string literals\n",
    "```\n",
    "\n",
    "### Why format-version=1 for Bronze?\n",
    "\n",
    "Iceberg v1 is sufficient for append-only Bronze. v2 adds row-level delete support\n",
    "(required for UPDATE/MERGE operations). Keeping Bronze at v1 makes it slightly faster\n",
    "to write and simpler to compact. Silver uses v2 because future maintenance (MERGE INTO for\n",
    "dedup, DELETE WHERE for GDPR) requires it.\n",
    "\n",
    "### 8.1 Bronze — Commented Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/03-bronze-raw-trips.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Flink SQL - Bronze Layer (Raw Trips)\n",
    "-- =============================================================================\n",
    "-- Creates the Bronze Iceberg table and starts a continuous INSERT job\n",
    "-- that reads from the Kafka source table.\n",
    "--\n",
    "-- Bronze layer preserves original column names from the source data.\n",
    "-- Timestamps are parsed from ISO 8601 strings to TIMESTAMP type.\n",
    "-- No filtering or cleaning is applied at this layer.\n",
    "-- =============================================================================\n",
    "\n",
    "-- Use the Iceberg catalog\n",
    "USE CATALOG iceberg_catalog;\n",
    "\n",
    "-- Create the Bronze database\n",
    "CREATE DATABASE IF NOT EXISTS bronze;\n",
    "USE bronze;\n",
    "\n",
    "-- Create the Bronze raw trips table\n",
    "CREATE TABLE IF NOT EXISTS raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    TIMESTAMP(3),\n",
    "    tpep_dropoff_datetime   TIMESTAMP(3),\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    ingestion_ts            TIMESTAMP(3)\n",
    ");\n",
    "\n",
    "-- Switch back to default catalog for the Kafka source table reference\n",
    "USE CATALOG default_catalog;\n",
    "USE default_database;\n",
    "\n",
    "-- Continuous INSERT from Kafka into Bronze Iceberg table\n",
    "-- Timestamps are parsed from ISO 8601 string format (e.g. \"2024-01-15T08:30:00\")\n",
    "INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "SELECT\n",
    "    VendorID,\n",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,\n",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    RatecodeID,\n",
    "    store_and_fwd_flag,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    payment_type,\n",
    "    fare_amount,\n",
    "    extra,\n",
    "    mta_tax,\n",
    "    tip_amount,\n",
    "    tolls_amount,\n",
    "    improvement_surcharge,\n",
    "    total_amount,\n",
    "    congestion_surcharge,\n",
    "    Airport_fee,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_raw_trips;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Bronze — Production File (05-bronze.sql)\n",
    "\n",
    "Used by `make process-bronze`. Runs as: `sql-client.sh embedded -i 00-init.sql -f 05-bronze.sql`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/05-bronze.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Bronze Layer (Kafka → Iceberg)\n",
    "-- =============================================================================\n",
    "-- Run: sql-client.sh embedded -i 00-init.sql -f 05-bronze.sql\n",
    "-- =============================================================================\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    TIMESTAMP(3),\n",
    "    tpep_dropoff_datetime   TIMESTAMP(3),\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    ingestion_ts            TIMESTAMP(3)\n",
    ")\n",
    "WITH (\n",
    "    'format-version' = '1',\n",
    "    'write.format.default' = 'parquet',\n",
    "    'write.parquet.compression-codec' = 'zstd',\n",
    "    'write.metadata.delete-after-commit.enabled' = 'true',\n",
    "    'write.metadata.previous-versions-max' = '10',\n",
    "    'write.target-file-size-bytes' = '134217728'\n",
    ");\n",
    "\n",
    "USE CATALOG default_catalog;\n",
    "USE default_database;\n",
    "\n",
    "INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "SELECT\n",
    "    VendorID,\n",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,\n",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    RatecodeID,\n",
    "    store_and_fwd_flag,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    payment_type,\n",
    "    fare_amount,\n",
    "    extra,\n",
    "    mta_tax,\n",
    "    tip_amount,\n",
    "    tolls_amount,\n",
    "    improvement_surcharge,\n",
    "    total_amount,\n",
    "    congestion_surcharge,\n",
    "    Airport_fee,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_raw_trips;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Flink SQL: Silver Layer — Deduplication + Quality Filtering\n",
    "\n",
    "Silver is the **trusted, clean** layer. It reads from Bronze and applies two things:\n",
    "1. **Quality filtering** — removes invalid/out-of-range records\n",
    "2. **Deduplication** — ROW_NUMBER() removes duplicate events (e.g., producer retries)\n",
    "\n",
    "### 9.1 Deduplication via ROW_NUMBER()\n",
    "\n",
    "```sql\n",
    "WITH deduped AS (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,\n",
    "                         PULocationID, DOLocationID, fare_amount, total_amount\n",
    "            ORDER BY ingestion_ts DESC        ← keep the LATEST ingestion of a duplicate\n",
    "        ) AS rn\n",
    "    FROM iceberg_catalog.bronze.raw_trips\n",
    "    WHERE ...quality filters...\n",
    ")\n",
    "SELECT ...columns... FROM deduped WHERE rn = 1;\n",
    "```\n",
    "\n",
    "**Natural key**: The combination of (vendor, pickup_time, dropoff_time, pickup_zone, dropoff_zone,\n",
    "fare, total) uniquely identifies a taxi trip. Two rows with the same natural key = same trip.\n",
    "\n",
    "**ORDER BY ingestion_ts DESC**: If a producer sent the same event twice due to a retry,\n",
    "we keep the most recently ingested copy (latest `ingestion_ts`).\n",
    "\n",
    "### Why ROW_NUMBER() and not DISTINCT?\n",
    "\n",
    "`DISTINCT` removes rows with identical values across ALL columns. It won't help if events\n",
    "differ in `ingestion_ts` (they always do — each event has a unique arrival timestamp).\n",
    "ROW_NUMBER partitioned by the **business natural key** correctly deduplicates trips that\n",
    "arrived twice with different timestamps.\n",
    "\n",
    "### Quality Filters Applied\n",
    "\n",
    "| Filter | SQL | Expected rejection |\n",
    "|--------|-----|--------------------|\n",
    "| Valid pickup time | `tpep_pickup_datetime IS NOT NULL` | Malformed events |\n",
    "| Valid dropoff time | `tpep_dropoff_datetime IS NOT NULL` | Malformed events |\n",
    "| Non-negative distance | `trip_distance >= 0` | GPS glitches |\n",
    "| Non-negative fare | `fare_amount >= 0` | Refund/test records |\n",
    "| Date range | `pickup_date BETWEEN 2024-01-01 AND 2024-01-31` | Out-of-period events |\n",
    "\n",
    "**Expected yield:** ~9,855 / 10,000 = 98.55% pass rate.\n",
    "\n",
    "### Silver Schema Changes vs Bronze\n",
    "\n",
    "| Column | Bronze | Silver | Change |\n",
    "|--------|--------|--------|--------|\n",
    "| `VendorID` | BIGINT | INT (as vendor_id in dbt) | Narrowed in dbt stg |\n",
    "| `fare_amount` | DOUBLE | DECIMAL(10,2) | Rounded + typed |\n",
    "| `pickup_date` | — | DATE | Added (for partitioning) |\n",
    "| `trip_id` | — | STRING | Added (MD5 surrogate key) |\n",
    "| `ingestion_ts` | TIMESTAMP(3) | — | Dropped (used only for dedup order) |\n",
    "\n",
    "### 9.1 Silver — Commented Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/04-silver-cleaned-trips.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Flink SQL - Silver Layer (Cleaned Trips)\n",
    "-- =============================================================================\n",
    "-- Creates the Silver Iceberg table and starts a continuous INSERT job\n",
    "-- that reads from the Bronze table, applies data quality filters,\n",
    "-- renames columns to snake_case, and computes enrichment columns.\n",
    "--\n",
    "-- Silver layer transformations:\n",
    "--   1. Column renaming (VendorID -> vendor_id, PULocationID -> pickup_location_id, etc.)\n",
    "--   2. Type casting (BIGINT -> INT where appropriate)\n",
    "--   3. Data quality filters:\n",
    "--      - Reject null timestamps\n",
    "--      - Reject negative fare amounts and trip distances\n",
    "--      - Reject pickup dates outside January 2024\n",
    "--   4. Surrogate key: MD5 hash of composite natural key\n",
    "--   5. Partition column: pickup_date (for Iceberg partitioning)\n",
    "-- =============================================================================\n",
    "\n",
    "-- Use the Iceberg catalog\n",
    "USE CATALOG iceberg_catalog;\n",
    "\n",
    "-- Create the Silver database\n",
    "CREATE DATABASE IF NOT EXISTS silver;\n",
    "USE silver;\n",
    "\n",
    "-- Create the Silver cleaned trips table\n",
    "CREATE TABLE IF NOT EXISTS cleaned_trips (\n",
    "    -- surrogate key\n",
    "    trip_id                 STRING,\n",
    "\n",
    "    -- identifiers\n",
    "    vendor_id               INT,\n",
    "    rate_code_id            INT,\n",
    "    pickup_location_id      INT,\n",
    "    dropoff_location_id     INT,\n",
    "    payment_type_id         INT,\n",
    "\n",
    "    -- timestamps\n",
    "    pickup_datetime         TIMESTAMP(3),\n",
    "    dropoff_datetime        TIMESTAMP(3),\n",
    "\n",
    "    -- trip info\n",
    "    passenger_count         INT,\n",
    "    trip_distance_miles     DOUBLE,\n",
    "    store_and_fwd_flag      STRING,\n",
    "\n",
    "    -- financials\n",
    "    fare_amount             DECIMAL(10, 2),\n",
    "    extra_amount            DECIMAL(10, 2),\n",
    "    mta_tax                 DECIMAL(10, 2),\n",
    "    tip_amount              DECIMAL(10, 2),\n",
    "    tolls_amount            DECIMAL(10, 2),\n",
    "    improvement_surcharge   DECIMAL(10, 2),\n",
    "    total_amount            DECIMAL(10, 2),\n",
    "    congestion_surcharge    DECIMAL(10, 2),\n",
    "    airport_fee             DECIMAL(10, 2),\n",
    "\n",
    "    -- computed: time dimensions\n",
    "    pickup_date             DATE\n",
    ") PARTITIONED BY (pickup_date)\n",
    "WITH (\n",
    "    'format-version' = '2',\n",
    "    'write.format.default' = 'parquet',\n",
    "    'write.parquet.compression-codec' = 'zstd',\n",
    "    'write.metadata.delete-after-commit.enabled' = 'true',\n",
    "    'write.metadata.previous-versions-max' = '10',\n",
    "    'write.target-file-size-bytes' = '134217728'\n",
    ");\n",
    "\n",
    "-- Continuous INSERT from Bronze into Silver with transformations\n",
    "INSERT INTO iceberg_catalog.silver.cleaned_trips\n",
    "SELECT\n",
    "    -- Surrogate key: MD5 hash of composite natural key\n",
    "    MD5(CONCAT_WS('|',\n",
    "        CAST(VendorID AS STRING),\n",
    "        CAST(tpep_pickup_datetime AS STRING),\n",
    "        CAST(tpep_dropoff_datetime AS STRING),\n",
    "        CAST(PULocationID AS STRING),\n",
    "        CAST(DOLocationID AS STRING),\n",
    "        CAST(fare_amount AS STRING),\n",
    "        CAST(total_amount AS STRING)\n",
    "    )) AS trip_id,\n",
    "\n",
    "    -- Identifiers (renamed + cast)\n",
    "    CAST(VendorID AS INT)       AS vendor_id,\n",
    "    CAST(RatecodeID AS INT)     AS rate_code_id,\n",
    "    CAST(PULocationID AS INT)   AS pickup_location_id,\n",
    "    CAST(DOLocationID AS INT)   AS dropoff_location_id,\n",
    "    CAST(payment_type AS INT)   AS payment_type_id,\n",
    "\n",
    "    -- Timestamps\n",
    "    tpep_pickup_datetime        AS pickup_datetime,\n",
    "    tpep_dropoff_datetime       AS dropoff_datetime,\n",
    "\n",
    "    -- Trip info\n",
    "    CAST(passenger_count AS INT) AS passenger_count,\n",
    "    trip_distance               AS trip_distance_miles,\n",
    "    store_and_fwd_flag,\n",
    "\n",
    "    -- Financials (rounded to 2 decimal places)\n",
    "    CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,\n",
    "    CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,\n",
    "    CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,\n",
    "    CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,\n",
    "    CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,\n",
    "    CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,\n",
    "    CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,\n",
    "    CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,\n",
    "    CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,\n",
    "\n",
    "    -- Computed: date dimensions\n",
    "    CAST(tpep_pickup_datetime AS DATE) AS pickup_date\n",
    "\n",
    "FROM iceberg_catalog.bronze.raw_trips\n",
    "\n",
    "-- Data quality filters\n",
    "WHERE tpep_pickup_datetime IS NOT NULL\n",
    "  AND tpep_dropoff_datetime IS NOT NULL\n",
    "  AND trip_distance >= 0\n",
    "  AND fare_amount >= 0\n",
    "  AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'\n",
    "  AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Silver — Production File (06-silver.sql)\n",
    "\n",
    "Used by `make process-silver`. This standalone file has the **full ROW_NUMBER() deduplication**\n",
    "logic — it produces the same result as the combined `05-run-all.sql` pipeline.\n",
    "\n",
    "> **Audit finding (Feb 2026):** An earlier version of `06-silver.sql` ran a plain INSERT without\n",
    "> deduplication, while `05-run-all.sql` had the correct ROW_NUMBER CTE. Running `make process-silver`\n",
    "> independently would produce duplicates. Both are now identical. Always verify row counts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/06-silver.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Silver Layer (Bronze Iceberg → Silver Iceberg)\n",
    "-- =============================================================================\n",
    "-- Run: sql-client.sh embedded -i 00-init.sql -f 06-silver.sql\n",
    "-- =============================================================================\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS silver;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS silver.cleaned_trips (\n",
    "    trip_id                 STRING,\n",
    "    vendor_id               INT,\n",
    "    rate_code_id            INT,\n",
    "    pickup_location_id      INT,\n",
    "    dropoff_location_id     INT,\n",
    "    payment_type_id         INT,\n",
    "    pickup_datetime         TIMESTAMP(3),\n",
    "    dropoff_datetime        TIMESTAMP(3),\n",
    "    passenger_count         INT,\n",
    "    trip_distance_miles     DOUBLE,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    fare_amount             DECIMAL(10, 2),\n",
    "    extra_amount            DECIMAL(10, 2),\n",
    "    mta_tax                 DECIMAL(10, 2),\n",
    "    tip_amount              DECIMAL(10, 2),\n",
    "    tolls_amount            DECIMAL(10, 2),\n",
    "    improvement_surcharge   DECIMAL(10, 2),\n",
    "    total_amount            DECIMAL(10, 2),\n",
    "    congestion_surcharge    DECIMAL(10, 2),\n",
    "    airport_fee             DECIMAL(10, 2),\n",
    "    pickup_date             DATE\n",
    ") PARTITIONED BY (pickup_date)\n",
    "WITH (\n",
    "    'format-version' = '2',\n",
    "    'write.format.default' = 'parquet',\n",
    "    'write.parquet.compression-codec' = 'zstd',\n",
    "    'write.metadata.delete-after-commit.enabled' = 'true',\n",
    "    'write.metadata.previous-versions-max' = '10',\n",
    "    'write.target-file-size-bytes' = '134217728'\n",
    ");\n",
    "\n",
    "-- Deduplication: ROW_NUMBER partitioned by natural key, keeping latest ingestion\n",
    "INSERT INTO iceberg_catalog.silver.cleaned_trips\n",
    "WITH deduped AS (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,\n",
    "                         PULocationID, DOLocationID, fare_amount, total_amount\n",
    "            ORDER BY ingestion_ts DESC\n",
    "        ) AS rn\n",
    "    FROM iceberg_catalog.bronze.raw_trips\n",
    "    WHERE tpep_pickup_datetime IS NOT NULL\n",
    "      AND tpep_dropoff_datetime IS NOT NULL\n",
    "      AND trip_distance >= 0\n",
    "      AND fare_amount >= 0\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01'\n",
    ")\n",
    "SELECT\n",
    "    CAST(MD5(CONCAT_WS('|',\n",
    "        CAST(VendorID AS STRING),\n",
    "        CAST(tpep_pickup_datetime AS STRING),\n",
    "        CAST(tpep_dropoff_datetime AS STRING),\n",
    "        CAST(PULocationID AS STRING),\n",
    "        CAST(DOLocationID AS STRING),\n",
    "        CAST(fare_amount AS STRING),\n",
    "        CAST(total_amount AS STRING)\n",
    "    )) AS STRING) AS trip_id,\n",
    "    CAST(VendorID AS INT)       AS vendor_id,\n",
    "    CAST(RatecodeID AS INT)     AS rate_code_id,\n",
    "    CAST(PULocationID AS INT)   AS pickup_location_id,\n",
    "    CAST(DOLocationID AS INT)   AS dropoff_location_id,\n",
    "    CAST(payment_type AS INT)   AS payment_type_id,\n",
    "    tpep_pickup_datetime        AS pickup_datetime,\n",
    "    tpep_dropoff_datetime       AS dropoff_datetime,\n",
    "    CAST(passenger_count AS INT) AS passenger_count,\n",
    "    trip_distance               AS trip_distance_miles,\n",
    "    store_and_fwd_flag,\n",
    "    CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,\n",
    "    CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,\n",
    "    CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,\n",
    "    CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,\n",
    "    CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,\n",
    "    CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,\n",
    "    CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,\n",
    "    CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,\n",
    "    CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,\n",
    "    CAST(tpep_pickup_datetime AS DATE) AS pickup_date\n",
    "FROM deduped\n",
    "WHERE rn = 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Combined Pipeline Reference (P01's 05-run-all.sql)\n",
    "\n",
    "P04 does not have a single combined SQL file — `make process` calls `process-bronze` then\n",
    "`process-silver` sequentially. Below is P01's combined file for reference — it shows how\n",
    "both layers can run in one SQL session with `table.dml-sync=true` ensuring ordering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference only — this is P01's 05-run-all.sql, not used by P04 directly\n",
    "# P04 uses: make process-bronze && make process-silver\n",
    "print(\"\"\"\n",
    "-- =============================================================================\n",
    "-- Pipeline 01: Flink SQL - Full Pipeline (Bronze + Silver)\n",
    "-- =============================================================================\n",
    "-- Run with init: sql-client.sh embedded -i 00-init.sql -f 05-run-all.sql\n",
    "-- =============================================================================\n",
    "\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "-- BRONZE LAYER: Raw data from Kafka → Iceberg\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    TIMESTAMP(3),\n",
    "    tpep_dropoff_datetime   TIMESTAMP(3),\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    ingestion_ts            TIMESTAMP(3)\n",
    ")\n",
    "WITH (\n",
    "    'format-version' = '1',\n",
    "    'write.format.default' = 'parquet',\n",
    "    'write.parquet.compression-codec' = 'zstd',\n",
    "    'write.metadata.delete-after-commit.enabled' = 'true',\n",
    "    'write.metadata.previous-versions-max' = '10',\n",
    "    'write.target-file-size-bytes' = '134217728'\n",
    ");\n",
    "\n",
    "-- Switch back to default catalog for Kafka source table reference\n",
    "USE CATALOG default_catalog;\n",
    "USE default_database;\n",
    "\n",
    "-- Insert from Kafka into Bronze Iceberg table\n",
    "INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "SELECT\n",
    "    VendorID,\n",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,\n",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    RatecodeID,\n",
    "    store_and_fwd_flag,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    payment_type,\n",
    "    fare_amount,\n",
    "    extra,\n",
    "    mta_tax,\n",
    "    tip_amount,\n",
    "    tolls_amount,\n",
    "    improvement_surcharge,\n",
    "    total_amount,\n",
    "    congestion_surcharge,\n",
    "    Airport_fee,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_raw_trips;\n",
    "\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "-- SILVER LAYER: Cleaned + enriched data from Bronze → Silver\n",
    "-- ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS silver;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS silver.cleaned_trips (\n",
    "    trip_id                 STRING,\n",
    "    vendor_id               INT,\n",
    "    rate_code_id            INT,\n",
    "    pickup_location_id      INT,\n",
    "    dropoff_location_id     INT,\n",
    "    payment_type_id         INT,\n",
    "    pickup_datetime         TIMESTAMP(3),\n",
    "    dropoff_datetime        TIMESTAMP(3),\n",
    "    passenger_count         INT,\n",
    "    trip_distance_miles     DOUBLE,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    fare_amount             DECIMAL(10, 2),\n",
    "    extra_amount            DECIMAL(10, 2),\n",
    "    mta_tax                 DECIMAL(10, 2),\n",
    "    tip_amount              DECIMAL(10, 2),\n",
    "    tolls_amount            DECIMAL(10, 2),\n",
    "    improvement_surcharge   DECIMAL(10, 2),\n",
    "    total_amount            DECIMAL(10, 2),\n",
    "    congestion_surcharge    DECIMAL(10, 2),\n",
    "    airport_fee             DECIMAL(10, 2),\n",
    "    pickup_date             DATE\n",
    ") PARTITIONED BY (pickup_date)\n",
    "WITH (\n",
    "    'format-version' = '2',\n",
    "    'write.format.default' = 'parquet',\n",
    "    'write.parquet.compression-codec' = 'zstd',\n",
    "    'write.metadata.delete-after-commit.enabled' = 'true',\n",
    "    'write.metadata.previous-versions-max' = '10',\n",
    "    'write.target-file-size-bytes' = '134217728'\n",
    ");\n",
    "\n",
    "-- Deduplication: ROW_NUMBER partitioned by natural key, keeping latest ingestion\n",
    "INSERT INTO iceberg_catalog.silver.cleaned_trips\n",
    "WITH deduped AS (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,\n",
    "                         PULocationID, DOLocationID, fare_amount, total_amount\n",
    "            ORDER BY ingestion_ts DESC\n",
    "        ) AS rn\n",
    "    FROM iceberg_catalog.bronze.raw_trips\n",
    "    WHERE tpep_pickup_datetime IS NOT NULL\n",
    "      AND tpep_dropoff_datetime IS NOT NULL\n",
    "      AND trip_distance >= 0\n",
    "      AND fare_amount >= 0\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'\n",
    "      AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01'\n",
    ")\n",
    "SELECT\n",
    "    CAST(MD5(CONCAT_WS('|',\n",
    "        CAST(VendorID AS STRING),\n",
    "        CAST(tpep_pickup_datetime AS STRING),\n",
    "        CAST(tpep_dropoff_datetime AS STRING),\n",
    "        CAST(PULocationID AS STRING),\n",
    "        CAST(DOLocationID AS STRING),\n",
    "        CAST(fare_amount AS STRING),\n",
    "        CAST(total_amount AS STRING)\n",
    "    )) AS STRING) AS trip_id,\n",
    "    CAST(VendorID AS INT)       AS vendor_id,\n",
    "    CAST(RatecodeID AS INT)     AS rate_code_id,\n",
    "    CAST(PULocationID AS INT)   AS pickup_location_id,\n",
    "    CAST(DOLocationID AS INT)   AS dropoff_location_id,\n",
    "    CAST(payment_type AS INT)   AS payment_type_id,\n",
    "    tpep_pickup_datetime        AS pickup_datetime,\n",
    "    tpep_dropoff_datetime       AS dropoff_datetime,\n",
    "    CAST(passenger_count AS INT) AS passenger_count,\n",
    "    trip_distance               AS trip_distance_miles,\n",
    "    store_and_fwd_flag,\n",
    "    CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,\n",
    "    CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,\n",
    "    CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,\n",
    "    CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,\n",
    "    CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,\n",
    "    CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,\n",
    "    CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,\n",
    "    CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,\n",
    "    CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,\n",
    "    CAST(tpep_pickup_datetime AS DATE) AS pickup_date\n",
    "FROM deduped\n",
    "WHERE rn = 1;\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Flink SQL: Streaming Bronze (Continuous Mode)\n",
    "\n",
    "`07-streaming-bronze.sql` is the **streaming alternative** to `05-bronze.sql`. Use it when you\n",
    "want Flink to continuously ingest new events as they arrive in Redpanda, rather than doing\n",
    "a one-time batch catch-up.\n",
    "\n",
    "### When to Use Streaming vs Batch\n",
    "\n",
    "| Scenario | Mode | Command |\n",
    "|----------|------|---------|\n",
    "| Initial backfill of historical data | Batch | `make process-bronze` |\n",
    "| Nightly scheduled catch-up | Batch | `make process` |\n",
    "| Continuous real-time ingestion | Streaming | `make process-streaming` |\n",
    "| CI/CD pipeline validation | Batch | `make benchmark` |\n",
    "\n",
    "### Streaming Job Lifecycle\n",
    "\n",
    "```\n",
    "make process-streaming\n",
    "  │\n",
    "  └─ sql-client.sh embedded -i 00-init-streaming.sql -f 07-streaming-bronze.sql\n",
    "       │\n",
    "       ├─ 00-init-streaming.sql: SET streaming mode, CREATE kafka_raw_trips (no bounded mode)\n",
    "       │\n",
    "       └─ 07-streaming-bronze.sql: INSERT INTO bronze.raw_trips FROM kafka_raw_trips\n",
    "            │\n",
    "            └─ Job runs indefinitely: reads Redpanda → writes Iceberg checkpoints every 30s\n",
    "                 Exactly-once guarantee: if TM crashes, Flink restores from last checkpoint\n",
    "                 Cancel with: Ctrl+C or Flink REST API DELETE /jobs/{jobId}\n",
    "```\n",
    "\n",
    "### Iceberg Streaming Write Behavior\n",
    "\n",
    "Flink's Iceberg sink in streaming mode writes files on checkpoint boundaries (every 30s).\n",
    "Each checkpoint produces one or more Parquet files and commits a new Iceberg snapshot.\n",
    "This means:\n",
    "- Data is visible to readers every 30s (not per-event)\n",
    "- Small files accumulate over time → schedule `rewrite_data_files` compaction periodically\n",
    "- Time travel works: `SELECT * FROM ... FOR TIMESTAMP AS OF TIMESTAMP '2024-01-15 09:00:00'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/flink/sql/07-streaming-bronze.sql\n",
    "-- =============================================================================\n",
    "-- Pipeline 04: Streaming Bronze Layer (Redpanda → Iceberg, continuous)\n",
    "-- =============================================================================\n",
    "-- Alternative to 05-bronze.sql that runs in STREAMING mode.\n",
    "-- Uses event_time watermarks defined in 00-init-streaming.sql for event-time processing.\n",
    "--\n",
    "-- Run: sql-client.sh embedded -i 00-init-streaming.sql -f 07-streaming-bronze.sql\n",
    "--\n",
    "-- NOTE: This job runs continuously until cancelled. It will process new Redpanda\n",
    "-- events as they arrive and write them to the Bronze Iceberg table.\n",
    "-- Streaming config (runtime-mode, checkpointing) is set in 00-init-streaming.sql.\n",
    "-- =============================================================================\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "CREATE DATABASE IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.raw_trips (\n",
    "    VendorID                BIGINT,\n",
    "    tpep_pickup_datetime    TIMESTAMP(3),\n",
    "    tpep_dropoff_datetime   TIMESTAMP(3),\n",
    "    passenger_count         BIGINT,\n",
    "    trip_distance           DOUBLE,\n",
    "    RatecodeID              BIGINT,\n",
    "    store_and_fwd_flag      STRING,\n",
    "    PULocationID            BIGINT,\n",
    "    DOLocationID            BIGINT,\n",
    "    payment_type            BIGINT,\n",
    "    fare_amount             DOUBLE,\n",
    "    extra                   DOUBLE,\n",
    "    mta_tax                 DOUBLE,\n",
    "    tip_amount              DOUBLE,\n",
    "    tolls_amount            DOUBLE,\n",
    "    improvement_surcharge   DOUBLE,\n",
    "    total_amount            DOUBLE,\n",
    "    congestion_surcharge    DOUBLE,\n",
    "    Airport_fee             DOUBLE,\n",
    "    ingestion_ts            TIMESTAMP(3)\n",
    ")\n",
    "WITH (\n",
    "    'format-version' = '1',\n",
    "    'write.format.default' = 'parquet',\n",
    "    'write.parquet.compression-codec' = 'zstd',\n",
    "    'write.metadata.delete-after-commit.enabled' = 'true',\n",
    "    'write.metadata.previous-versions-max' = '10',\n",
    "    'write.target-file-size-bytes' = '134217728'\n",
    ");\n",
    "\n",
    "-- Switch back to default catalog for Redpanda source table reference\n",
    "USE CATALOG default_catalog;\n",
    "USE default_database;\n",
    "\n",
    "-- Streaming INSERT: runs continuously, processing new Redpanda events as they arrive\n",
    "INSERT INTO iceberg_catalog.bronze.raw_trips\n",
    "SELECT\n",
    "    VendorID,\n",
    "    TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,\n",
    "    TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')  AS tpep_dropoff_datetime,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    RatecodeID,\n",
    "    store_and_fwd_flag,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    payment_type,\n",
    "    fare_amount,\n",
    "    extra,\n",
    "    mta_tax,\n",
    "    tip_amount,\n",
    "    tolls_amount,\n",
    "    improvement_surcharge,\n",
    "    total_amount,\n",
    "    congestion_surcharge,\n",
    "    Airport_fee,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_raw_trips;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. dbt Project Configuration\n",
    "\n",
    "dbt handles the **Silver → Gold** transformation. It reads from the Iceberg Silver table via\n",
    "DuckDB's `iceberg_scan()` function and produces analytics-ready Gold layer tables.\n",
    "\n",
    "### How dbt-duckdb + Iceberg Works\n",
    "\n",
    "```python\n",
    "# 1. DuckDB opens as an in-process embedded database\n",
    "# 2. DuckDB loads the Iceberg extension\n",
    "# 3. DuckDB reads Iceberg metadata from MinIO (S3-compatible via httpfs)\n",
    "# 4. DuckDB reads Parquet data files from MinIO\n",
    "# 5. dbt materializes results as DuckDB tables (stored in a local .duckdb file)\n",
    "\n",
    "# In sources.yml:\n",
    "external_location: \"iceberg_scan('s3://warehouse/silver/cleaned_trips', allow_moved_paths=true)\"\n",
    "\n",
    "# DuckDB resolves this as:\n",
    "# 1. Read s3://warehouse/silver/cleaned_trips/metadata/version-hint.text → get version N\n",
    "# 2. Read s3://warehouse/silver/cleaned_trips/metadata/vN.metadata.json → get manifest list\n",
    "# 3. Read manifest files → get Parquet file list\n",
    "# 4. Read Parquet files → return rows\n",
    "```\n",
    "\n",
    "**Why `allow_moved_paths=true`?** In Docker, MinIO is accessible as `minio:9000` from inside\n",
    "the container, but as `localhost:9000` from outside. The Iceberg metadata records the path used\n",
    "during write (e.g., `s3a://warehouse/...`) which differs from the read path (`s3://warehouse/...`).\n",
    "`allow_moved_paths=true` tells DuckDB to ignore path mismatches between metadata and actual location.\n",
    "\n",
    "### Silver as the dbt Source (Post-Audit)\n",
    "\n",
    "> **Critical audit finding (Feb 2026):** The original P04 `sources.yml` pointed to the Bronze\n",
    "> table (`s3://warehouse/bronze/raw_trips`). Every dbt model was built on raw, unvalidated,\n",
    "> potentially duplicate Bronze data — completely defeating Flink's Silver cleaning work.\n",
    "> Fixed to point to Silver (`s3://warehouse/silver/cleaned_trips`).\n",
    ">\n",
    "> The stg_yellow_trips.sql model is now a **passthrough**: Silver already has clean column names,\n",
    "> correct types, and validated data. Flink Silver = dbt Staging Source.\n",
    "\n",
    "### 11.1 dbt_project.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/dbt_project.yml\n",
    "name: nyc_taxi_pipeline_04\n",
    "version: \"1.0.0\"\n",
    "config-version: 2\n",
    "profile: nyc_taxi_pipeline_04\n",
    "model-paths: [\"models\"]\n",
    "analysis-paths: [\"analyses\"]\n",
    "test-paths: [\"tests\"]\n",
    "seed-paths: [\"seeds\"]\n",
    "macro-paths: [\"macros\"]\n",
    "snapshot-paths: [\"snapshots\"]\n",
    "target-path: \"target\"\n",
    "clean-targets: [\"target\", \"dbt_packages\"]\n",
    "\n",
    "dispatch:\n",
    "  - macro_namespace: nyc_taxi_dbt\n",
    "    search_order: [nyc_taxi_pipeline_04, nyc_taxi_dbt]\n",
    "\n",
    "seeds:\n",
    "  nyc_taxi_pipeline_04:\n",
    "    +schema: raw\n",
    "\n",
    "models:\n",
    "  nyc_taxi_pipeline_04:\n",
    "    staging:\n",
    "      +materialized: view\n",
    "      +schema: staging\n",
    "    intermediate:\n",
    "      +materialized: view\n",
    "      +schema: intermediate\n",
    "    marts:\n",
    "      core:\n",
    "        +materialized: table\n",
    "        +schema: marts\n",
    "      analytics:\n",
    "        +materialized: table\n",
    "        +schema: marts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 profiles.yml — DuckDB + Iceberg + MinIO Connection\n",
    "\n",
    "The profile configures three DuckDB extensions:\n",
    "- `httpfs` — allows DuckDB to read files from HTTP/S3 endpoints\n",
    "- `iceberg` — adds the `iceberg_scan()` function\n",
    "- `aws` — provides S3 credential management helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/profiles.yml\n",
    "nyc_taxi_pipeline_04:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: duckdb\n",
    "      path: /dbt/warehouse.duckdb\n",
    "      extensions:\n",
    "        - iceberg\n",
    "        - httpfs\n",
    "      settings:\n",
    "        s3_region: us-east-1\n",
    "        s3_endpoint: minio:9000\n",
    "        s3_access_key_id: minioadmin\n",
    "        s3_secret_access_key: minioadmin\n",
    "        s3_use_ssl: false\n",
    "        s3_url_style: path\n",
    "      threads: 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 packages.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/packages.yml\n",
    "packages:\n",
    "  - package: dbt-labs/dbt_utils\n",
    "    version: \">=1.1.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 sources.yml — Silver Iceberg as Source\n",
    "\n",
    "Note: `external_location` uses `s3://` (DuckDB httpfs format), not `s3a://` (Hadoop format).\n",
    "These are equivalent paths to the same MinIO storage but use different drivers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/sources/sources.yml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: raw_nyc_taxi\n",
    "    description: \"NYC TLC Yellow Taxi data - Iceberg Silver table via DuckDB iceberg_scan\"\n",
    "    schema: main\n",
    "    freshness:\n",
    "      # Adjust thresholds for your SLA (generous window for historical data)\n",
    "      warn_after: {count: 30, period: day}\n",
    "      error_after: {count: 365, period: day}\n",
    "    loaded_at_field: pickup_datetime\n",
    "    tables:\n",
    "      - name: raw_yellow_trips\n",
    "        description: \"Silver-layer cleaned trips from Iceberg (Flink-filtered, deduplicated, snake_case columns)\"\n",
    "        config:\n",
    "          external_location: \"iceberg_scan('s3://warehouse/silver/cleaned_trips', allow_moved_paths = true)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. dbt Seeds: Reference Data\n",
    "\n",
    "Seeds are static CSV files that dbt materializes as tables at `dbt seed` time.\n",
    "They provide the dimension lookup data for JOIN operations.\n",
    "\n",
    "### Why Seeds Instead of External Tables?\n",
    "\n",
    "Seeds are version-controlled with the project, always available, and fast to load.\n",
    "The alternative (external dimension tables) requires additional infrastructure. For\n",
    "relatively static reference data (265 NYC taxi zones don't change often), seeds are ideal.\n",
    "\n",
    "### 12.1 payment_type_lookup.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/seeds/payment_type_lookup.csv\n",
    "payment_type_id,payment_type_name\n",
    "1,Credit card\n",
    "2,Cash\n",
    "3,No charge\n",
    "4,Dispute\n",
    "5,Unknown\n",
    "6,Voided trip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 rate_code_lookup.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/seeds/rate_code_lookup.csv\n",
    "rate_code_id,rate_code_name\n",
    "1,Standard rate\n",
    "2,JFK\n",
    "3,Newark\n",
    "4,Nassau or Westchester\n",
    "5,Negotiated fare\n",
    "6,Group ride\n",
    "99,Unknown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 seed_properties.yml — Explicit column types\n",
    "\n",
    "Without explicit types, DuckDB may infer `payment_type_id` as `BIGINT` when the seed CSV\n",
    "has integer values. The fact table uses `INT` → the JOIN would fail with a type mismatch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/seeds/seed_properties.yml\n",
    "version: 2\n",
    "\n",
    "seeds:\n",
    "  - name: taxi_zone_lookup\n",
    "    description: \"NYC TLC Taxi Zone lookup table (~265 zones)\"\n",
    "    columns:\n",
    "      - name: LocationID\n",
    "        tests: [unique, not_null]\n",
    "      - name: Borough\n",
    "        tests: [not_null]\n",
    "\n",
    "  - name: payment_type_lookup\n",
    "    description: \"Payment type ID to description mapping\"\n",
    "    columns:\n",
    "      - name: payment_type_id\n",
    "        tests: [unique, not_null]\n",
    "\n",
    "  - name: rate_code_lookup\n",
    "    description: \"Rate code ID to description mapping\"\n",
    "    columns:\n",
    "      - name: rate_code_id\n",
    "        tests: [unique, not_null]\n",
    "\n",
    "  - name: vendor_lookup\n",
    "    description: \"TPEP vendor ID mapping (1=CMT, 2=VTS)\"\n",
    "    columns:\n",
    "      - name: vendor_id\n",
    "        tests: [unique, not_null]\n",
    "      - name: vendor_name\n",
    "        tests: [not_null]\n",
    "      - name: vendor_abbr\n",
    "        tests: [unique, not_null]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 taxi_zone_lookup.csv (265 NYC zones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/seeds/taxi_zone_lookup.csv\n",
    "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\n",
    "1,\"EWR\",\"Newark Airport\",\"EWR\"\n",
    "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\n",
    "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\n",
    "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\n",
    "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\n",
    "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\n",
    "7,\"Queens\",\"Astoria\",\"Boro Zone\"\n",
    "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\n",
    "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\n",
    "10,\"Queens\",\"Baisley Park\",\"Boro Zone\"\n",
    "11,\"Brooklyn\",\"Bath Beach\",\"Boro Zone\"\n",
    "12,\"Manhattan\",\"Battery Park\",\"Yellow Zone\"\n",
    "13,\"Manhattan\",\"Battery Park City\",\"Yellow Zone\"\n",
    "14,\"Brooklyn\",\"Bay Ridge\",\"Boro Zone\"\n",
    "15,\"Queens\",\"Bay Terrace/Fort Totten\",\"Boro Zone\"\n",
    "16,\"Queens\",\"Bayside\",\"Boro Zone\"\n",
    "17,\"Brooklyn\",\"Bedford\",\"Boro Zone\"\n",
    "18,\"Bronx\",\"Bedford Park\",\"Boro Zone\"\n",
    "19,\"Queens\",\"Bellerose\",\"Boro Zone\"\n",
    "20,\"Bronx\",\"Belmont\",\"Boro Zone\"\n",
    "21,\"Brooklyn\",\"Bensonhurst East\",\"Boro Zone\"\n",
    "22,\"Brooklyn\",\"Bensonhurst West\",\"Boro Zone\"\n",
    "23,\"Staten Island\",\"Bloomfield/Emerson Hill\",\"Boro Zone\"\n",
    "24,\"Manhattan\",\"Bloomingdale\",\"Yellow Zone\"\n",
    "25,\"Brooklyn\",\"Boerum Hill\",\"Boro Zone\"\n",
    "26,\"Brooklyn\",\"Borough Park\",\"Boro Zone\"\n",
    "27,\"Queens\",\"Breezy Point/Fort Tilden/Riis Beach\",\"Boro Zone\"\n",
    "28,\"Queens\",\"Briarwood/Jamaica Hills\",\"Boro Zone\"\n",
    "29,\"Brooklyn\",\"Brighton Beach\",\"Boro Zone\"\n",
    "30,\"Queens\",\"Broad Channel\",\"Boro Zone\"\n",
    "31,\"Bronx\",\"Bronx Park\",\"Boro Zone\"\n",
    "32,\"Bronx\",\"Bronxdale\",\"Boro Zone\"\n",
    "33,\"Brooklyn\",\"Brooklyn Heights\",\"Boro Zone\"\n",
    "34,\"Brooklyn\",\"Brooklyn Navy Yard\",\"Boro Zone\"\n",
    "35,\"Brooklyn\",\"Brownsville\",\"Boro Zone\"\n",
    "36,\"Brooklyn\",\"Bushwick North\",\"Boro Zone\"\n",
    "37,\"Brooklyn\",\"Bushwick South\",\"Boro Zone\"\n",
    "38,\"Queens\",\"Cambria Heights\",\"Boro Zone\"\n",
    "39,\"Brooklyn\",\"Canarsie\",\"Boro Zone\"\n",
    "40,\"Brooklyn\",\"Carroll Gardens\",\"Boro Zone\"\n",
    "41,\"Manhattan\",\"Central Harlem\",\"Boro Zone\"\n",
    "42,\"Manhattan\",\"Central Harlem North\",\"Boro Zone\"\n",
    "43,\"Manhattan\",\"Central Park\",\"Yellow Zone\"\n",
    "44,\"Staten Island\",\"Charleston/Tottenville\",\"Boro Zone\"\n",
    "45,\"Manhattan\",\"Chinatown\",\"Yellow Zone\"\n",
    "46,\"Bronx\",\"City Island\",\"Boro Zone\"\n",
    "47,\"Bronx\",\"Claremont/Bathgate\",\"Boro Zone\"\n",
    "48,\"Manhattan\",\"Clinton East\",\"Yellow Zone\"\n",
    "49,\"Brooklyn\",\"Clinton Hill\",\"Boro Zone\"\n",
    "50,\"Manhattan\",\"Clinton West\",\"Yellow Zone\"\n",
    "51,\"Bronx\",\"Co-Op City\",\"Boro Zone\"\n",
    "52,\"Brooklyn\",\"Cobble Hill\",\"Boro Zone\"\n",
    "53,\"Queens\",\"College Point\",\"Boro Zone\"\n",
    "54,\"Brooklyn\",\"Columbia Street\",\"Boro Zone\"\n",
    "55,\"Brooklyn\",\"Coney Island\",\"Boro Zone\"\n",
    "56,\"Queens\",\"Corona\",\"Boro Zone\"\n",
    "57,\"Queens\",\"Corona\",\"Boro Zone\"\n",
    "58,\"Bronx\",\"Country Club\",\"Boro Zone\"\n",
    "59,\"Bronx\",\"Crotona Park\",\"Boro Zone\"\n",
    "60,\"Bronx\",\"Crotona Park East\",\"Boro Zone\"\n",
    "61,\"Brooklyn\",\"Crown Heights North\",\"Boro Zone\"\n",
    "62,\"Brooklyn\",\"Crown Heights South\",\"Boro Zone\"\n",
    "63,\"Brooklyn\",\"Cypress Hills\",\"Boro Zone\"\n",
    "64,\"Queens\",\"Douglaston\",\"Boro Zone\"\n",
    "65,\"Brooklyn\",\"Downtown Brooklyn/MetroTech\",\"Boro Zone\"\n",
    "66,\"Brooklyn\",\"DUMBO/Vinegar Hill\",\"Boro Zone\"\n",
    "67,\"Brooklyn\",\"Dyker Heights\",\"Boro Zone\"\n",
    "68,\"Manhattan\",\"East Chelsea\",\"Yellow Zone\"\n",
    "69,\"Bronx\",\"East Concourse/Concourse Village\",\"Boro Zone\"\n",
    "70,\"Queens\",\"East Elmhurst\",\"Boro Zone\"\n",
    "71,\"Brooklyn\",\"East Flatbush/Farragut\",\"Boro Zone\"\n",
    "72,\"Brooklyn\",\"East Flatbush/Remsen Village\",\"Boro Zone\"\n",
    "73,\"Queens\",\"East Flushing\",\"Boro Zone\"\n",
    "74,\"Manhattan\",\"East Harlem North\",\"Boro Zone\"\n",
    "75,\"Manhattan\",\"East Harlem South\",\"Boro Zone\"\n",
    "76,\"Brooklyn\",\"East New York\",\"Boro Zone\"\n",
    "77,\"Brooklyn\",\"East New York/Pennsylvania Avenue\",\"Boro Zone\"\n",
    "78,\"Bronx\",\"East Tremont\",\"Boro Zone\"\n",
    "79,\"Manhattan\",\"East Village\",\"Yellow Zone\"\n",
    "80,\"Brooklyn\",\"East Williamsburg\",\"Boro Zone\"\n",
    "81,\"Bronx\",\"Eastchester\",\"Boro Zone\"\n",
    "82,\"Queens\",\"Elmhurst\",\"Boro Zone\"\n",
    "83,\"Queens\",\"Elmhurst/Maspeth\",\"Boro Zone\"\n",
    "84,\"Staten Island\",\"Eltingville/Annadale/Prince's Bay\",\"Boro Zone\"\n",
    "85,\"Brooklyn\",\"Erasmus\",\"Boro Zone\"\n",
    "86,\"Queens\",\"Far Rockaway\",\"Boro Zone\"\n",
    "87,\"Manhattan\",\"Financial District North\",\"Yellow Zone\"\n",
    "88,\"Manhattan\",\"Financial District South\",\"Yellow Zone\"\n",
    "89,\"Brooklyn\",\"Flatbush/Ditmas Park\",\"Boro Zone\"\n",
    "90,\"Manhattan\",\"Flatiron\",\"Yellow Zone\"\n",
    "91,\"Brooklyn\",\"Flatlands\",\"Boro Zone\"\n",
    "92,\"Queens\",\"Flushing\",\"Boro Zone\"\n",
    "93,\"Queens\",\"Flushing Meadows-Corona Park\",\"Boro Zone\"\n",
    "94,\"Bronx\",\"Fordham South\",\"Boro Zone\"\n",
    "95,\"Queens\",\"Forest Hills\",\"Boro Zone\"\n",
    "96,\"Queens\",\"Forest Park/Highland Park\",\"Boro Zone\"\n",
    "97,\"Brooklyn\",\"Fort Greene\",\"Boro Zone\"\n",
    "98,\"Queens\",\"Fresh Meadows\",\"Boro Zone\"\n",
    "99,\"Staten Island\",\"Freshkills Park\",\"Boro Zone\"\n",
    "100,\"Manhattan\",\"Garment District\",\"Yellow Zone\"\n",
    "101,\"Queens\",\"Glen Oaks\",\"Boro Zone\"\n",
    "102,\"Queens\",\"Glendale\",\"Boro Zone\"\n",
    "103,\"Manhattan\",\"Governor's Island/Ellis Island/Liberty Island\",\"Yellow Zone\"\n",
    "104,\"Manhattan\",\"Governor's Island/Ellis Island/Liberty Island\",\"Yellow Zone\"\n",
    "105,\"Manhattan\",\"Governor's Island/Ellis Island/Liberty Island\",\"Yellow Zone\"\n",
    "106,\"Brooklyn\",\"Gowanus\",\"Boro Zone\"\n",
    "107,\"Manhattan\",\"Gramercy\",\"Yellow Zone\"\n",
    "108,\"Brooklyn\",\"Gravesend\",\"Boro Zone\"\n",
    "109,\"Staten Island\",\"Great Kills\",\"Boro Zone\"\n",
    "110,\"Staten Island\",\"Great Kills Park\",\"Boro Zone\"\n",
    "111,\"Brooklyn\",\"Green-Wood Cemetery\",\"Boro Zone\"\n",
    "112,\"Brooklyn\",\"Greenpoint\",\"Boro Zone\"\n",
    "113,\"Manhattan\",\"Greenwich Village North\",\"Yellow Zone\"\n",
    "114,\"Manhattan\",\"Greenwich Village South\",\"Yellow Zone\"\n",
    "115,\"Staten Island\",\"Grymes Hill/Clifton\",\"Boro Zone\"\n",
    "116,\"Manhattan\",\"Hamilton Heights\",\"Boro Zone\"\n",
    "117,\"Queens\",\"Hammels/Arverne\",\"Boro Zone\"\n",
    "118,\"Staten Island\",\"Heartland Village/Todt Hill\",\"Boro Zone\"\n",
    "119,\"Bronx\",\"Highbridge\",\"Boro Zone\"\n",
    "120,\"Manhattan\",\"Highbridge Park\",\"Boro Zone\"\n",
    "121,\"Queens\",\"Hillcrest/Pomonok\",\"Boro Zone\"\n",
    "122,\"Queens\",\"Hollis\",\"Boro Zone\"\n",
    "123,\"Brooklyn\",\"Homecrest\",\"Boro Zone\"\n",
    "124,\"Queens\",\"Howard Beach\",\"Boro Zone\"\n",
    "125,\"Manhattan\",\"Hudson Sq\",\"Yellow Zone\"\n",
    "126,\"Bronx\",\"Hunts Point\",\"Boro Zone\"\n",
    "127,\"Manhattan\",\"Inwood\",\"Boro Zone\"\n",
    "128,\"Manhattan\",\"Inwood Hill Park\",\"Boro Zone\"\n",
    "129,\"Queens\",\"Jackson Heights\",\"Boro Zone\"\n",
    "130,\"Queens\",\"Jamaica\",\"Boro Zone\"\n",
    "131,\"Queens\",\"Jamaica Estates\",\"Boro Zone\"\n",
    "132,\"Queens\",\"JFK Airport\",\"Airports\"\n",
    "133,\"Brooklyn\",\"Kensington\",\"Boro Zone\"\n",
    "134,\"Queens\",\"Kew Gardens\",\"Boro Zone\"\n",
    "135,\"Queens\",\"Kew Gardens Hills\",\"Boro Zone\"\n",
    "136,\"Bronx\",\"Kingsbridge Heights\",\"Boro Zone\"\n",
    "137,\"Manhattan\",\"Kips Bay\",\"Yellow Zone\"\n",
    "138,\"Queens\",\"LaGuardia Airport\",\"Airports\"\n",
    "139,\"Queens\",\"Laurelton\",\"Boro Zone\"\n",
    "140,\"Manhattan\",\"Lenox Hill East\",\"Yellow Zone\"\n",
    "141,\"Manhattan\",\"Lenox Hill West\",\"Yellow Zone\"\n",
    "142,\"Manhattan\",\"Lincoln Square East\",\"Yellow Zone\"\n",
    "143,\"Manhattan\",\"Lincoln Square West\",\"Yellow Zone\"\n",
    "144,\"Manhattan\",\"Little Italy/NoLiTa\",\"Yellow Zone\"\n",
    "145,\"Queens\",\"Long Island City/Hunters Point\",\"Boro Zone\"\n",
    "146,\"Queens\",\"Long Island City/Queens Plaza\",\"Boro Zone\"\n",
    "147,\"Bronx\",\"Longwood\",\"Boro Zone\"\n",
    "148,\"Manhattan\",\"Lower East Side\",\"Yellow Zone\"\n",
    "149,\"Brooklyn\",\"Madison\",\"Boro Zone\"\n",
    "150,\"Brooklyn\",\"Manhattan Beach\",\"Boro Zone\"\n",
    "151,\"Manhattan\",\"Manhattan Valley\",\"Yellow Zone\"\n",
    "152,\"Manhattan\",\"Manhattanville\",\"Boro Zone\"\n",
    "153,\"Manhattan\",\"Marble Hill\",\"Boro Zone\"\n",
    "154,\"Brooklyn\",\"Marine Park/Floyd Bennett Field\",\"Boro Zone\"\n",
    "155,\"Brooklyn\",\"Marine Park/Mill Basin\",\"Boro Zone\"\n",
    "156,\"Staten Island\",\"Mariners Harbor\",\"Boro Zone\"\n",
    "157,\"Queens\",\"Maspeth\",\"Boro Zone\"\n",
    "158,\"Manhattan\",\"Meatpacking/West Village West\",\"Yellow Zone\"\n",
    "159,\"Bronx\",\"Melrose South\",\"Boro Zone\"\n",
    "160,\"Queens\",\"Middle Village\",\"Boro Zone\"\n",
    "161,\"Manhattan\",\"Midtown Center\",\"Yellow Zone\"\n",
    "162,\"Manhattan\",\"Midtown East\",\"Yellow Zone\"\n",
    "163,\"Manhattan\",\"Midtown North\",\"Yellow Zone\"\n",
    "164,\"Manhattan\",\"Midtown South\",\"Yellow Zone\"\n",
    "165,\"Brooklyn\",\"Midwood\",\"Boro Zone\"\n",
    "166,\"Manhattan\",\"Morningside Heights\",\"Boro Zone\"\n",
    "167,\"Bronx\",\"Morrisania/Melrose\",\"Boro Zone\"\n",
    "168,\"Bronx\",\"Mott Haven/Port Morris\",\"Boro Zone\"\n",
    "169,\"Bronx\",\"Mount Hope\",\"Boro Zone\"\n",
    "170,\"Manhattan\",\"Murray Hill\",\"Yellow Zone\"\n",
    "171,\"Queens\",\"Murray Hill-Queens\",\"Boro Zone\"\n",
    "172,\"Staten Island\",\"New Dorp/Midland Beach\",\"Boro Zone\"\n",
    "173,\"Queens\",\"North Corona\",\"Boro Zone\"\n",
    "174,\"Bronx\",\"Norwood\",\"Boro Zone\"\n",
    "175,\"Queens\",\"Oakland Gardens\",\"Boro Zone\"\n",
    "176,\"Staten Island\",\"Oakwood\",\"Boro Zone\"\n",
    "177,\"Brooklyn\",\"Ocean Hill\",\"Boro Zone\"\n",
    "178,\"Brooklyn\",\"Ocean Parkway South\",\"Boro Zone\"\n",
    "179,\"Queens\",\"Old Astoria\",\"Boro Zone\"\n",
    "180,\"Queens\",\"Ozone Park\",\"Boro Zone\"\n",
    "181,\"Brooklyn\",\"Park Slope\",\"Boro Zone\"\n",
    "182,\"Bronx\",\"Parkchester\",\"Boro Zone\"\n",
    "183,\"Bronx\",\"Pelham Bay\",\"Boro Zone\"\n",
    "184,\"Bronx\",\"Pelham Bay Park\",\"Boro Zone\"\n",
    "185,\"Bronx\",\"Pelham Parkway\",\"Boro Zone\"\n",
    "186,\"Manhattan\",\"Penn Station/Madison Sq West\",\"Yellow Zone\"\n",
    "187,\"Staten Island\",\"Port Richmond\",\"Boro Zone\"\n",
    "188,\"Brooklyn\",\"Prospect-Lefferts Gardens\",\"Boro Zone\"\n",
    "189,\"Brooklyn\",\"Prospect Heights\",\"Boro Zone\"\n",
    "190,\"Brooklyn\",\"Prospect Park\",\"Boro Zone\"\n",
    "191,\"Queens\",\"Queens Village\",\"Boro Zone\"\n",
    "192,\"Queens\",\"Queensboro Hill\",\"Boro Zone\"\n",
    "193,\"Queens\",\"Queensbridge/Ravenswood\",\"Boro Zone\"\n",
    "194,\"Manhattan\",\"Randalls Island\",\"Yellow Zone\"\n",
    "195,\"Brooklyn\",\"Red Hook\",\"Boro Zone\"\n",
    "196,\"Queens\",\"Rego Park\",\"Boro Zone\"\n",
    "197,\"Queens\",\"Richmond Hill\",\"Boro Zone\"\n",
    "198,\"Queens\",\"Ridgewood\",\"Boro Zone\"\n",
    "199,\"Bronx\",\"Rikers Island\",\"Boro Zone\"\n",
    "200,\"Bronx\",\"Riverdale/North Riverdale/Fieldston\",\"Boro Zone\"\n",
    "201,\"Queens\",\"Rockaway Park\",\"Boro Zone\"\n",
    "202,\"Manhattan\",\"Roosevelt Island\",\"Boro Zone\"\n",
    "203,\"Queens\",\"Rosedale\",\"Boro Zone\"\n",
    "204,\"Staten Island\",\"Rossville/Woodrow\",\"Boro Zone\"\n",
    "205,\"Queens\",\"Saint Albans\",\"Boro Zone\"\n",
    "206,\"Staten Island\",\"Saint George/New Brighton\",\"Boro Zone\"\n",
    "207,\"Queens\",\"Saint Michaels Cemetery/Woodside\",\"Boro Zone\"\n",
    "208,\"Bronx\",\"Schuylerville/Edgewater Park\",\"Boro Zone\"\n",
    "209,\"Manhattan\",\"Seaport\",\"Yellow Zone\"\n",
    "210,\"Brooklyn\",\"Sheepshead Bay\",\"Boro Zone\"\n",
    "211,\"Manhattan\",\"SoHo\",\"Yellow Zone\"\n",
    "212,\"Bronx\",\"Soundview/Bruckner\",\"Boro Zone\"\n",
    "213,\"Bronx\",\"Soundview/Castle Hill\",\"Boro Zone\"\n",
    "214,\"Staten Island\",\"South Beach/Dongan Hills\",\"Boro Zone\"\n",
    "215,\"Queens\",\"South Jamaica\",\"Boro Zone\"\n",
    "216,\"Queens\",\"South Ozone Park\",\"Boro Zone\"\n",
    "217,\"Brooklyn\",\"South Williamsburg\",\"Boro Zone\"\n",
    "218,\"Queens\",\"Springfield Gardens North\",\"Boro Zone\"\n",
    "219,\"Queens\",\"Springfield Gardens South\",\"Boro Zone\"\n",
    "220,\"Bronx\",\"Spuyten Duyvil/Kingsbridge\",\"Boro Zone\"\n",
    "221,\"Staten Island\",\"Stapleton\",\"Boro Zone\"\n",
    "222,\"Brooklyn\",\"Starrett City\",\"Boro Zone\"\n",
    "223,\"Queens\",\"Steinway\",\"Boro Zone\"\n",
    "224,\"Manhattan\",\"Stuy Town/Peter Cooper Village\",\"Yellow Zone\"\n",
    "225,\"Brooklyn\",\"Stuyvesant Heights\",\"Boro Zone\"\n",
    "226,\"Queens\",\"Sunnyside\",\"Boro Zone\"\n",
    "227,\"Brooklyn\",\"Sunset Park East\",\"Boro Zone\"\n",
    "228,\"Brooklyn\",\"Sunset Park West\",\"Boro Zone\"\n",
    "229,\"Manhattan\",\"Sutton Place/Turtle Bay North\",\"Yellow Zone\"\n",
    "230,\"Manhattan\",\"Times Sq/Theatre District\",\"Yellow Zone\"\n",
    "231,\"Manhattan\",\"TriBeCa/Civic Center\",\"Yellow Zone\"\n",
    "232,\"Manhattan\",\"Two Bridges/Seward Park\",\"Yellow Zone\"\n",
    "233,\"Manhattan\",\"UN/Turtle Bay South\",\"Yellow Zone\"\n",
    "234,\"Manhattan\",\"Union Sq\",\"Yellow Zone\"\n",
    "235,\"Bronx\",\"University Heights/Morris Heights\",\"Boro Zone\"\n",
    "236,\"Manhattan\",\"Upper East Side North\",\"Yellow Zone\"\n",
    "237,\"Manhattan\",\"Upper East Side South\",\"Yellow Zone\"\n",
    "238,\"Manhattan\",\"Upper West Side North\",\"Yellow Zone\"\n",
    "239,\"Manhattan\",\"Upper West Side South\",\"Yellow Zone\"\n",
    "240,\"Bronx\",\"Van Cortlandt Park\",\"Boro Zone\"\n",
    "241,\"Bronx\",\"Van Cortlandt Village\",\"Boro Zone\"\n",
    "242,\"Bronx\",\"Van Nest/Morris Park\",\"Boro Zone\"\n",
    "243,\"Manhattan\",\"Washington Heights North\",\"Boro Zone\"\n",
    "244,\"Manhattan\",\"Washington Heights South\",\"Boro Zone\"\n",
    "245,\"Staten Island\",\"West Brighton\",\"Boro Zone\"\n",
    "246,\"Manhattan\",\"West Chelsea/Hudson Yards\",\"Yellow Zone\"\n",
    "247,\"Bronx\",\"West Concourse\",\"Boro Zone\"\n",
    "248,\"Bronx\",\"West Farms/Bronx River\",\"Boro Zone\"\n",
    "249,\"Manhattan\",\"West Village\",\"Yellow Zone\"\n",
    "250,\"Bronx\",\"Westchester Village/Unionport\",\"Boro Zone\"\n",
    "251,\"Staten Island\",\"Westerleigh\",\"Boro Zone\"\n",
    "252,\"Queens\",\"Whitestone\",\"Boro Zone\"\n",
    "253,\"Queens\",\"Willets Point\",\"Boro Zone\"\n",
    "254,\"Bronx\",\"Williamsbridge/Olinville\",\"Boro Zone\"\n",
    "255,\"Brooklyn\",\"Williamsburg (North Side)\",\"Boro Zone\"\n",
    "256,\"Brooklyn\",\"Williamsburg (South Side)\",\"Boro Zone\"\n",
    "257,\"Brooklyn\",\"Windsor Terrace\",\"Boro Zone\"\n",
    "258,\"Queens\",\"Woodhaven\",\"Boro Zone\"\n",
    "259,\"Bronx\",\"Woodlawn/Wakefield\",\"Boro Zone\"\n",
    "260,\"Queens\",\"Woodside\",\"Boro Zone\"\n",
    "261,\"Manhattan\",\"World Trade Center\",\"Yellow Zone\"\n",
    "262,\"Manhattan\",\"Yorkville East\",\"Yellow Zone\"\n",
    "263,\"Manhattan\",\"Yorkville West\",\"Yellow Zone\"\n",
    "264,\"Unknown\",\"N/A\",\"N/A\"\n",
    "265,\"N/A\",\"Outside of NYC\",\"N/A\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5 vendor_lookup.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/seeds/vendor_lookup.csv\n",
    "vendor_id,vendor_name,vendor_abbr\n",
    "1,Creative Mobile Technologies,CMT\n",
    "2,VeriFone Inc.,VFI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. dbt Macros: Cross-Database Compatibility\n",
    "\n",
    "dbt macros use Jinja2 templating to provide database-agnostic SQL. The `adapter.dispatch()`\n",
    "pattern means the same model works on DuckDB, Snowflake, BigQuery, or Spark — the macro\n",
    "selects the right implementation for the target adapter.\n",
    "\n",
    "**For this pipeline:** All macros render the DuckDB variant (it's our only adapter).\n",
    "When porting to another database, only the macro implementations need to change.\n",
    "\n",
    "### 13.1 cents_to_dollars (DuckDB: integer division + cast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/macros/cents_to_dollars.sql\n",
    "/*\n",
    "    Macro: Convert a cents column to dollars with rounding.\n",
    "\n",
    "    Usage:\n",
    "        {{ cents_to_dollars('fare_cents') }}\n",
    "        {{ cents_to_dollars('fare_cents', 4) }}\n",
    "*/\n",
    "\n",
    "{% macro cents_to_dollars(column_name, precision=2) %}\n",
    "    round(cast({{ column_name }} as decimal(10, {{ precision }})) / 100, {{ precision }})\n",
    "{% endmacro %}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2–13.4 Other macros\n",
    "\n",
    "- `dayname_compat` — day-of-week name: DuckDB uses `strftime('%A', ...)`, Spark uses `date_format(..., 'EEEE')`\n",
    "- `duration_minutes` — DuckDB: `epoch_ms(end_ts - start_ts) / 60000.0`\n",
    "- `test_positive_value` — reusable test: fails if any row has the named column ≤ 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/macros/dayname_compat.sql\n",
    "/*\n",
    "    Macro: Get day-of-week name from a timestamp.\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL (RisingWave), and Spark.\n",
    "\n",
    "    Usage:\n",
    "        {{ dayname_compat('pickup_datetime') }}\n",
    "*/\n",
    "\n",
    "{% macro dayname_compat(col) %}\n",
    "    {{ return(adapter.dispatch('dayname_compat', 'nyc_taxi_dbt')(col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__dayname_compat(col) %}\n",
    "    dayname({{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__dayname_compat(col) %}\n",
    "    trim(to_char({{ col }}, 'Day'))\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__dayname_compat(col) %}\n",
    "    date_format({{ col }}, 'EEEE')\n",
    "{% endmacro %}\n",
    "\n",
    "\n",
    "/*\n",
    "    Macro: Get month name from a timestamp.\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL (RisingWave), and Spark.\n",
    "*/\n",
    "\n",
    "{% macro monthname_compat(col) %}\n",
    "    {{ return(adapter.dispatch('monthname_compat', 'nyc_taxi_dbt')(col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__monthname_compat(col) %}\n",
    "    monthname({{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__monthname_compat(col) %}\n",
    "    trim(to_char({{ col }}, 'Month'))\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__monthname_compat(col) %}\n",
    "    date_format({{ col }}, 'MMMM')\n",
    "{% endmacro %}\n",
    "\n",
    "\n",
    "/*\n",
    "    Macro: Statistical mode (most common value).\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL, and Spark.\n",
    "*/\n",
    "\n",
    "{% macro mode_compat(col) %}\n",
    "    {{ return(adapter.dispatch('mode_compat', 'nyc_taxi_dbt')(col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__mode_compat(col) %}\n",
    "    mode({{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__mode_compat(col) %}\n",
    "    mode() WITHIN GROUP (ORDER BY {{ col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__mode_compat(col) %}\n",
    "    mode({{ col }})\n",
    "{% endmacro %}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/macros/duration_minutes.sql\n",
    "/*\n",
    "    Macro: Calculate duration between two timestamps in minutes.\n",
    "    Adapter-dispatched for DuckDB, PostgreSQL (RisingWave), and Spark.\n",
    "\n",
    "    Usage:\n",
    "        {{ duration_minutes('pickup_datetime', 'dropoff_datetime') }}\n",
    "*/\n",
    "\n",
    "{% macro duration_minutes(start_col, end_col) %}\n",
    "    {{ return(adapter.dispatch('duration_minutes', 'nyc_taxi_dbt')(start_col, end_col)) }}\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro duckdb__duration_minutes(start_col, end_col) %}\n",
    "    datediff('minute', {{ start_col }}, {{ end_col }})\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro postgres__duration_minutes(start_col, end_col) %}\n",
    "    (EXTRACT(EPOCH FROM ({{ end_col }} - {{ start_col }})) / 60)::bigint\n",
    "{% endmacro %}\n",
    "\n",
    "{% macro spark__duration_minutes(start_col, end_col) %}\n",
    "    CAST((UNIX_TIMESTAMP({{ end_col }}) - UNIX_TIMESTAMP({{ start_col }})) / 60 AS BIGINT)\n",
    "{% endmacro %}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/macros/test_positive_value.sql\n",
    "/*\n",
    "    Custom generic test: Asserts that all values in a column are >= 0.\n",
    "\n",
    "    Usage in schema.yml:\n",
    "        columns:\n",
    "          - name: fare_amount\n",
    "            tests:\n",
    "              - positive_value\n",
    "*/\n",
    "\n",
    "{% test positive_value(model, column_name) %}\n",
    "\n",
    "select\n",
    "    {{ column_name }} as invalid_value\n",
    "from {{ model }}\n",
    "where {{ column_name }} < 0\n",
    "\n",
    "{% endtest %}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. dbt Staging Models\n",
    "\n",
    "Staging models are **thin wrappers** — they standardize column names, types, and apply the\n",
    "minimal filter set needed to make downstream models reliable. No business logic here.\n",
    "\n",
    "### The Silver Passthrough Pattern\n",
    "\n",
    "Because Flink Silver already:\n",
    "- Renamed: `VendorID` → `vendor_id`, `PULocationID` → `pickup_location_id`, etc.\n",
    "- Type-cast: `BIGINT` → `INT`, `DOUBLE` → `DECIMAL(10,2)`\n",
    "- Quality-filtered and deduplicated\n",
    "\n",
    "`stg_yellow_trips.sql` is essentially a passthrough with minor DuckDB type compatibility\n",
    "casts (`TIMESTAMP(3)` → `TIMESTAMP`). No cleaning needed at this stage.\n",
    "\n",
    "This is the **correct medallion architecture**: Flink does the hard infrastructure work,\n",
    "dbt staging is the clean entry point.\n",
    "\n",
    "### 14.1 stg_yellow_trips.sql — Main staging model (Silver passthrough)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/staging/stg_yellow_trips.sql\n",
    "{#\n",
    "    Staging model: Yellow taxi trip records (Iceberg pipeline variant)\n",
    "\n",
    "    This is a simple passthrough since Flink already performed the heavy lifting:\n",
    "      - Column renaming (VendorID -> vendor_id, etc.)\n",
    "      - Type casting (BIGINT -> INT, DOUBLE -> DECIMAL)\n",
    "      - Data quality filtering (nulls, negative fares, date range)\n",
    "      - Deduplication (ROW_NUMBER OVER natural key, latest ingestion wins)\n",
    "      - Surrogate key generation (MD5 hash → trip_id)\n",
    "\n",
    "    The source reads the Silver Iceberg table via DuckDB iceberg_scan().\n",
    "#}\n",
    "\n",
    "with source as (\n",
    "    select * from {{ source('raw_nyc_taxi', 'raw_yellow_trips') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        -- Flink already generated the surrogate key\n",
    "        trip_id,\n",
    "\n",
    "        -- identifiers (already renamed and cast by Flink)\n",
    "        vendor_id,\n",
    "        rate_code_id,\n",
    "        pickup_location_id,\n",
    "        dropoff_location_id,\n",
    "        payment_type_id,\n",
    "\n",
    "        -- timestamps (already parsed by Flink)\n",
    "        cast(pickup_datetime as timestamp) as pickup_datetime,\n",
    "        cast(dropoff_datetime as timestamp) as dropoff_datetime,\n",
    "\n",
    "        -- trip info\n",
    "        passenger_count,\n",
    "        trip_distance_miles,\n",
    "        store_and_fwd_flag,\n",
    "\n",
    "        -- financials (already rounded by Flink)\n",
    "        round(cast(fare_amount as decimal(10, 2)), 2) as fare_amount,\n",
    "        round(cast(extra_amount as decimal(10, 2)), 2) as extra_amount,\n",
    "        round(cast(mta_tax as decimal(10, 2)), 2) as mta_tax,\n",
    "        round(cast(tip_amount as decimal(10, 2)), 2) as tip_amount,\n",
    "        round(cast(tolls_amount as decimal(10, 2)), 2) as tolls_amount,\n",
    "        round(cast(improvement_surcharge as decimal(10, 2)), 2) as improvement_surcharge,\n",
    "        round(cast(total_amount as decimal(10, 2)), 2) as total_amount,\n",
    "        round(cast(congestion_surcharge as decimal(10, 2)), 2) as congestion_surcharge,\n",
    "        round(cast(airport_fee as decimal(10, 2)), 2) as airport_fee\n",
    "\n",
    "    from source\n",
    "    -- Flink already applied quality filters; this is a safety net\n",
    "    where pickup_datetime is not null\n",
    "      and dropoff_datetime is not null\n",
    ")\n",
    "\n",
    "select * from final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2–14.5 Dimension staging models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/staging/stg_payment_types.sql\n",
    "/*\n",
    "    Staging model: Payment type lookup\n",
    "    Maps payment_type_id to human-readable names.\n",
    "*/\n",
    "\n",
    "with source as (\n",
    "    select * from {{ ref('payment_type_lookup') }}\n",
    "),\n",
    "\n",
    "renamed as (\n",
    "    select\n",
    "        payment_type_id,\n",
    "        payment_type_name\n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from renamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/staging/stg_rate_codes.sql\n",
    "/*\n",
    "    Staging model: Rate code lookup\n",
    "    Maps rate_code_id to human-readable names.\n",
    "*/\n",
    "\n",
    "with source as (\n",
    "    select * from {{ ref('rate_code_lookup') }}\n",
    "),\n",
    "\n",
    "renamed as (\n",
    "    select\n",
    "        rate_code_id,\n",
    "        rate_code_name\n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from renamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/staging/stg_taxi_zones.sql\n",
    "/*\n",
    "    Staging model: Taxi zone lookup\n",
    "    Maps LocationID to borough and zone name.\n",
    "*/\n",
    "\n",
    "with source as (\n",
    "    select * from {{ ref('taxi_zone_lookup') }}\n",
    "),\n",
    "\n",
    "renamed as (\n",
    "    select\n",
    "        cast(\"LocationID\" as integer) as location_id,\n",
    "        \"Borough\" as borough,\n",
    "        \"Zone\" as zone_name,\n",
    "        service_zone\n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from renamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/staging/stg_vendors.sql\n",
    "/*\n",
    "    Staging model: Vendor lookup\n",
    "    Maps vendor_id to vendor name and abbreviation.\n",
    "*/\n",
    "\n",
    "with source as (\n",
    "    select * from {{ ref('vendor_lookup') }}\n",
    "),\n",
    "\n",
    "renamed as (\n",
    "    select\n",
    "        vendor_id,\n",
    "        vendor_name,\n",
    "        vendor_abbr\n",
    "    from source\n",
    ")\n",
    "\n",
    "select * from renamed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.6 staging.yml — Schema documentation + tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/staging/staging.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: stg_yellow_trips\n",
    "    description: \"Cleaned and renamed yellow taxi trip records. Filters out nulls and negative fares/distances.\"\n",
    "    columns:\n",
    "      - name: trip_id\n",
    "        description: \"Surrogate key generated from VendorID + timestamps + locations + fare/total amounts\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - unique\n",
    "      - name: vendor_id\n",
    "        description: \"TPEP provider: 1=Creative Mobile Technologies, 2=VeriFone Inc., 6=Unknown/Other\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              arguments:\n",
    "                values: [1, 2, 6]\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: rate_code_id\n",
    "        description: \"Rate code: 1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester, 5=Negotiated, 6=Group\"\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              arguments:\n",
    "                values: [1, 2, 3, 4, 5, 6, 99]\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: pickup_location_id\n",
    "        description: \"TLC Taxi Zone ID for pickup\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - relationships:\n",
    "              arguments:\n",
    "                to: ref('stg_taxi_zones')\n",
    "                field: location_id\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: dropoff_location_id\n",
    "        description: \"TLC Taxi Zone ID for dropoff\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - relationships:\n",
    "              arguments:\n",
    "                to: ref('stg_taxi_zones')\n",
    "                field: location_id\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: payment_type_id\n",
    "        description: \"Payment method\"\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              arguments:\n",
    "                values: [0, 1, 2, 3, 4, 5, 6]\n",
    "              config:\n",
    "                severity: warn\n",
    "      - name: pickup_datetime\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: dropoff_datetime\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: trip_distance_miles\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: fare_amount\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: total_amount\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: stg_taxi_zones\n",
    "    description: \"Taxi zone reference mapping location IDs to borough and zone names\"\n",
    "    columns:\n",
    "      - name: location_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: borough\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: zone_name\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: stg_payment_types\n",
    "    description: \"Payment type reference\"\n",
    "    columns:\n",
    "      - name: payment_type_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: payment_type_name\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: stg_rate_codes\n",
    "    description: \"Rate code reference\"\n",
    "    columns:\n",
    "      - name: rate_code_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: rate_code_name\n",
    "        tests:\n",
    "          - not_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. dbt Intermediate Models\n",
    "\n",
    "Intermediate models add **business logic** on top of staging. This is where computed metrics\n",
    "and multi-model joins live — the place Flink explicitly does NOT touch.\n",
    "\n",
    "### Separation of Concerns: Flink vs dbt\n",
    "\n",
    "| Computation | Where | Reason |\n",
    "|-------------|-------|--------|\n",
    "| Timestamp parsing | Flink Bronze | Must happen before Iceberg write (type compatibility) |\n",
    "| Null filtering | Flink Silver | Stream-level quality gate |\n",
    "| Deduplication | Flink Silver | Needs global ordering (ingestion_ts across partitions) |\n",
    "| Column renaming | Flink Silver | Consistent schema for iceberg_scan |\n",
    "| `duration_minutes` | **dbt int_trip_metrics** | Business metric, testable, versionable |\n",
    "| `avg_speed_mph` | **dbt int_trip_metrics** | Business metric |\n",
    "| `tip_percentage` | **dbt int_trip_metrics** | Business metric |\n",
    "| `is_weekend` | **dbt int_trip_metrics** | Calendar attribute |\n",
    "| Revenue aggregation | **dbt mart_daily_revenue** | Analytics aggregate |\n",
    "\n",
    "> **Audit finding (Feb 2026):** An earlier version of the pipeline computed `duration_minutes`,\n",
    "> `avg_speed_mph`, `cost_per_mile`, `tip_percentage`, `pickup_hour`, and `is_weekend` directly\n",
    "> in the Flink Silver SQL. This was removed — business logic belongs in dbt where it's testable\n",
    "> and analysts can modify it without touching the stream processor.\n",
    "\n",
    "### 15.1 int_trip_metrics.sql — Trip-level enrichment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/intermediate/int_trip_metrics.sql\n",
    "/*\n",
    "    Intermediate model: Trip-level enrichment with calculated metrics.\n",
    "    Uses adapter-dispatched macros for cross-dialect compatibility.\n",
    "*/\n",
    "\n",
    "with trips as (\n",
    "    select * from {{ ref('stg_yellow_trips') }}\n",
    "),\n",
    "\n",
    "enriched as (\n",
    "    select\n",
    "        trip_id,\n",
    "        vendor_id,\n",
    "        rate_code_id,\n",
    "        pickup_location_id,\n",
    "        dropoff_location_id,\n",
    "        payment_type_id,\n",
    "        pickup_datetime,\n",
    "        dropoff_datetime,\n",
    "        passenger_count,\n",
    "        trip_distance_miles,\n",
    "        store_and_fwd_flag,\n",
    "\n",
    "        -- calculated: duration in minutes\n",
    "        {{ duration_minutes('pickup_datetime', 'dropoff_datetime') }} as trip_duration_minutes,\n",
    "\n",
    "        -- calculated: average speed (avoid division by zero)\n",
    "        case\n",
    "            when {{ duration_minutes('pickup_datetime', 'dropoff_datetime') }} > 0\n",
    "            then round(\n",
    "                trip_distance_miles / ({{ duration_minutes('pickup_datetime', 'dropoff_datetime') }} / 60.0),\n",
    "                2\n",
    "            )\n",
    "            else null\n",
    "        end as avg_speed_mph,\n",
    "\n",
    "        -- calculated: cost per mile\n",
    "        case\n",
    "            when trip_distance_miles > 0\n",
    "            then round(fare_amount / trip_distance_miles, 2)\n",
    "            else null\n",
    "        end as cost_per_mile,\n",
    "\n",
    "        -- calculated: tip percentage\n",
    "        case\n",
    "            when fare_amount > 0\n",
    "            then round((tip_amount / fare_amount) * 100, 2)\n",
    "            else null\n",
    "        end as tip_percentage,\n",
    "\n",
    "        -- time dimensions (using adapter-dispatched macros)\n",
    "        date_trunc('day', pickup_datetime)::date as pickup_date,\n",
    "        extract(hour from pickup_datetime) as pickup_hour,\n",
    "        {{ dayname_compat('pickup_datetime') }} as pickup_day_of_week,\n",
    "        case\n",
    "            when extract(dow from pickup_datetime) in (0, 6) then true\n",
    "            else false\n",
    "        end as is_weekend,\n",
    "\n",
    "        -- financials passthrough\n",
    "        fare_amount,\n",
    "        extra_amount,\n",
    "        mta_tax,\n",
    "        tip_amount,\n",
    "        tolls_amount,\n",
    "        improvement_surcharge,\n",
    "        total_amount,\n",
    "        congestion_surcharge,\n",
    "        airport_fee\n",
    "\n",
    "    from trips\n",
    ")\n",
    "\n",
    "select *\n",
    "from enriched\n",
    "where trip_duration_minutes between 1 and 720\n",
    "  and (avg_speed_mph is null or avg_speed_mph < 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 int_daily_summary.sql — One row per pickup date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/intermediate/int_daily_summary.sql\n",
    "/*\n",
    "    Intermediate model: Daily aggregated trip and revenue metrics.\n",
    "    One row per day with counts, averages, and revenue totals.\n",
    "*/\n",
    "\n",
    "with trip_metrics as (\n",
    "    select * from {{ ref('int_trip_metrics') }}\n",
    "),\n",
    "\n",
    "daily_agg as (\n",
    "    select\n",
    "        pickup_date,\n",
    "        pickup_day_of_week,\n",
    "        is_weekend,\n",
    "\n",
    "        count(*) as total_trips,\n",
    "        sum(passenger_count) as total_passengers,\n",
    "\n",
    "        round(avg(trip_distance_miles), 2) as avg_trip_distance,\n",
    "        round(avg(trip_duration_minutes), 2) as avg_trip_duration_min,\n",
    "        round(avg(avg_speed_mph), 2) as avg_speed_mph,\n",
    "\n",
    "        round(sum(fare_amount), 2) as total_fare_revenue,\n",
    "        round(sum(tip_amount), 2) as total_tip_revenue,\n",
    "        round(sum(total_amount), 2) as total_revenue,\n",
    "        round(avg(total_amount), 2) as avg_trip_revenue,\n",
    "        round(avg(tip_percentage), 2) as avg_tip_percentage,\n",
    "\n",
    "        count(case when payment_type_id = 1 then 1 end) as credit_card_trips,\n",
    "        count(case when payment_type_id = 2 then 1 end) as cash_trips\n",
    "\n",
    "    from trip_metrics\n",
    "    group by pickup_date, pickup_day_of_week, is_weekend\n",
    ")\n",
    "\n",
    "select * from daily_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3 int_hourly_patterns.sql — One row per pickup_date × pickup_hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/intermediate/int_hourly_patterns.sql\n",
    "/*\n",
    "    Intermediate model: Hourly trip patterns by date.\n",
    "    One row per date + hour combination.\n",
    "*/\n",
    "\n",
    "with trip_metrics as (\n",
    "    select * from {{ ref('int_trip_metrics') }}\n",
    "),\n",
    "\n",
    "hourly_agg as (\n",
    "    select\n",
    "        pickup_date,\n",
    "        pickup_hour,\n",
    "        pickup_day_of_week,\n",
    "        is_weekend,\n",
    "\n",
    "        count(*) as total_trips,\n",
    "        round(avg(trip_distance_miles), 2) as avg_distance,\n",
    "        round(avg(trip_duration_minutes), 2) as avg_duration_min,\n",
    "        round(sum(total_amount), 2) as total_revenue,\n",
    "        round(avg(total_amount), 2) as avg_revenue\n",
    "\n",
    "    from trip_metrics\n",
    "    group by pickup_date, pickup_hour, pickup_day_of_week, is_weekend\n",
    ")\n",
    "\n",
    "select * from hourly_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4 intermediate.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/intermediate/intermediate.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: int_trip_metrics\n",
    "    description: \"Trip records enriched with calculated metrics.\"\n",
    "    columns:\n",
    "      - name: trip_id\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: trip_duration_minutes\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 1\n",
    "                max_value: 720\n",
    "      - name: pickup_date\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: pickup_hour\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 0\n",
    "                max_value: 23\n",
    "      - name: is_weekend\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: int_daily_summary\n",
    "    description: \"Daily aggregated trip counts, revenue, and average metrics\"\n",
    "    columns:\n",
    "      - name: pickup_date\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: total_trips\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 0\n",
    "      - name: total_revenue\n",
    "        tests:\n",
    "          - not_null\n",
    "\n",
    "  - name: int_hourly_patterns\n",
    "    description: \"Hourly trip aggregations by date\"\n",
    "    columns:\n",
    "      - name: pickup_date\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: pickup_hour\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.accepted_range:\n",
    "              arguments:\n",
    "                min_value: 0\n",
    "                max_value: 23\n",
    "      - name: total_trips\n",
    "        tests:\n",
    "          - not_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. dbt Core Marts (Gold Layer — Star Schema)\n",
    "\n",
    "Core marts form the **star schema** that BI tools query directly.\n",
    "\n",
    "```\n",
    "                    dim_dates\n",
    "                    (31 rows)\n",
    "                        │\n",
    "dim_vendors ────── fct_trips ────── dim_payment_types\n",
    "(2 rows)        (~9,855 rows)        (6 rows)\n",
    "                        │\n",
    "                  dim_locations\n",
    "                  (265 rows)\n",
    "```\n",
    "\n",
    "**Grain of fct_trips:** One row per taxi trip, identified by `trip_id` (MD5 surrogate key\n",
    "generated by Flink Silver from the natural key). Joins `int_trip_metrics` with all dimensions.\n",
    "\n",
    "### 16.1 fct_trips.sql — Central fact table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/core/fct_trips.sql\n",
    "/*\n",
    "    Fact table: Fully enriched trip records with location names.\n",
    "    Incremental with delete+insert strategy.\n",
    "*/\n",
    "\n",
    "{{\n",
    "  config(\n",
    "    materialized='incremental',\n",
    "    unique_key='trip_id',\n",
    "    incremental_strategy='delete+insert',\n",
    "    on_schema_change='fail'\n",
    "  )\n",
    "}}\n",
    "\n",
    "with trip_metrics as (\n",
    "    select * from {{ ref('int_trip_metrics') }}\n",
    "),\n",
    "\n",
    "pickup_locations as (\n",
    "    select * from {{ ref('dim_locations') }}\n",
    "),\n",
    "\n",
    "dropoff_locations as (\n",
    "    select * from {{ ref('dim_locations') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        t.trip_id,\n",
    "        t.vendor_id,\n",
    "        t.rate_code_id,\n",
    "        t.payment_type_id,\n",
    "        t.pickup_location_id,\n",
    "        t.dropoff_location_id,\n",
    "        t.pickup_datetime,\n",
    "        t.dropoff_datetime,\n",
    "        t.pickup_date,\n",
    "        t.pickup_hour,\n",
    "        t.pickup_day_of_week,\n",
    "        t.is_weekend,\n",
    "        t.passenger_count,\n",
    "        t.trip_distance_miles,\n",
    "        t.trip_duration_minutes,\n",
    "        t.avg_speed_mph,\n",
    "        t.cost_per_mile,\n",
    "        t.fare_amount,\n",
    "        t.extra_amount,\n",
    "        t.mta_tax,\n",
    "        t.tip_amount,\n",
    "        t.tip_percentage,\n",
    "        t.tolls_amount,\n",
    "        t.improvement_surcharge,\n",
    "        t.total_amount,\n",
    "        t.congestion_surcharge,\n",
    "        t.airport_fee,\n",
    "\n",
    "        -- enriched from dimensions\n",
    "        pu.borough as pickup_borough,\n",
    "        pu.zone_name as pickup_zone,\n",
    "        do_loc.borough as dropoff_borough,\n",
    "        do_loc.zone_name as dropoff_zone\n",
    "\n",
    "    from trip_metrics t\n",
    "    left join pickup_locations pu\n",
    "        on t.pickup_location_id = pu.location_id\n",
    "    left join dropoff_locations do_loc\n",
    "        on t.dropoff_location_id = do_loc.location_id\n",
    "\n",
    "    {% if is_incremental() %}\n",
    "    where t.pickup_datetime > (select max(pickup_datetime) from {{ this }})\n",
    "    {% endif %}\n",
    ")\n",
    "\n",
    "select * from final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.2–16.5 Dimension tables\n",
    "\n",
    "- `dim_dates` — one row per calendar date (Jan 2024) with week, month, is_weekend attributes\n",
    "- `dim_locations` — 265 NYC taxi zones with borough and service_zone\n",
    "- `dim_payment_types` — maps payment_type_id to description\n",
    "- `dim_vendors` — maps VendorID to vendor name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/core/dim_dates.sql\n",
    "/*\n",
    "    Dimension table: Calendar dates for January 2024.\n",
    "    Uses adapter-dispatched macros for dayname/monthname.\n",
    "*/\n",
    "\n",
    "with date_spine as (\n",
    "    {{ dbt_utils.date_spine(\n",
    "        datepart=\"day\",\n",
    "        start_date=\"cast('2024-01-01' as date)\",\n",
    "        end_date=\"cast('2024-02-01' as date)\"\n",
    "    ) }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        cast(date_day as date) as date_key,\n",
    "        extract(year from date_day) as year,\n",
    "        extract(month from date_day) as month,\n",
    "        extract(day from date_day) as day_of_month,\n",
    "        extract(dow from date_day) as day_of_week_num,\n",
    "        {{ dayname_compat('date_day') }} as day_of_week_name,\n",
    "        {{ monthname_compat('date_day') }} as month_name,\n",
    "        extract(week from date_day) as week_of_year,\n",
    "        case\n",
    "            when extract(dow from date_day) in (0, 6) then true\n",
    "            else false\n",
    "        end as is_weekend,\n",
    "        case\n",
    "            when cast(date_day as date) in (\n",
    "                cast('2024-01-01' as date),\n",
    "                cast('2024-01-15' as date)\n",
    "            ) then true\n",
    "            else false\n",
    "        end as is_holiday\n",
    "\n",
    "    from date_spine\n",
    ")\n",
    "\n",
    "select * from final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/core/dim_locations.sql\n",
    "/*\n",
    "    Dimension table: TLC Taxi Zone locations.\n",
    "*/\n",
    "\n",
    "with zones as (\n",
    "    select * from {{ ref('stg_taxi_zones') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        location_id,\n",
    "        borough,\n",
    "        zone_name,\n",
    "        service_zone\n",
    "    from zones\n",
    ")\n",
    "\n",
    "select * from final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/core/dim_payment_types.sql\n",
    "/*\n",
    "    Dimension table: Payment type descriptions.\n",
    "*/\n",
    "\n",
    "with payment_types as (\n",
    "    select * from {{ ref('stg_payment_types') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        payment_type_id,\n",
    "        payment_type_name\n",
    "    from payment_types\n",
    ")\n",
    "\n",
    "select * from final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/core/dim_vendors.sql\n",
    "/*\n",
    "    Dimension table: Taxi vendor descriptions.\n",
    "    TPEP provider: 1=Creative Mobile Technologies (CMT), 2=VeriFone Inc. (VFI)\n",
    "*/\n",
    "\n",
    "with vendors as (\n",
    "    select * from {{ ref('stg_vendors') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        vendor_id,\n",
    "        vendor_name,\n",
    "        vendor_abbr\n",
    "    from vendors\n",
    ")\n",
    "\n",
    "select * from final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.6 core.yml — Data contracts + referential integrity tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/core/core.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: fct_trips\n",
    "    description: \"Fact table with fully enriched trip records.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: trip_id\n",
    "        data_type: varchar\n",
    "        tests: [not_null, unique]\n",
    "      - name: vendor_id\n",
    "        data_type: integer\n",
    "      - name: rate_code_id\n",
    "        data_type: integer\n",
    "      - name: payment_type_id\n",
    "        data_type: integer\n",
    "      - name: pickup_location_id\n",
    "        data_type: integer\n",
    "      - name: dropoff_location_id\n",
    "        data_type: integer\n",
    "      - name: pickup_datetime\n",
    "        data_type: timestamp\n",
    "        tests: [not_null]\n",
    "      - name: dropoff_datetime\n",
    "        data_type: timestamp\n",
    "      - name: pickup_date\n",
    "        data_type: date\n",
    "      - name: pickup_hour\n",
    "        data_type: bigint\n",
    "      - name: pickup_day_of_week\n",
    "        data_type: varchar\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "      - name: passenger_count\n",
    "        data_type: integer\n",
    "      - name: trip_distance_miles\n",
    "        data_type: double\n",
    "      - name: trip_duration_minutes\n",
    "        data_type: bigint\n",
    "      - name: avg_speed_mph\n",
    "        data_type: double\n",
    "      - name: cost_per_mile\n",
    "        data_type: double\n",
    "      - name: fare_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: extra_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: mta_tax\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: tip_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: tip_percentage\n",
    "        data_type: double\n",
    "      - name: tolls_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: improvement_surcharge\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: total_amount\n",
    "        data_type: \"decimal(10,2)\"\n",
    "        tests: [not_null]\n",
    "      - name: congestion_surcharge\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: airport_fee\n",
    "        data_type: \"decimal(10,2)\"\n",
    "      - name: pickup_borough\n",
    "        data_type: varchar\n",
    "      - name: pickup_zone\n",
    "        data_type: varchar\n",
    "      - name: dropoff_borough\n",
    "        data_type: varchar\n",
    "      - name: dropoff_zone\n",
    "        data_type: varchar\n",
    "\n",
    "  - name: dim_locations\n",
    "    description: \"Location dimension\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: location_id\n",
    "        data_type: integer\n",
    "        tests: [unique, not_null]\n",
    "      - name: borough\n",
    "        data_type: varchar\n",
    "        tests: [not_null]\n",
    "      - name: zone_name\n",
    "        data_type: varchar\n",
    "        tests: [not_null]\n",
    "      - name: service_zone\n",
    "        data_type: varchar\n",
    "\n",
    "  - name: dim_dates\n",
    "    description: \"Date dimension for January 2024\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: date_key\n",
    "        data_type: date\n",
    "        tests: [unique, not_null]\n",
    "      - name: year\n",
    "        data_type: bigint\n",
    "      - name: month\n",
    "        data_type: bigint\n",
    "      - name: day_of_month\n",
    "        data_type: bigint\n",
    "      - name: day_of_week_num\n",
    "        data_type: bigint\n",
    "      - name: day_of_week_name\n",
    "        data_type: varchar\n",
    "        tests: [not_null]\n",
    "      - name: month_name\n",
    "        data_type: varchar\n",
    "      - name: week_of_year\n",
    "        data_type: bigint\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "        tests: [not_null]\n",
    "      - name: is_holiday\n",
    "        data_type: boolean\n",
    "        tests: [not_null]\n",
    "\n",
    "  - name: dim_payment_types\n",
    "    description: \"Payment type dimension\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: payment_type_id\n",
    "        data_type: integer\n",
    "        tests: [unique, not_null]\n",
    "      - name: payment_type_name\n",
    "        data_type: varchar\n",
    "        tests: [not_null]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. dbt Analytics Marts (Gold Layer — Business KPIs)\n",
    "\n",
    "Analytics marts are pre-aggregated tables optimized for dashboards. They answer specific\n",
    "business questions without requiring analysts to write complex SQL.\n",
    "\n",
    "### 17.1 mart_daily_revenue.sql — Daily revenue KPIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/analytics/mart_daily_revenue.sql\n",
    "/*\n",
    "    Analytics mart: Daily revenue metrics with running totals.\n",
    "*/\n",
    "\n",
    "with daily as (\n",
    "    select * from {{ ref('int_daily_summary') }}\n",
    "),\n",
    "\n",
    "dates as (\n",
    "    select * from {{ ref('dim_dates') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        d.date_key,\n",
    "        d.day_of_week_name,\n",
    "        d.is_weekend,\n",
    "        d.is_holiday,\n",
    "        d.week_of_year,\n",
    "\n",
    "        daily.total_trips,\n",
    "        daily.total_passengers,\n",
    "        daily.total_fare_revenue,\n",
    "        daily.total_tip_revenue,\n",
    "        daily.total_revenue,\n",
    "        daily.avg_trip_revenue,\n",
    "        daily.avg_tip_percentage,\n",
    "        daily.credit_card_trips,\n",
    "        daily.cash_trips,\n",
    "        daily.avg_trip_distance,\n",
    "        daily.avg_trip_duration_min,\n",
    "\n",
    "        -- running total\n",
    "        sum(daily.total_revenue) over (order by d.date_key) as cumulative_revenue,\n",
    "\n",
    "        -- day-over-day change\n",
    "        daily.total_revenue - lag(daily.total_revenue) over (order by d.date_key) as revenue_change_vs_prior_day\n",
    "\n",
    "    from daily\n",
    "    inner join dates d\n",
    "        on daily.pickup_date = d.date_key\n",
    ")\n",
    "\n",
    "select * from final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.2 mart_hourly_demand.sql — Hourly demand patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/analytics/mart_hourly_demand.sql\n",
    "/*\n",
    "    Analytics mart: Hourly demand patterns.\n",
    "*/\n",
    "\n",
    "with hourly as (\n",
    "    select * from {{ ref('int_hourly_patterns') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        pickup_hour,\n",
    "        is_weekend,\n",
    "\n",
    "        count(*) as days_observed,\n",
    "        round(avg(total_trips), 0) as avg_trips_per_period,\n",
    "        round(avg(avg_distance), 2) as avg_distance,\n",
    "        round(avg(avg_duration_min), 2) as avg_duration_min,\n",
    "        round(avg(total_revenue), 2) as avg_revenue_per_period,\n",
    "        sum(total_trips) as total_trips_all_days\n",
    "\n",
    "    from hourly\n",
    "    group by pickup_hour, is_weekend\n",
    ")\n",
    "\n",
    "select * from final\n",
    "order by is_weekend, pickup_hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.3 mart_location_performance.sql — Per-zone analytics\n",
    "\n",
    "Includes `rank() OVER (ORDER BY total_revenue DESC)` window function for revenue ranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/analytics/mart_location_performance.sql\n",
    "/*\n",
    "    Analytics mart: Location-level performance summary.\n",
    "    Uses adapter-dispatched mode_compat() for cross-dialect support.\n",
    "*/\n",
    "\n",
    "with trips as (\n",
    "    select * from {{ ref('fct_trips') }}\n",
    "),\n",
    "\n",
    "final as (\n",
    "    select\n",
    "        pickup_location_id,\n",
    "        pickup_borough,\n",
    "        pickup_zone,\n",
    "\n",
    "        count(*) as total_pickups,\n",
    "        round(avg(trip_distance_miles), 2) as avg_trip_distance,\n",
    "        round(avg(trip_duration_minutes), 2) as avg_trip_duration_min,\n",
    "        round(sum(total_amount), 2) as total_revenue,\n",
    "        round(avg(total_amount), 2) as avg_revenue_per_trip,\n",
    "        round(avg(tip_percentage), 2) as avg_tip_pct,\n",
    "        round(avg(passenger_count), 2) as avg_passengers,\n",
    "\n",
    "        -- most common dropoff destination\n",
    "        {{ mode_compat('dropoff_zone') }} as most_common_dropoff_zone,\n",
    "\n",
    "        -- busiest hour\n",
    "        {{ mode_compat('pickup_hour') }} as peak_pickup_hour\n",
    "\n",
    "    from trips\n",
    "    where pickup_zone is not null\n",
    "    group by pickup_location_id, pickup_borough, pickup_zone\n",
    ")\n",
    "\n",
    "select * from final\n",
    "order by total_pickups desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4 analytics.yml — Analytics model tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/models/marts/analytics/analytics.yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: mart_daily_revenue\n",
    "    description: \"Daily revenue metrics with running totals.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: date_key\n",
    "        data_type: date\n",
    "        tests: [unique, not_null]\n",
    "      - name: day_of_week_name\n",
    "        data_type: varchar\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "      - name: is_holiday\n",
    "        data_type: boolean\n",
    "      - name: week_of_year\n",
    "        data_type: bigint\n",
    "      - name: total_trips\n",
    "        data_type: bigint\n",
    "      - name: total_passengers\n",
    "        data_type: hugeint\n",
    "      - name: total_fare_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: total_tip_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: total_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "        tests: [not_null]\n",
    "      - name: avg_trip_revenue\n",
    "        data_type: double\n",
    "      - name: avg_tip_percentage\n",
    "        data_type: double\n",
    "      - name: credit_card_trips\n",
    "        data_type: bigint\n",
    "      - name: cash_trips\n",
    "        data_type: bigint\n",
    "      - name: avg_trip_distance\n",
    "        data_type: double\n",
    "      - name: avg_trip_duration_min\n",
    "        data_type: double\n",
    "      - name: cumulative_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: revenue_change_vs_prior_day\n",
    "        data_type: \"decimal(38,2)\"\n",
    "\n",
    "  - name: mart_location_performance\n",
    "    description: \"Per-zone performance summary.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: pickup_location_id\n",
    "        data_type: integer\n",
    "        tests: [unique, not_null]\n",
    "      - name: pickup_borough\n",
    "        data_type: varchar\n",
    "      - name: pickup_zone\n",
    "        data_type: varchar\n",
    "      - name: total_pickups\n",
    "        data_type: bigint\n",
    "        tests: [not_null]\n",
    "      - name: avg_trip_distance\n",
    "        data_type: double\n",
    "      - name: avg_trip_duration_min\n",
    "        data_type: double\n",
    "      - name: total_revenue\n",
    "        data_type: \"decimal(38,2)\"\n",
    "      - name: avg_revenue_per_trip\n",
    "        data_type: double\n",
    "      - name: avg_tip_pct\n",
    "        data_type: double\n",
    "      - name: avg_passengers\n",
    "        data_type: double\n",
    "      - name: most_common_dropoff_zone\n",
    "        data_type: varchar\n",
    "      - name: peak_pickup_hour\n",
    "        data_type: bigint\n",
    "\n",
    "  - name: mart_hourly_demand\n",
    "    description: \"Hourly demand patterns.\"\n",
    "    config:\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: pickup_hour\n",
    "        data_type: bigint\n",
    "        tests: [not_null]\n",
    "      - name: is_weekend\n",
    "        data_type: boolean\n",
    "        tests: [not_null]\n",
    "      - name: days_observed\n",
    "        data_type: bigint\n",
    "      - name: avg_trips_per_period\n",
    "        data_type: double\n",
    "      - name: avg_distance\n",
    "        data_type: double\n",
    "      - name: avg_duration_min\n",
    "        data_type: double\n",
    "      - name: avg_revenue_per_period\n",
    "        data_type: double\n",
    "      - name: total_trips_all_days\n",
    "        data_type: hugeint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. dbt Tests: Data Quality Assertions\n",
    "\n",
    "dbt has two test types:\n",
    "\n",
    "| Type | Location | Failure condition | Example |\n",
    "|------|----------|------------------|---------|\n",
    "| **Schema tests** | `.yml` files | SQL returns rows with violations | `not_null`, `unique`, `accepted_values` |\n",
    "| **Singular tests** | `tests/*.sql` | Query returns ANY rows | `assert_fare_not_exceeds_total.sql` |\n",
    "\n",
    "### Total test count: 91 tests\n",
    "\n",
    "- Staging schema tests: ~35 (not_null, unique, accepted_values on all columns)\n",
    "- Intermediate tests: ~20 (relationships between models)\n",
    "- Core mart tests: ~25 (referential integrity, not_null on fact)\n",
    "- Analytics mart tests: ~8 (aggregated column tests)\n",
    "- Singular tests: 2 (business rule assertions)\n",
    "- Seed tests: 1 (payment_type uniqueness)\n",
    "\n",
    "### 18.1 assert_fare_not_exceeds_total.sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/tests/assert_fare_not_exceeds_total.sql\n",
    "/*\n",
    "    Singular test: fare_amount should not exceed total_amount.\n",
    "*/\n",
    "\n",
    "select\n",
    "    trip_id,\n",
    "    fare_amount,\n",
    "    total_amount\n",
    "from {{ ref('stg_yellow_trips') }}\n",
    "where fare_amount > total_amount + 0.01\n",
    "  and total_amount > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2 assert_trip_duration_positive.sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/dbt_project/tests/assert_trip_duration_positive.sql\n",
    "/*\n",
    "    Singular test: No trip should have negative duration.\n",
    "*/\n",
    "\n",
    "select\n",
    "    trip_id,\n",
    "    pickup_datetime,\n",
    "    dropoff_datetime,\n",
    "    trip_duration_minutes\n",
    "from {{ ref('int_trip_metrics') }}\n",
    "where trip_duration_minutes < 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Pipeline Makefile: One-Command Orchestration\n",
    "\n",
    "The Makefile provides named targets for every pipeline operation.\n",
    "\n",
    "### Target Map\n",
    "\n",
    "```\n",
    "make up                → docker compose up -d (start 6 always-on containers)\n",
    "make create-topics     → rpk topic create taxi.raw_trips + taxi.raw_trips.dlq\n",
    "make generate          → start data generator (unlimited events, burst mode)\n",
    "make generate-limited  → 10,000 events then stop (benchmark mode)\n",
    "make process-bronze    → Flink SQL batch: Redpanda → Bronze Iceberg\n",
    "make process-silver    → Flink SQL batch: Bronze → Silver Iceberg (dedup + filter)\n",
    "make process           → process-bronze + process-silver\n",
    "make process-streaming → Flink SQL streaming: Redpanda → Bronze (continuous, indefinite)\n",
    "make dbt-build         → dbt deps && dbt build --full-refresh (all models + 91 tests)\n",
    "make benchmark         → full E2E: down→up→topics→generate→process→sleep5→dbt→down\n",
    "make status            → show running containers + Redpanda topic list + Flink job list\n",
    "make health            → check Flink UI, MinIO health, Redpanda cluster health\n",
    "make down              → docker compose down -v (stop + remove volumes)\n",
    "```\n",
    "\n",
    "### Key Makefile Patterns\n",
    "\n",
    "```makefile\n",
    "# Windows Git Bash compatibility\n",
    "MSYS_NO_PATHCONV=1 $(COMPOSE) exec flink-jobmanager /opt/flink/bin/sql-client.sh embedded ...\n",
    "#   ↑ Prevents MSYS2 from converting /opt/flink to C:/Program Files/...\n",
    "\n",
    "# Flink SQL with init + execute\n",
    "$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/05-bronze.sql\n",
    "#                      ──────────────────────────    ─────────────────────────────\n",
    "#                      session state (tables, catalog)  execute this file\n",
    "\n",
    "# Benchmark with race condition fix\n",
    "$(MAKE) process && \\\n",
    "    sleep 5 && \\        ← wait for Iceberg metadata to fully flush\n",
    "    $(MAKE) dbt-build   ← then dbt can see committed Iceberg snapshots\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pipelines/04-redpanda-flink-iceberg/Makefile\n",
    "SHELL := bash\n",
    "# =============================================================================\n",
    "# Pipeline 04: Redpanda + Flink + Iceberg\n",
    "# =============================================================================\n",
    "# Makefile for orchestrating the complete streaming pipeline lifecycle.\n",
    "# Fork of Pipeline 01 with Redpanda replacing Kafka + Schema Registry.\n",
    "# =============================================================================\n",
    "\n",
    "COMPOSE = docker compose\n",
    "FLINK_SQL_CLIENT = MSYS_NO_PATHCONV=1 $(COMPOSE) exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded\n",
    "\n",
    ".PHONY: help up down generate create-topics process process-bronze process-silver \\\n",
    "        process-streaming dbt-build benchmark logs status clean ps restart \\\n",
    "        check-lag health compact-silver expire-snapshots vacuum maintain\n",
    "\n",
    "help: ## Show this help\n",
    "\t@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | \\\n",
    "\t\tawk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-20s\\033[0m %s\\n\", $$1, $$2}'\n",
    "\n",
    "# =============================================================================\n",
    "# Lifecycle\n",
    "# =============================================================================\n",
    "\n",
    "up: ## Start all infrastructure services\n",
    "\t$(COMPOSE) up -d\n",
    "\t@echo \"\"\n",
    "\t@echo \"=== Pipeline 04: Redpanda + Flink + Iceberg ===\"\n",
    "\t@echo \"Redpanda Kafka API: localhost:19092\"\n",
    "\t@echo \"Schema Registry:    http://localhost:18081\"\n",
    "\t@echo \"Pandaproxy:         http://localhost:18082\"\n",
    "\t@echo \"Redpanda Admin:     http://localhost:9644\"\n",
    "\t@echo \"Flink Dashboard:    http://localhost:8081\"\n",
    "\t@echo \"MinIO Console:      http://localhost:9001  (minioadmin/minioadmin)\"\n",
    "\t@echo \"\"\n",
    "\t@echo \"Next steps:\"\n",
    "\t@echo \"  make create-topics   # Create Redpanda topics\"\n",
    "\t@echo \"  make generate        # Produce taxi events to Redpanda\"\n",
    "\t@echo \"  make process         # Submit Flink SQL jobs\"\n",
    "\t@echo \"  make dbt-build       # Run dbt transformations\"\n",
    "\n",
    "down: ## Stop all services and remove volumes\n",
    "\t$(COMPOSE) --profile generator --profile dbt down -v\n",
    "\t@echo \"Pipeline 04 stopped and volumes removed.\"\n",
    "\n",
    "clean: ## Stop everything and prune all related resources\n",
    "\t$(COMPOSE) --profile generator --profile dbt down -v --remove-orphans\n",
    "\tdocker network rm p04-pipeline-net 2>/dev/null || true\n",
    "\t@echo \"Pipeline 04 fully cleaned.\"\n",
    "\n",
    "restart: ## Restart all services\n",
    "\t$(MAKE) down\n",
    "\t$(MAKE) up\n",
    "\n",
    "# =============================================================================\n",
    "# Topic Management\n",
    "# =============================================================================\n",
    "\n",
    "create-topics: ## Create Redpanda topics (primary + Dead Letter Queue)\n",
    "\t$(COMPOSE) exec redpanda rpk topic create taxi.raw_trips \\\n",
    "\t\t--brokers localhost:9092 \\\n",
    "\t\t--partitions 3 \\\n",
    "\t\t--replicas 1 \\\n",
    "\t\t--topic-config retention.ms=259200000 \\\n",
    "\t\t--topic-config cleanup.policy=delete || true\n",
    "\t$(COMPOSE) exec redpanda rpk topic create taxi.raw_trips.dlq \\\n",
    "\t\t--brokers localhost:9092 \\\n",
    "\t\t--partitions 1 \\\n",
    "\t\t--replicas 1 \\\n",
    "\t\t--topic-config retention.ms=604800000 \\\n",
    "\t\t--topic-config cleanup.policy=delete || true\n",
    "\t@$(COMPOSE) exec redpanda rpk topic list --brokers localhost:9092\n",
    "\t@echo \"Topics created: taxi.raw_trips (3 partitions) + taxi.raw_trips.dlq (DLQ, 7-day retention).\"\n",
    "\n",
    "# =============================================================================\n",
    "# Data Generation\n",
    "# =============================================================================\n",
    "\n",
    "generate: ## Produce taxi trip events to Redpanda (burst mode)\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm data-generator\n",
    "\t@echo \"Data generation complete.\"\n",
    "\n",
    "generate-limited: ## Produce limited events for testing (10k)\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm -e MAX_EVENTS=10000 data-generator\n",
    "\t@echo \"Limited data generation complete (10k events).\"\n",
    "\n",
    "# =============================================================================\n",
    "# Flink SQL Processing\n",
    "# =============================================================================\n",
    "\n",
    "process: process-bronze process-silver ## Submit all Flink SQL jobs (Bronze + Silver)\n",
    "\t@echo \"All Flink SQL jobs complete.\"\n",
    "\n",
    "process-bronze: ## Submit Bronze layer Flink SQL jobs (batch mode)\n",
    "\t@echo \"=== Bronze: Kafka → Iceberg ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/05-bronze.sql\n",
    "\t@echo \"Bronze layer complete.\"\n",
    "\n",
    "process-silver: ## Submit Silver layer Flink SQL jobs (batch mode)\n",
    "\t@echo \"=== Silver: Bronze → Cleaned Iceberg ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/06-silver.sql\n",
    "\t@echo \"Silver layer complete.\"\n",
    "\n",
    "process-streaming: ## Start continuous streaming Bronze job (Redpanda → Iceberg, runs indefinitely)\n",
    "\t@echo \"=== Streaming Bronze: Redpanda → Iceberg (continuous) ===\"\n",
    "\t@echo \"NOTE: This job runs indefinitely. Cancel with Ctrl+C or kill the Flink job.\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init-streaming.sql -f /opt/flink/sql/07-streaming-bronze.sql\n",
    "\n",
    "# =============================================================================\n",
    "# dbt Transformations\n",
    "# =============================================================================\n",
    "\n",
    "dbt-build: ## Run dbt build (full-refresh) on Iceberg Silver data\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm --entrypoint /bin/sh dbt -c \"dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir .\"\n",
    "\t@echo \"dbt build complete.\"\n",
    "\n",
    "dbt-test: ## Run dbt tests only\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm dbt test --profiles-dir .\n",
    "\n",
    "dbt-docs: ## Generate dbt documentation\n",
    "\tMSYS_NO_PATHCONV=1 $(COMPOSE) run --rm dbt docs generate --profiles-dir .\n",
    "\n",
    "# =============================================================================\n",
    "# Benchmark (Full E2E)\n",
    "# =============================================================================\n",
    "\n",
    "benchmark: ## Full end-to-end benchmark: down -> up → topics → generate → process → dbt → down\n",
    "\t@echo \"============================================================\"\n",
    "\t@echo \"  Pipeline 04 Benchmark: Redpanda + Flink + Iceberg\"\n",
    "\t@echo \"============================================================\"\n",
    "\t@START_TIME=$$(date +%s) && \\\n",
    "\t$(MAKE) down 2>/dev/null || true && \\\n",
    "\t$(MAKE) up && \\\n",
    "\techo \"Waiting for services to stabilize...\" && \\\n",
    "\tsleep 15 && \\\n",
    "\t$(MAKE) create-topics && \\\n",
    "\t$(MAKE) generate-limited && \\\n",
    "\techo \"Waiting for Flink processing to catch up...\" && \\\n",
    "\tsleep 10 && \\\n",
    "\t$(MAKE) process && \\\n",
    "\techo \"Waiting for Iceberg metadata commits to finalize...\" && \\\n",
    "\tsleep 15 && \\\n",
    "\t$(MAKE) dbt-build && \\\n",
    "\tEND_TIME=$$(date +%s) && \\\n",
    "\tELAPSED=$$((END_TIME - START_TIME)) && \\\n",
    "\techo \"\" && \\\n",
    "\techo \"============================================================\" && \\\n",
    "\techo \"  BENCHMARK COMPLETE\" && \\\n",
    "\techo \"  Total elapsed: $${ELAPSED}s\" && \\\n",
    "\techo \"============================================================\" && \\\n",
    "\tmkdir -p benchmark_results && \\\n",
    "\techo \"{\\\"pipeline\\\": \\\"04-redpanda-flink-iceberg\\\", \\\"elapsed_seconds\\\": $$ELAPSED, \\\"timestamp\\\": \\\"$$(date -Iseconds)\\\"}\" > benchmark_results/latest.json && \\\n",
    "\techo \"Results saved to benchmark_results/latest.json\" && \\\n",
    "\t$(MAKE) down\n",
    "\n",
    "# =============================================================================\n",
    "# Observability\n",
    "# =============================================================================\n",
    "\n",
    "logs: ## Tail logs from all services\n",
    "\t$(COMPOSE) logs -f --tail=100\n",
    "\n",
    "logs-redpanda: ## Tail Redpanda logs\n",
    "\t$(COMPOSE) logs -f redpanda\n",
    "\n",
    "logs-flink: ## Tail Flink JobManager logs\n",
    "\t$(COMPOSE) logs -f flink-jobmanager\n",
    "\n",
    "logs-flink-tm: ## Tail Flink TaskManager logs\n",
    "\t$(COMPOSE) logs -f flink-taskmanager\n",
    "\n",
    "status: ## Show service status\n",
    "\t@echo \"=== Pipeline 04: Service Status ===\"\n",
    "\t$(COMPOSE) ps\n",
    "\t@echo \"\"\n",
    "\t@echo \"=== Redpanda Topics ===\"\n",
    "\t$(COMPOSE) exec redpanda rpk topic list --brokers localhost:9092 2>/dev/null || echo \"(Redpanda not running)\"\n",
    "\t@echo \"\"\n",
    "\t@echo \"=== Flink Jobs ===\"\n",
    "\t@curl -s http://localhost:8081/jobs/overview 2>/dev/null | python3 -m json.tool 2>/dev/null || echo \"(Flink not running)\"\n",
    "\n",
    "ps: ## Show running containers\n",
    "\t$(COMPOSE) ps\n",
    "\n",
    "# =============================================================================\n",
    "# Health & Diagnostics\n",
    "# =============================================================================\n",
    "\n",
    "check-lag: ## Show Redpanda consumer group lag and DLQ status\n",
    "\t$(COMPOSE) exec redpanda rpk group describe flink-consumer --brokers localhost:9092\n",
    "\t@echo \"\"\n",
    "\t@$(COMPOSE) exec -T redpanda rpk topic consume taxi.raw_trips.dlq \\\n",
    "\t    --brokers localhost:9092 --num 1 2>/dev/null \\\n",
    "\t    && echo \"DLQ has messages - investigate!\" || echo \"DLQ: empty (OK)\"\n",
    "\n",
    "health: ## Quick health check of all services\n",
    "\t@echo \"=== Pipeline 04: Health Check ===\"\n",
    "\t@echo -n \"Redpanda:         \" && $(COMPOSE) exec -T redpanda \\\n",
    "\t    rpk cluster health --brokers localhost:9092 2>/dev/null | grep -q \"Healthy\" \\\n",
    "\t    && echo \"OK\" || echo \"FAIL\"\n",
    "\t@echo -n \"Flink Dashboard:  \" && curl -sf http://localhost:8081/overview > /dev/null 2>&1 \\\n",
    "\t    && echo \"OK\" || echo \"FAIL\"\n",
    "\t@echo -n \"MinIO:            \" && curl -sf http://localhost:9000/minio/health/live > /dev/null 2>&1 \\\n",
    "\t    && echo \"OK\" || echo \"FAIL\"\n",
    "\t@echo -n \"Redpanda Console: \" && curl -sf http://localhost:8085 > /dev/null 2>&1 \\\n",
    "\t    && echo \"OK\" || echo \"FAIL\"\n",
    "\n",
    "# =============================================================================\n",
    "# Iceberg Maintenance (run periodically in production)\n",
    "# =============================================================================\n",
    "\n",
    "compact-silver: ## Compact Silver Iceberg files to target 128MB file size\n",
    "\t@echo \"=== Compacting Silver Iceberg table ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql <<'EOF'\n",
    "\tCALL iceberg_catalog.system.rewrite_data_files(\n",
    "\t    table => 'silver.cleaned_trips',\n",
    "\t    options => map['target-file-size-bytes', '134217728',\n",
    "\t                   'min-file-size-threshold', '33554432']\n",
    "\t);\n",
    "\tEOF\n",
    "\t@echo \"Silver compaction complete.\"\n",
    "\n",
    "expire-snapshots: ## Expire Iceberg snapshots older than 7 days\n",
    "\t@echo \"=== Expiring old Iceberg snapshots ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql <<'EOF'\n",
    "\tCALL iceberg_catalog.system.expire_snapshots(\n",
    "\t    table => 'silver.cleaned_trips',\n",
    "\t    older_than => TIMESTAMPADD(DAY, -7, NOW())\n",
    "\t);\n",
    "\tCALL iceberg_catalog.system.expire_snapshots(\n",
    "\t    table => 'bronze.raw_trips',\n",
    "\t    older_than => TIMESTAMPADD(DAY, -7, NOW())\n",
    "\t);\n",
    "\tEOF\n",
    "\t@echo \"Snapshot expiry complete.\"\n",
    "\n",
    "vacuum: ## Remove orphan files from Iceberg warehouse (run weekly)\n",
    "\t@echo \"=== Removing orphan Iceberg files ===\"\n",
    "\t$(FLINK_SQL_CLIENT) -i /opt/flink/sql/00-init.sql <<'EOF'\n",
    "\tCALL iceberg_catalog.system.remove_orphan_files(\n",
    "\t    table => 'silver.cleaned_trips',\n",
    "\t    older_than => TIMESTAMPADD(DAY, -7, NOW())\n",
    "\t);\n",
    "\tEOF\n",
    "\t@echo \"Vacuum complete.\"\n",
    "\n",
    "maintain: compact-silver expire-snapshots ## Run all routine Iceberg maintenance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Running the Pipeline\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Docker Desktop running (Linux containers), 8+ GB RAM allocated\n",
    "- Docker Compose V2 (`docker compose`, not `docker-compose`)\n",
    "- `make` installed (Windows: `scoop install make`)\n",
    "- Free ports: `19092` (Redpanda), `8081` (Flink UI), `8085` (Redpanda Console), `9000-9001` (MinIO)\n",
    "- Data: `data/yellow_tripdata_2024-01.parquet` in project root\n",
    "\n",
    "### Quick Start (Batch Benchmark)\n",
    "\n",
    "```bash\n",
    "cd pipelines/04-redpanda-flink-iceberg\n",
    "\n",
    "# One command — full E2E benchmark with timing\n",
    "make benchmark\n",
    "# Expected output: Total elapsed: ~88s (includes 5s metadata flush sleep)\n",
    "# Results written to: benchmark_results/latest.json\n",
    "```\n",
    "\n",
    "### Step-by-Step with Validation\n",
    "\n",
    "```bash\n",
    "# 1. Start all services\n",
    "make up\n",
    "\n",
    "# 2. Verify everything is healthy (~20s for Redpanda to become healthy)\n",
    "make status\n",
    "\n",
    "# 3. Create topics (primary + DLQ)\n",
    "make create-topics\n",
    "# Expected:\n",
    "#   NAME               PARTITIONS  REPLICAS\n",
    "#   taxi.raw_trips     3           1\n",
    "#   taxi.raw_trips.dlq 1           1\n",
    "\n",
    "# 4. Generate 10,000 taxi events\n",
    "make generate-limited\n",
    "# Expected: ~3s, ~25,000 events/sec\n",
    "\n",
    "# 5. Process: Redpanda → Bronze → Silver\n",
    "make process\n",
    "# Expected:\n",
    "#   Bronze: 10,000 rows written to s3://warehouse/bronze/raw_trips/\n",
    "#   Silver:  9,855 rows written to s3://warehouse/silver/cleaned_trips/\n",
    "\n",
    "# 6. Run dbt: Silver → Gold (91 tests)\n",
    "make dbt-build\n",
    "# Expected: 14 models created, 91 tests PASS\n",
    "\n",
    "# 7. Verify in Flink SQL (interactive)\n",
    "docker exec -it p04-flink-jobmanager \\\n",
    "    /opt/flink/bin/sql-client.sh embedded \\\n",
    "    -i /opt/flink/sql/00-init.sql\n",
    "# Then run:\n",
    "# SELECT COUNT(*) FROM iceberg_catalog.bronze.raw_trips;   -- should be 10000\n",
    "# SELECT COUNT(*) FROM iceberg_catalog.silver.cleaned_trips; -- should be ~9855\n",
    "```\n",
    "\n",
    "### Streaming Mode (Continuous Ingestion)\n",
    "\n",
    "```bash\n",
    "# Terminal 1: Start continuous Bronze ingestion\n",
    "make process-streaming\n",
    "# This job runs until you cancel it (Ctrl+C)\n",
    "\n",
    "# Terminal 2: Generate events (will be picked up continuously)\n",
    "make generate-limited\n",
    "\n",
    "# Terminal 3: Watch Bronze grow\n",
    "watch -n5 'docker exec p04-flink-jobmanager \\\n",
    "    bash -c \"echo '\"'\"'SELECT COUNT(*) FROM iceberg_catalog.bronze.raw_trips;'\"'\"' | \\\n",
    "    MSYS_NO_PATHCONV=1 /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql\"'\n",
    "```\n",
    "\n",
    "### Web UIs (while services are running)\n",
    "\n",
    "| Service | URL | Login | What to look at |\n",
    "|---------|-----|-------|-----------------|\n",
    "| **Flink Dashboard** | http://localhost:8081 | none | Running jobs, task slots, checkpoints |\n",
    "| **Redpanda Console** | http://localhost:8085 | none | Topics, messages, consumer lag |\n",
    "| **MinIO Console** | http://localhost:9001 | minioadmin/minioadmin | Iceberg warehouse bucket, Parquet files |\n",
    "\n",
    "### Expected Benchmark Timing (10k events)\n",
    "\n",
    "| Phase | Duration | Notes |\n",
    "|-------|----------|-------|\n",
    "| `make up` + healthchecks | ~20s | Redpanda starts in ~3s; Flink needs ~15s |\n",
    "| `make create-topics` | <1s | rpk is a native binary |\n",
    "| `make generate-limited` | ~3s | ~25k events/sec to Redpanda |\n",
    "| `sleep 10` (stabilize) | 10s | Wait for consumer lag to settle |\n",
    "| `make process-bronze` | ~22s | Flink batch: Redpanda → Iceberg |\n",
    "| `make process-silver` | ~21s | Flink batch: Bronze → Silver |\n",
    "| `sleep 5` (metadata flush) | 5s | Wait for Iceberg metadata to commit |\n",
    "| `make dbt-build` | ~15s | dbt: 14 models + 91 tests |\n",
    "| **Total E2E** | **~88s** | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Production Operations + Troubleshooting\n",
    "\n",
    "### Production Hardening Checklist\n",
    "\n",
    "This pipeline implements **defense-in-depth**: multiple independent layers prevent data quality issues.\n",
    "\n",
    "| Layer | Mechanism | What it prevents |\n",
    "|-------|-----------|-----------------|\n",
    "| **Producer** | `enable.idempotence=True`, `acks=all` | Duplicate events from producer retries |\n",
    "| **DLQ** | `taxi.raw_trips.dlq` topic | Malformed events blocking the primary stream |\n",
    "| **Bronze** | Event-time watermark (10s) | Out-of-order events in streaming window functions |\n",
    "| **Silver** | `ROW_NUMBER()` deduplication | Duplicate rows surviving to analytics layer |\n",
    "| **Silver** | Quality filters (fare≥0, date range) | Invalid records reaching Gold models |\n",
    "| **dbt** | 91 schema + singular tests | Regressions detected before data reaches BI tools |\n",
    "| **Containers** | CPU limits + restart policies | Cascading failures from resource exhaustion |\n",
    "\n",
    "### Troubleshooting Guide\n",
    "\n",
    "| Symptom | Root Cause | Diagnosis | Fix |\n",
    "|---------|-----------|-----------|-----|\n",
    "| `Object 'iceberg_catalog' not found` | 00-init.sql not used as init | Check `-i` flag in command | Ensure `make process-bronze` uses `-i 00-init.sql` |\n",
    "| `Silver has 0 rows` | Bronze job failed silently | `SELECT COUNT(*) FROM bronze.raw_trips` | Check Bronze count first; rerun `make process-bronze` |\n",
    "| `LEADER_NOT_AVAILABLE` | Redpanda not fully ready | `docker logs p04-redpanda` | Wait 5-10s more; check healthcheck status |\n",
    "| `s3://warehouse: No such file` | MinIO bucket not created | `docker logs p04-mc-init` | Check mc-init completed successfully |\n",
    "| `classloader leak` warning | Missing config setting | `grep classloader flink/conf/config.yaml` | Add `classloader.check-leaked-classloader: false` |\n",
    "| `Port 19092 already in use` | Another pipeline running | `docker ps | grep 19092` | `cd ../01-kafka-flink-iceberg && make down` |\n",
    "| `dbt: No files found in COPY FROM '/old/path'` | Stale partial_parse.msgpack | `ls dbt_project/target/` | `rm dbt_project/target/partial_parse.msgpack` |\n",
    "| `dbt: source not found` | sources.yml points to wrong path | Check `external_location` in sources.yml | Must be `silver/cleaned_trips`, not `bronze/raw_trips` |\n",
    "\n",
    "### Iceberg Table Inspection\n",
    "\n",
    "```bash\n",
    "# Check what Iceberg tables exist (via MinIO)\n",
    "docker exec p04-minio mc ls myminio/warehouse/ --recursive --summarize\n",
    "\n",
    "# Query Iceberg table stats via Flink SQL\n",
    "docker exec -it p04-flink-jobmanager bash -c \"\n",
    "  echo 'SELECT COUNT(*), MIN(pickup_date), MAX(pickup_date) FROM iceberg_catalog.silver.cleaned_trips;' | \\\n",
    "  /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql\n",
    "\"\n",
    "\n",
    "# Inspect Iceberg snapshots (time travel metadata)\n",
    "docker exec -it p04-flink-jobmanager bash -c \"\n",
    "  echo 'SELECT snapshot_id, committed_at, operation, summary\n",
    "        FROM iceberg_catalog.silver.\\`cleaned_trips\\$snapshots\\`;' | \\\n",
    "  /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql\n",
    "\"\n",
    "```\n",
    "\n",
    "### Migrating to Managed Kafka/Redpanda\n",
    "\n",
    "The only file to change is `00-init.sql`:\n",
    "\n",
    "```sql\n",
    "-- Change ONLY this one line (and add auth if needed):\n",
    "'properties.bootstrap.servers' = 'broker1.company.com:9092,broker2.company.com:9092',\n",
    "\n",
    "-- For Confluent Cloud / MSK (SASL):\n",
    "'properties.security.protocol' = 'SASL_SSL',\n",
    "'properties.sasl.mechanism' = 'PLAIN',\n",
    "'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.plain.PlainLoginModule\n",
    "  required username=\"API_KEY\" password=\"API_SECRET\";'\n",
    "```\n",
    "\n",
    "Everything else — dbt models, Makefile targets, Iceberg tables — is **100% portable**.\n",
    "\n",
    "### Scheduling (Adding an Orchestrator)\n",
    "\n",
    "P04 is Makefile-based. To add scheduling (nightly runs, dependency management):\n",
    "- **P07 Kestra:** Lightest (+1 container, YAML-based, +5s overhead)\n",
    "- **P09 Dagster:** Asset-centric, lineage tracking (+750 MB)\n",
    "- **P08 Airflow:** Battle-tested, Astronomer support (+1.5 GB, +20s overhead)\n",
    "\n",
    "The Makefile targets become shell operators in the orchestrator DAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Adapting to Your Own Dataset\n",
    "\n",
    "This section is the learning guide: how to take P04's pattern and apply it to any dataset.\n",
    "The NYC Taxi pipeline is the **template** — every decision here generalizes.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Define Your Event Schema\n",
    "\n",
    "Start with the JSON events that will flow through Redpanda. Ask:\n",
    "- What does one event represent? (one taxi trip → one IoT reading, one order, one click)\n",
    "- What fields are always present? (required → filter nulls in Silver)\n",
    "- What's the natural key? (for deduplication in Silver)\n",
    "- What timestamp represents the event? (for watermarks in streaming mode)\n",
    "\n",
    "**Template for your `00-init.sql` source table:**\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS kafka_your_events (\n",
    "    -- Copy your JSON fields here with Flink SQL types:\n",
    "    -- JSON string fields → STRING\n",
    "    -- JSON integer fields → BIGINT (safest; cast to INT in Silver)\n",
    "    -- JSON float/decimal → DOUBLE (cast to DECIMAL in Silver)\n",
    "    -- JSON booleans → BOOLEAN\n",
    "    -- Timestamps (ISO 8601 strings) → STRING (parse in Bronze INSERT)\n",
    "    field_one           STRING,\n",
    "    numeric_field       BIGINT,\n",
    "    amount_field        DOUBLE,\n",
    "    event_timestamp_str STRING,   -- raw string from JSON\n",
    "\n",
    "    -- Add watermark for streaming mode:\n",
    "    event_time AS TO_TIMESTAMP(event_timestamp_str, 'yyyy-MM-dd''T''HH:mm:ss'),\n",
    "    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'your.topic.name',\n",
    "    'properties.bootstrap.servers' = 'redpanda:9092',\n",
    "    -- For batch mode:\n",
    "    'scan.startup.mode' = 'earliest-offset',\n",
    "    'scan.bounded.mode' = 'latest-offset',\n",
    "    -- For streaming mode (in 00-init-streaming.sql): omit scan.bounded.mode\n",
    "    'format' = 'json'\n",
    ");\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Design Bronze (Raw Landing)\n",
    "\n",
    "Bronze SQL changes are minimal — just match your source schema:\n",
    "\n",
    "```sql\n",
    "-- 05-bronze.sql template\n",
    "INSERT INTO iceberg_catalog.bronze.your_events\n",
    "SELECT\n",
    "    field_one,\n",
    "    numeric_field,\n",
    "    CAST(amount_field AS DOUBLE),\n",
    "    -- Parse your event timestamp:\n",
    "    TO_TIMESTAMP(event_timestamp_str, 'yyyy-MM-dd''T''HH:mm:ss') AS event_time,\n",
    "    CURRENT_TIMESTAMP AS ingestion_ts\n",
    "FROM kafka_your_events;\n",
    "```\n",
    "\n",
    "**What NOT to do in Bronze:**\n",
    "- ❌ Filter rows (except for parse errors)\n",
    "- ❌ Rename columns to snake_case (do this in Silver or dbt)\n",
    "- ❌ Compute derived metrics (duration, ratio, etc.)\n",
    "- ❌ Join with other tables\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Design Silver (Dedup + Clean)\n",
    "\n",
    "This is where you need to think carefully. Key decisions:\n",
    "\n",
    "**A) What is your natural key?** (for deduplication)\n",
    "```sql\n",
    "-- Taxi: pickup_time + dropoff_time + vendor + location + fare + total\n",
    "-- IoT: device_id + event_timestamp (usually unique already)\n",
    "-- E-commerce orders: order_id (if truly unique, skip dedup)\n",
    "-- Clickstream: session_id + event_type + timestamp (approximate dedup window)\n",
    "\n",
    "ROW_NUMBER() OVER (\n",
    "    PARTITION BY your_field1, your_field2, ...,  -- natural key\n",
    "    ORDER BY ingestion_ts DESC\n",
    ") AS rn\n",
    "```\n",
    "\n",
    "**B) What quality filters make sense?**\n",
    "```sql\n",
    "-- Taxi: fare_amount >= 0, trip_distance >= 0, location_id BETWEEN 1 AND 265\n",
    "-- IoT: sensor_value BETWEEN min_valid AND max_valid\n",
    "-- Orders: amount > 0, customer_id IS NOT NULL\n",
    "-- Always: event_timestamp IS NOT NULL\n",
    "```\n",
    "\n",
    "**C) What columns to cast?**\n",
    "```sql\n",
    "-- Cast BIGINT → INT for dimension keys (saves storage, needed for JOIN types)\n",
    "-- Cast DOUBLE → DECIMAL(10,2) for money (exact precision)\n",
    "-- Always add a surrogate key (MD5 or UUID of natural key fields)\n",
    "CAST(MD5(CONCAT_WS('|', CAST(field1 AS STRING), ...)) AS STRING) AS event_id\n",
    "```\n",
    "\n",
    "**D) How to partition Silver?**\n",
    "```sql\n",
    "-- Time-series data: PARTITIONED BY (event_date DATE)\n",
    "-- Geographic data: PARTITIONED BY (region_code) or (event_date, region_code)\n",
    "-- High-cardinality: avoid partitioning by user_id (too many small partitions)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Design dbt Staging\n",
    "\n",
    "With the Silver passthrough pattern, staging is simple:\n",
    "\n",
    "```sql\n",
    "-- models/staging/stg_your_events.sql\n",
    "with source as (\n",
    "    select * from {{ source('your_source', 'your_events') }}\n",
    "),\n",
    "final as (\n",
    "    select\n",
    "        event_id,                              -- surrogate key from Silver\n",
    "        field_one,                             -- already clean from Silver\n",
    "        numeric_field,\n",
    "        cast(amount_field as decimal(10, 2)),  -- minor DuckDB type compat\n",
    "        cast(event_time as timestamp)          -- TIMESTAMP(3) → TIMESTAMP\n",
    "    from source\n",
    "    where event_time is not null               -- safety net (Silver already filtered)\n",
    ")\n",
    "select * from final\n",
    "```\n",
    "\n",
    "**sources.yml:**\n",
    "```yaml\n",
    "sources:\n",
    "  - name: your_source\n",
    "    tables:\n",
    "      - name: your_events\n",
    "        config:\n",
    "          external_location: >-\n",
    "            iceberg_scan('s3://warehouse/silver/your_events', allow_moved_paths=true)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Design dbt Intermediate\n",
    "\n",
    "Here's where your domain knowledge goes. For each dataset, ask:\n",
    "\n",
    "1. **What are the computed metrics?** (duration, speed, ratio, percentage)\n",
    "2. **What granularities do analysts need?** (per-event, daily, hourly, per-entity)\n",
    "3. **What joins are needed?** (fact + dimensions, event + user profile, etc.)\n",
    "\n",
    "```sql\n",
    "-- models/intermediate/int_your_event_metrics.sql\n",
    "with events as (\n",
    "    select * from {{ ref('stg_your_events') }}\n",
    "),\n",
    "enriched as (\n",
    "    select\n",
    "        event_id,\n",
    "        -- Computed metrics (NOT in Flink — these belong here):\n",
    "        amount_field / duration_seconds as rate_per_second,\n",
    "        case when numeric_field > threshold then 'high' else 'low' end as category,\n",
    "        -- Calendar attributes:\n",
    "        extract(hour from event_time) as event_hour,\n",
    "        dayofweek(event_time) = 0 as is_sunday\n",
    "    from events\n",
    ")\n",
    "select * from enriched\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Design dbt Marts\n",
    "\n",
    "Marts answer specific business questions. For each question, create one mart:\n",
    "\n",
    "| Question | Mart name | Grain |\n",
    "|----------|-----------|-------|\n",
    "| \"How much revenue per day?\" | `mart_daily_revenue` | 1 row per date |\n",
    "| \"Which customers are most active?\" | `mart_customer_activity` | 1 row per customer |\n",
    "| \"What's the hourly throughput?\" | `mart_hourly_throughput` | 1 row per date × hour |\n",
    "| \"Where are errors concentrated?\" | `mart_error_locations` | 1 row per region |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Write dbt Tests\n",
    "\n",
    "For every column that downstream systems depend on, write a test:\n",
    "\n",
    "```yaml\n",
    "# staging.yml\n",
    "models:\n",
    "  - name: stg_your_events\n",
    "    columns:\n",
    "      - name: event_id\n",
    "        tests: [not_null, unique]\n",
    "      - name: amount_field\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_utils.expression_is_true:\n",
    "              expression: \"amount_field >= 0\"\n",
    "```\n",
    "\n",
    "**Custom singular tests:**\n",
    "```sql\n",
    "-- tests/assert_event_time_not_in_future.sql\n",
    "select *\n",
    "from {{ ref('stg_your_events') }}\n",
    "where event_time > current_timestamp + interval 1 hour\n",
    "-- Any rows = test failure (events shouldn't be >1h in the future)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### General Patterns That Always Apply\n",
    "\n",
    "| Pattern | What it prevents |\n",
    "|---------|-----------------|\n",
    "| Idempotent producer (`enable.idempotence=True, acks=all`) | Duplicate events from network retries |\n",
    "| DLQ topic (1 partition, longer retention) | Malformed events blocking the stream |\n",
    "| Bronze = raw landing, Silver = clean | Ability to reprocess without re-reading source |\n",
    "| ROW_NUMBER dedup on natural key | Duplicates reaching analytics layer |\n",
    "| `table.dml-sync=true` in batch mode | Silver reading empty Bronze |\n",
    "| `sleep 5` after Flink before dbt | dbt reading before Iceberg metadata commits |\n",
    "| CPU limits on TaskManager | Resource starvation in shared Docker pool |\n",
    "| All images pinned to specific tags | Pipeline breaking when `:latest` changes |\n",
    "| `classloader.check-leaked-classloader: false` | Flink + Iceberg class loading errors |\n",
    "| `s3_endpoint: \"minio:9000\"` (no http://) | DuckDB httpfs rejects protocol prefix |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. What We Learned: Key Decisions Explained\n",
    "\n",
    "This section is a retrospective of the most important decisions made while building P04,\n",
    "and the lessons they carry to any real-time pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. \"Kafka-compatible\" ≠ \"Zero code changes\"\n",
    "**What happened:** Redpanda claims 100% Kafka API compatibility. We verified this by\n",
    "running the exact same Flink SQL, the exact same dbt models, the exact same producer code —\n",
    "and it worked. The only change was one line: `bootstrap.servers = 'redpanda:9092'`.\n",
    "\n",
    "**The lesson:** Evaluate broker alternatives on *actual* protocol compatibility, not marketing.\n",
    "The Kafka wire protocol is well-specified. Redpanda, WarpStream, AutoMQ, and others implement\n",
    "it faithfully. The ecosystem (Flink, Debezium, Python confluent-kafka, Java clients) doesn't\n",
    "care which broker is on the other end.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Silver source bug: a silent failure that passes all tests\n",
    "\n",
    "**What happened:** P04's `sources.yml` originally pointed to the Bronze table. Flink Silver\n",
    "was writing clean data to one location. dbt was reading raw data from another. 91 tests passed\n",
    "because the Bronze data was *similar enough* to Silver — same row count, same column names after\n",
    "Flink Bronze's timestamp parsing.\n",
    "\n",
    "**The lesson:** Passing tests prove the tests passed — not that the pipeline is correct.\n",
    "Always verify: `SELECT COUNT(*) FROM bronze.raw_trips` vs `SELECT COUNT(*) FROM silver.cleaned_trips`.\n",
    "If they're the same (~10,000 each when you expect ~9,855 in Silver), something is wrong.\n",
    "The deduplication + quality filtering didn't run.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Flink's 06-silver.sql vs 05-run-all.sql inconsistency\n",
    "\n",
    "**What happened:** The combined pipeline (`05-run-all.sql`) had the correct ROW_NUMBER()\n",
    "deduplication. The standalone Silver file (`06-silver.sql`) ran a plain INSERT without\n",
    "dedup. Running `make process-silver` independently produced different results than `make process`.\n",
    "\n",
    "**The lesson:** When the same operation exists in multiple files (standalone + combined),\n",
    "they must be kept in sync. Add a test: run `make process-silver` and verify row count matches\n",
    "the combined pipeline's Silver output. If they differ, the files diverged.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. The benchmark race condition\n",
    "\n",
    "**What happened:** Flink's batch job returns when the INSERT statement completes. But \"INSERT\n",
    "completes\" means Flink flushed data to Parquet files in MinIO — not that Iceberg metadata\n",
    "(the `.metadata.json` snapshot) was committed and visible to readers. dbt would start and\n",
    "sometimes see 0 rows because Iceberg's metadata wasn't ready.\n",
    "\n",
    "**The lesson:** Flink's `table.dml-sync=true` waits for the job to finish writing *data*.\n",
    "Iceberg commits metadata separately in a final atomic rename. Add `sleep 5` between Flink\n",
    "and dbt in any batch pipeline. In streaming mode with proper checkpoints, this race doesn't\n",
    "exist because DuckDB always reads the latest committed snapshot.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Enrichment columns belong in dbt, not Flink\n",
    "\n",
    "**What happened:** An early version of Silver computed `duration_minutes`, `avg_speed_mph`,\n",
    "`cost_per_mile`, `tip_percentage`, `pickup_hour`, and `is_weekend` directly in the Flink\n",
    "Silver SQL. These were removed and moved to `int_trip_metrics.sql`.\n",
    "\n",
    "**The lesson:** Apply the single-responsibility principle to data layers. Flink is uniquely\n",
    "positioned to do: ordering, deduplication, type coercion, and quality filtering at stream\n",
    "speed. It's not the right place for business metrics because:\n",
    "1. Business rules change. Changing Flink SQL requires reprocessing the entire Bronze table.\n",
    "2. Business metrics are domain knowledge — analysts understand them, not infrastructure engineers.\n",
    "3. dbt tests can verify business rule correctness. Flink SQL can't.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. CPU limits prevent silent failures in Docker Desktop\n",
    "\n",
    "**What happened:** Without CPU limits, during Flink processing the TaskManager consumed\n",
    "all available cores on Docker Desktop. MinIO's S3 server starved for CPU → S3A write requests\n",
    "timed out → Flink retried → more load → eventual failure. The error message looked like\n",
    "a network error, not a resource problem.\n",
    "\n",
    "**The lesson:** Always set both `memory` AND `cpus` limits on CPU-intensive containers\n",
    "in Docker Desktop environments. In production Kubernetes, use resource requests/limits on\n",
    "every pod. Docker Desktop doesn't enforce CPU limits by default — your JVM containers will\n",
    "happily consume 100% of all available cores.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. The DLQ must exist in the Makefile, not just the shell script\n",
    "\n",
    "**What happened:** The `create-topics.sh` shell script created both topics correctly.\n",
    "But the `Makefile`'s `create-topics` target ran `rpk topic create taxi.raw_trips` inline,\n",
    "bypassing the shell script. Running `make create-topics` only created one topic.\n",
    "\n",
    "**The lesson:** When you have multiple entry points to the same operation (shell script + Makefile),\n",
    "test them independently. The Makefile target is what everyone uses in practice (`make create-topics`\n",
    "is more natural than `bash kafka/create-topics.sh`). Keep them synchronized or have one call the other.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Streaming mode's `table.dml-sync` trap\n",
    "\n",
    "**What happened:** Developers familiar with batch mode copied `SET 'table.dml-sync' = 'true'`\n",
    "into the streaming init file. The Flink session blocked forever on the first INSERT (which\n",
    "runs indefinitely in streaming mode), never getting to execute the second statement.\n",
    "\n",
    "**The lesson:** Batch and streaming modes have fundamentally different lifecycles. `table.dml-sync`\n",
    "is the batch synchronization primitive that makes no sense for infinite streaming jobs. Never\n",
    "copy settings between batch and streaming init files without understanding what each setting does.\n",
    "A streaming pipeline that \"works\" with `dml-sync=true` is actually running only one job —\n",
    "it never progressed past the first INSERT.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: The Mental Model\n",
    "\n",
    "```\n",
    "Everything a streaming data pipeline does falls into one of three buckets:\n",
    "\n",
    "1. INFRASTRUCTURE WORK (Flink):\n",
    "   Parse timestamps, enforce types, deduplicate on the natural key,\n",
    "   apply basic validity filters, write to the storage format.\n",
    "   This runs at stream speed and shouldn't be changed often.\n",
    "\n",
    "2. BUSINESS LOGIC (dbt):\n",
    "   Compute metrics, join dimensions, aggregate to business granularities,\n",
    "   test business rules, version-control analytical decisions.\n",
    "   This changes with business requirements and should be easy to modify.\n",
    "\n",
    "3. OPERATIONS (Makefile + Docker Compose):\n",
    "   Manage infrastructure lifecycle, health checks, topic creation,\n",
    "   DLQ, resource limits, benchmark timing, image pinning.\n",
    "   This is the glue that makes the pipeline reproducible and observable.\n",
    "\n",
    "When something is in the wrong bucket, you get problems:\n",
    "- Business logic in Flink → hard to change, hard to test, requires stream reprocessing\n",
    "- Infrastructure work in dbt → slow (batch query instead of stream), misses the point\n",
    "- Missing operations (no DLQ, no CPU limits, no sleep) → silent failures that pass tests\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}