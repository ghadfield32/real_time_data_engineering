cd astro-project && astro dev stop
docker compose --profile generator --profile dbt down -v
Pipeline 08 stopped and volumes removed.
docker compose up -d
 Network p08-pipeline-net  Creating
 Network p08-pipeline-net  Created
 Volume 08-airflow-orchestrated_minio-data  Creating
 Volume 08-airflow-orchestrated_minio-data  Created
 Volume 08-airflow-orchestrated_flink-checkpoints  Creating
 Volume 08-airflow-orchestrated_flink-checkpoints  Created
 Container p08-minio  Creating
 Container p08-kafka  Creating
 Container p08-minio  Created
 Container p08-mc-init  Creating
 Container p08-kafka  Created
 Container p08-mc-init  Created
 Container p08-flink-jobmanager  Creating
 Container p08-flink-jobmanager  Created
 Container p08-flink-taskmanager  Creating
 Container p08-flink-taskmanager  Created
 Container p08-minio  Starting
 Container p08-kafka  Starting
 Container p08-minio  Started
 Container p08-minio  Waiting
 Container p08-kafka  Started
 Container p08-minio  Healthy
 Container p08-mc-init  Starting
 Container p08-mc-init  Started
 Container p08-kafka  Waiting
 Container p08-mc-init  Waiting
 Container p08-mc-init  Exited
 Container p08-kafka  Healthy
 Container p08-flink-jobmanager  Starting
 Container p08-flink-jobmanager  Started
 Container p08-flink-jobmanager  Waiting
 Container p08-flink-jobmanager  Healthy
 Container p08-flink-taskmanager  Starting
 Container p08-flink-taskmanager  Started
cd astro-project && astro dev start
the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'
Error: error adding connections: error listing connections: error encountered while running the airflow command: exit status 1
make: *** [Makefile:25: up] Error 1
cd astro-project && astro dev stop
docker compose --profile generator --profile dbt down -v
 Container p08-flink-taskmanager  Stopping
 Container p08-flink-taskmanager  Stopped
 Container p08-flink-taskmanager  Removing
 Container p08-flink-taskmanager  Removed
 Container p08-flink-jobmanager  Stopping
 Container p08-flink-jobmanager  Stopped
 Container p08-flink-jobmanager  Removing
 Container p08-flink-jobmanager  Removed
 Container p08-kafka  Stopping
 Container p08-mc-init  Stopping
 Container p08-mc-init  Stopped
 Container p08-mc-init  Removing
 Container p08-mc-init  Removed
 Container p08-minio  Stopping
 Container p08-minio  Stopped
 Container p08-minio  Removing
 Container p08-minio  Removed
 Container p08-kafka  Stopped
 Container p08-kafka  Removing
 Container p08-kafka  Removed
 Volume 08-airflow-orchestrated_flink-checkpoints  Removing
 Volume 08-airflow-orchestrated_minio-data  Removing
 Network p08-pipeline-net  Removing
 Volume 08-airflow-orchestrated_flink-checkpoints  Removed
 Volume 08-airflow-orchestrated_minio-data  Removed
 Network p08-pipeline-net  Removed
Pipeline 08 stopped and volumes removed.
cd astro-project && astro dev stop || true
docker compose --profile generator --profile dbt down -v --remove-orphans
docker network rm p08-pipeline-net 2>/dev/null || true
Pipeline 08 fully cleaned.
cd astro-project && astro dev stop
docker compose --profile generator --profile dbt down -v
Pipeline 08 stopped and volumes removed.
docker compose up -d
 Network p08-pipeline-net  Creating
 Network p08-pipeline-net  Created
 Volume 08-airflow-orchestrated_flink-checkpoints  Creating
 Volume 08-airflow-orchestrated_flink-checkpoints  Created
 Volume 08-airflow-orchestrated_minio-data  Creating
 Volume 08-airflow-orchestrated_minio-data  Created
 Container p08-kafka  Creating
 Container p08-minio  Creating
 Container p08-kafka  Created
 Container p08-minio  Created
 Container p08-mc-init  Creating
 Container p08-mc-init  Created
 Container p08-flink-jobmanager  Creating
 Container p08-flink-jobmanager  Created
 Container p08-flink-taskmanager  Creating
 Container p08-flink-taskmanager  Created
 Container p08-minio  Starting
 Container p08-kafka  Starting
 Container p08-minio  Started
 Container p08-minio  Waiting
 Container p08-kafka  Started
 Container p08-minio  Healthy
 Container p08-mc-init  Starting
 Container p08-mc-init  Started
 Container p08-kafka  Waiting
 Container p08-mc-init  Waiting
 Container p08-mc-init  Exited
 Container p08-kafka  Healthy
 Container p08-flink-jobmanager  Starting
 Container p08-flink-jobmanager  Started
 Container p08-flink-jobmanager  Waiting
 Container p08-flink-jobmanager  Healthy
 Container p08-flink-taskmanager  Starting
 Container p08-flink-taskmanager  Started
cd astro-project && astro dev start || echo "[WARN] Airflow/Astronomer requires an interactive TTY â€” skipping. Core Kafka+Flink+dbt pipeline will proceed. Run 'cd astro-project && astro dev start' manually for the Airflow UI."
the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'
Error: error adding connections: error listing connections: error encountered while running the airflow command: exit status 1
[WARN] Airflow/Astronomer requires an interactive TTY Ã¢â‚¬â€ skipping. Core Kafka+Flink+dbt pipeline will proceed. Run 'cd astro-project && astro dev start' manually for the Airflow UI.

=== Pipeline 08: Airflow Orchestrated ===
Kafka:            localhost:9092
Schema Registry:  http://localhost:8085
Flink Dashboard:  http://localhost:8081
MinIO Console:    http://localhost:9001  (minioadmin/minioadmin)
Airflow UI:       http://localhost:8080  (admin/admin)

Next steps:
  make create-topics   # Create Kafka topics
  make generate        # Produce taxi events to Kafka
  make process         # Submit Flink SQL jobs
  make dbt-build       # Run dbt transformations
  make airflow-ui      # Open Airflow UI
============================================================
  Pipeline 08 Benchmark: Airflow Orchestrated
============================================================
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
docker compose up -d
 Container p08-kafka  Running
 Container p08-minio  Running
 Container p08-flink-jobmanager  Running
 Container p08-flink-taskmanager  Running
 Container p08-minio  Waiting
 Container p08-minio  Healthy
 Container p08-mc-init  Starting
 Container p08-mc-init  Started
 Container p08-kafka  Waiting
 Container p08-mc-init  Waiting
 Container p08-mc-init  Exited
 Container p08-kafka  Healthy
 Container p08-flink-jobmanager  Waiting
 Container p08-flink-jobmanager  Healthy
cd astro-project && astro dev start || echo "[WARN] Airflow/Astronomer requires an interactive TTY â€” skipping. Core Kafka+Flink+dbt pipeline will proceed. Run 'cd astro-project && astro dev start' manually for the Airflow UI."
the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'
Error: error adding connections: error listing connections: error encountered while running the airflow command: exit status 1
[WARN] Airflow/Astronomer requires an interactive TTY Ã¢â‚¬â€ skipping. Core Kafka+Flink+dbt pipeline will proceed. Run 'cd astro-project && astro dev start' manually for the Airflow UI.

=== Pipeline 08: Airflow Orchestrated ===
Kafka:            localhost:9092
Schema Registry:  http://localhost:8085
Flink Dashboard:  http://localhost:8081
MinIO Console:    http://localhost:9001  (minioadmin/minioadmin)
Airflow UI:       http://localhost:8080  (admin/admin)

Next steps:
  make create-topics   # Create Kafka topics
  make generate        # Produce taxi events to Kafka
  make process         # Submit Flink SQL jobs
  make dbt-build       # Run dbt transformations
  make airflow-ui      # Open Airflow UI
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
Waiting for services to stabilize...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create \
	--topic taxi.raw_trips \
	--partitions 3 \
	--replication-factor 1 \
	--if-not-exists
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created (3 partitions).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
MSYS_NO_PATHCONV=1 docker compose run --rm -e MAX_EVENTS=10000 data-generator
 Container p08-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.91s
  Rate:    10,962 events/sec
============================================================
  Metrics written to /tmp/generator_metrics.json
Limited data generation complete (10k events).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
Waiting for Flink processing to catch up...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
=== Bronze: Kafka Ã¢â€ â€™ Iceberg ===
MSYS_NO_PATHCONV=1 docker compose exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/05-bronze.sql
2026-02-22 12:19:51,034 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-02-22 12:19:51,072 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).
2026-02-22 12:19:51,072 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started
Successfully initialized from sql script: file:/opt/flink/sql/00-init.sql
Feb 22, 2026 12:19:51 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
[34;1m[INFO] Executing SQL from file.[0m

Command history file path: /root/.flink-sql-history
Flink SQL> -- =============================================================================
> -- Pipeline 01: Bronze Layer (Kafka â†’ Iceberg)[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> CREATE TABLE IF NOT EXISTS bronze.raw_trips (
>     VendorID                BIGINT,
>     tpep_pickup_datetime    TIMESTAMP(3),
>     tpep_dropoff_datetime   TIMESTAMP(3),
>     passenger_count         BIGINT,
>     trip_distance           DOUBLE,
>     RatecodeID              BIGINT,
>     store_and_fwd_flag      STRING,
>     PULocationID            BIGINT,
>     DOLocationID            BIGINT,
>     payment_type            BIGINT,
>     fare_amount             DOUBLE,
>     extra                   DOUBLE,
>     mta_tax                 DOUBLE,
>     tip_amount              DOUBLE,
>     tolls_amount            DOUBLE,
>     improvement_surcharge   DOUBLE,
>     total_amount            DOUBLE,
>     congestion_surcharge    DOUBLE,
>     Airport_fee             DOUBLE,
>     ingestion_ts            TIMESTAMP(3)
> )
> WITH (
>     'format-version' = '1',
>     'write.format.default' = 'parquet',
>     'write.parquet.compression-codec' = 'zstd',
>     'write.metadata.delete-after-commit.enabled' = 'true',
>     'write.metadata.previous-versions-max' = '10',
>     'write.target-file-size-bytes' = '134217728'
> )[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> INSERT INTO iceberg_catalog.bronze.raw_trips
> SELECT
>     VendorID,
>     TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,
>     TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')[34;1m[INFO] Complete execution of the SQL update statement.[0m

Flink SQL> 
Shutting down the session...
done.
Bronze layer complete.
=== Silver: Bronze Ã¢â€ â€™ Cleaned Iceberg ===
MSYS_NO_PATHCONV=1 docker compose exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/06-silver.sql
2026-02-22 12:20:03,197 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-02-22 12:20:03,238 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).
2026-02-22 12:20:03,238 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started
Successfully initialized from sql script: file:/opt/flink/sql/00-init.sql
Feb 22, 2026 12:20:03 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
[34;1m[INFO] Executing SQL from file.[0m

Command history file path: /root/.flink-sql-history
Flink SQL> -- =============================================================================
> -- Pipeline 01: Silver Layer (Bronze Iceberg â†’ Silver Iceberg)[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> CREATE TABLE IF NOT EXISTS silver.cleaned_trips (
>     trip_id                 STRING,
>     vendor_id               INT,
>     rate_code_id            INT,
>     pickup_location_id      INT,
>     dropoff_location_id     INT,
>     payment_type_id         INT,
>     pickup_datetime         TIMESTAMP(3),
>     dropoff_datetime        TIMESTAMP(3),
>     passenger_count         INT,
>     trip_distance_miles     DOUBLE,
>     store_and_fwd_flag      STRING,
>     fare_amount             DECIMAL(10, 2),
>     extra_amount            DECIMAL(10, 2),
>     mta_tax                 DECIMAL(10, 2),
>     tip_amount              DECIMAL(10, 2),
>     tolls_amount            DECIMAL(10, 2),
>     improvement_surcharge   DECIMAL(10, 2),
>     total_amount            DECIMAL(10, 2),
>     congestion_surcharge    DECIMAL(10, 2),
>     airport_fee             DECIMAL(10, 2),
>     pickup_date             DATE
> ) PARTITIONED BY (pickup_date)
> WITH (
>     'format-version' = '2',
>     'write.format.default' = 'parquet',
>     'write.parquet.compression-codec' = 'zstd',
>     'write.metadata.delete-after-commit.enabled' = 'true',
>     'write.metadata.previous-versions-max' = '10',
>     'write.target-file-size-bytes' = '134217728'
> )[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> -- Deduplication: ROW_NUMBER partitioned by natural key, keeping latest ingestion
> INSERT INTO iceberg_catalog.silver.cleaned_trips
> WITH deduped AS (
>     SELECT *,
>         ROW_NUMBER() OVER (
>             PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,
>                          PULocationID, DOLocationID, fare_amount, total_amount
>             ORDER BY ingestion_ts DESC
>         ) AS rn
>     FROM iceberg_catalog.bronze.raw_trips
>     WHERE tpep_pickup_datetime IS NOT NULL
>       AND tpep_dropoff_datetime IS NOT NULL
>       AND trip_distance >= 0
>       AND fare_amount >= 0
>       AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'
>       AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01'
> )
> SELECT
>     CAST(MD5(CONCAT_WS('|',
>         CAST(VendorID AS STRING),
>         CAST(tpep_pickup_datetime AS STRING),
>         CAST(tpep_dropoff_datetime AS STRING),
>         CAST(PULocationID AS STRING),
>         CAST(DOLocationID AS STRING),
>         CAST(fare_amount AS STRING),
>         CAST(total_amount AS STRING)
>     )) AS STRING) AS trip_id,
>     CAST(VendorID AS INT)       AS vendor_id,
>     CAST(RatecodeID AS INT)     AS rate_code_id,
>     CAST(PULocationID AS INT)   AS pickup_location_id,
>     CAST(DOLocationID AS INT)   AS dropoff_location_id,
>     CAST(payment_type AS INT)   AS payment_type_id,
>     tpep_pickup_datetime        AS pickup_datetime,
>     tpep_dropoff_datetime       AS dropoff_datetime,
>     CAST(passenger_count AS INT) AS passenger_count,
>     trip_distance               AS trip_distance_miles,
>     store_and_fwd_flag,
>     CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,
>     CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,
>     CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,
>     CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,
>     CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,
>     CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,
>     CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,
>     CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,
>     CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,
>     CAST(tpep_pickup_datetime AS DATE)[34;1m[INFO] Complete execution of the SQL update statement.[0m

Flink SQL> 
Shutting down the session...
done.
2026-02-22 12:20:09,065 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Stopping s3a-file-system metrics system...
Silver layer complete.
All Flink SQL jobs submitted.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir ."
 Container p08-minio  Running
[0m12:20:12  Running with dbt=1.11.5
[0m12:20:12  Installing dbt-labs/dbt_utils
[0m12:20:16  Installed from version 1.3.3
[0m12:20:16  Up to date!
[0m12:20:17  Running with dbt=1.11.5
[0m12:20:17  Registered adapter: duckdb=1.10.0
[0m12:20:18  Found 14 models, 74 data tests, 3 seeds, 1 source, 606 macros
[0m12:20:18  
[0m12:20:18  Concurrency: 4 threads (target='dev')
[0m12:20:18  
[0m12:20:23  1 of 91 START sql table model main_marts.dim_dates ............................. [RUN]
[0m12:20:23  2 of 91 START sql table model main_staging.stg_yellow_trips .................... [RUN]
[0m12:20:23  3 of 91 START seed file main_raw.payment_type_lookup ........................... [RUN]
[0m12:20:23  4 of 91 START seed file main_raw.rate_code_lookup .............................. [RUN]
[0m12:20:23  3 of 91 OK loaded seed file main_raw.payment_type_lookup ....................... [[32mCREATE 6[0m in 0.14s]
[0m12:20:23  5 of 91 START seed file main_raw.taxi_zone_lookup .............................. [RUN]
[0m12:20:23  4 of 91 OK loaded seed file main_raw.rate_code_lookup .......................... [[32mCREATE 7[0m in 0.14s]
[0m12:20:23  6 of 91 START test not_null_payment_type_lookup_payment_type_id ................ [RUN]
[0m12:20:23  2 of 91 OK created sql table model main_staging.stg_yellow_trips ............... [[32mOK[0m in 0.18s]
[0m12:20:23  7 of 91 START test unique_payment_type_lookup_payment_type_id .................. [RUN]
[0m12:20:23  1 of 91 OK created sql table model main_marts.dim_dates ........................ [[32mOK[0m in 0.20s]
[0m12:20:23  8 of 91 START test not_null_rate_code_lookup_rate_code_id ...................... [RUN]
[0m12:20:23  6 of 91 PASS not_null_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.07s]
[0m12:20:23  9 of 91 START test unique_rate_code_lookup_rate_code_id ........................ [RUN]
[0m12:20:23  5 of 91 OK loaded seed file main_raw.taxi_zone_lookup .......................... [[32mCREATE 265[0m in 0.08s]
[0m12:20:23  10 of 91 START test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [RUN]
[0m12:20:23  7 of 91 PASS unique_payment_type_lookup_payment_type_id ........................ [[32mPASS[0m in 0.06s]
[0m12:20:23  8 of 91 PASS not_null_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.05s]
[0m12:20:23  11 of 91 START test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [RUN]
[0m12:20:23  12 of 91 START test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ........ [RUN]
[0m12:20:23  9 of 91 PASS unique_rate_code_lookup_rate_code_id .............................. [[32mPASS[0m in 0.05s]
[0m12:20:23  13 of 91 START test assert_fare_not_exceeds_total .............................. [RUN]
[0m12:20:23  10 of 91 PASS accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[32mPASS[0m in 0.05s]
[0m12:20:23  14 of 91 START test not_null_stg_yellow_trips_dropoff_datetime ................. [RUN]
[0m12:20:23  11 of 91 PASS accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[32mPASS[0m in 0.05s]
[0m12:20:23  12 of 91 PASS accepted_values_stg_yellow_trips_vendor_id__1__2__6 .............. [[32mPASS[0m in 0.05s]
[0m12:20:23  15 of 91 START test not_null_stg_yellow_trips_dropoff_location_id .............. [RUN]
[0m12:20:23  16 of 91 START test not_null_stg_yellow_trips_fare_amount ...................... [RUN]
[0m12:20:23  13 of 91 PASS assert_fare_not_exceeds_total .................................... [[32mPASS[0m in 0.06s]
[0m12:20:23  17 of 91 START test not_null_stg_yellow_trips_pickup_datetime .................. [RUN]
[0m12:20:23  14 of 91 PASS not_null_stg_yellow_trips_dropoff_datetime ....................... [[32mPASS[0m in 0.06s]
[0m12:20:23  18 of 91 START test not_null_stg_yellow_trips_pickup_location_id ............... [RUN]
[0m12:20:23  16 of 91 PASS not_null_stg_yellow_trips_fare_amount ............................ [[32mPASS[0m in 0.05s]
[0m12:20:23  19 of 91 START test not_null_stg_yellow_trips_total_amount ..................... [RUN]
[0m12:20:23  15 of 91 PASS not_null_stg_yellow_trips_dropoff_location_id .................... [[32mPASS[0m in 0.06s]
[0m12:20:23  17 of 91 PASS not_null_stg_yellow_trips_pickup_datetime ........................ [[32mPASS[0m in 0.05s]
[0m12:20:23  20 of 91 START test not_null_stg_yellow_trips_trip_distance_miles .............. [RUN]
[0m12:20:23  21 of 91 START test not_null_stg_yellow_trips_trip_id .......................... [RUN]
[0m12:20:23  18 of 91 PASS not_null_stg_yellow_trips_pickup_location_id ..................... [[32mPASS[0m in 0.05s]
[0m12:20:23  22 of 91 START test not_null_stg_yellow_trips_vendor_id ........................ [RUN]
[0m12:20:23  20 of 91 PASS not_null_stg_yellow_trips_trip_distance_miles .................... [[32mPASS[0m in 0.04s]
[0m12:20:23  21 of 91 PASS not_null_stg_yellow_trips_trip_id ................................ [[32mPASS[0m in 0.04s]
[0m12:20:23  23 of 91 START test unique_stg_yellow_trips_trip_id ............................ [RUN]
[0m12:20:23  19 of 91 PASS not_null_stg_yellow_trips_total_amount ........................... [[32mPASS[0m in 0.06s]
[0m12:20:23  24 of 91 START test not_null_dim_dates_date_key ................................ [RUN]
[0m12:20:23  25 of 91 START test not_null_dim_dates_day_of_week_name ........................ [RUN]
[0m12:20:23  22 of 91 PASS not_null_stg_yellow_trips_vendor_id .............................. [[32mPASS[0m in 0.04s]
[0m12:20:23  26 of 91 START test not_null_dim_dates_is_holiday .............................. [RUN]
[0m12:20:23  23 of 91 PASS unique_stg_yellow_trips_trip_id .................................. [[32mPASS[0m in 0.05s]
[0m12:20:23  27 of 91 START test not_null_dim_dates_is_weekend .............................. [RUN]
[0m12:20:23  25 of 91 PASS not_null_dim_dates_day_of_week_name .............................. [[32mPASS[0m in 0.05s]
[0m12:20:23  28 of 91 START test unique_dim_dates_date_key .................................. [RUN]
[0m12:20:23  24 of 91 PASS not_null_dim_dates_date_key ...................................... [[32mPASS[0m in 0.05s]
[0m12:20:23  29 of 91 START test not_null_taxi_zone_lookup_Borough .......................... [RUN]
[0m12:20:23  26 of 91 PASS not_null_dim_dates_is_holiday .................................... [[32mPASS[0m in 0.06s]
[0m12:20:23  30 of 91 START test not_null_taxi_zone_lookup_LocationID ....................... [RUN]
[0m12:20:23  27 of 91 PASS not_null_dim_dates_is_weekend .................................... [[32mPASS[0m in 0.05s]
[0m12:20:23  31 of 91 START test unique_taxi_zone_lookup_LocationID ......................... [RUN]
[0m12:20:23  28 of 91 PASS unique_dim_dates_date_key ........................................ [[32mPASS[0m in 0.04s]
[0m12:20:23  32 of 91 START sql view model main_staging.stg_payment_types ................... [RUN]
[0m12:20:23  29 of 91 PASS not_null_taxi_zone_lookup_Borough ................................ [[32mPASS[0m in 0.05s]
[0m12:20:23  33 of 91 START sql view model main_staging.stg_rate_codes ...................... [RUN]
[0m12:20:23  30 of 91 PASS not_null_taxi_zone_lookup_LocationID ............................. [[32mPASS[0m in 0.04s]
[0m12:20:23  34 of 91 START sql view model main_intermediate.int_trip_metrics ............... [RUN]
[0m12:20:24  31 of 91 PASS unique_taxi_zone_lookup_LocationID ............................... [[32mPASS[0m in 0.06s]
[0m12:20:24  35 of 91 START sql view model main_staging.stg_taxi_zones ...................... [RUN]
[0m12:20:24  32 of 91 OK created sql view model main_staging.stg_payment_types .............. [[32mOK[0m in 0.07s]
[0m12:20:24  36 of 91 START test not_null_stg_payment_types_payment_type_id ................. [RUN]
[0m12:20:24  33 of 91 OK created sql view model main_staging.stg_rate_codes ................. [[32mOK[0m in 0.08s]
[0m12:20:24  37 of 91 START test not_null_stg_payment_types_payment_type_name ............... [RUN]
[0m12:20:24  34 of 91 OK created sql view model main_intermediate.int_trip_metrics .......... [[32mOK[0m in 0.08s]
[0m12:20:24  38 of 91 START test unique_stg_payment_types_payment_type_id ................... [RUN]
[0m12:20:24  37 of 91 PASS not_null_stg_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.04s]
[0m12:20:24  36 of 91 PASS not_null_stg_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m12:20:24  35 of 91 OK created sql view model main_staging.stg_taxi_zones ................. [[32mOK[0m in 0.08s]
[0m12:20:24  39 of 91 START test not_null_stg_rate_codes_rate_code_id ....................... [RUN]
[0m12:20:24  40 of 91 START test not_null_stg_rate_codes_rate_code_name ..................... [RUN]
[0m12:20:24  41 of 91 START test unique_stg_rate_codes_rate_code_id ......................... [RUN]
[0m12:20:24  38 of 91 PASS unique_stg_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m12:20:24  42 of 91 START test assert_trip_duration_positive .............................. [RUN]
[0m12:20:24  41 of 91 PASS unique_stg_rate_codes_rate_code_id ............................... [[32mPASS[0m in 0.05s]
[0m12:20:24  40 of 91 PASS not_null_stg_rate_codes_rate_code_name ........................... [[32mPASS[0m in 0.05s]
[0m12:20:24  43 of 91 START test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [RUN]
[0m12:20:24  44 of 91 START test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [RUN]
[0m12:20:24  39 of 91 PASS not_null_stg_rate_codes_rate_code_id ............................. [[32mPASS[0m in 0.07s]
[0m12:20:24  45 of 91 START test not_null_int_trip_metrics_is_weekend ....................... [RUN]
[0m12:20:24  42 of 91 PASS assert_trip_duration_positive .................................... [[32mPASS[0m in 0.06s]
[0m12:20:24  46 of 91 START test not_null_int_trip_metrics_pickup_date ...................... [RUN]
[0m12:20:24  45 of 91 PASS not_null_int_trip_metrics_is_weekend ............................. [[32mPASS[0m in 0.10s]
[0m12:20:24  47 of 91 START test not_null_int_trip_metrics_pickup_hour ...................... [RUN]
[0m12:20:24  44 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[32mPASS[0m in 0.11s]
[0m12:20:24  43 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0 ..... [[32mPASS[0m in 0.12s]
[0m12:20:24  48 of 91 START test not_null_int_trip_metrics_trip_duration_minutes ............ [RUN]
[0m12:20:24  49 of 91 START test not_null_int_trip_metrics_trip_id .......................... [RUN]
[0m12:20:24  46 of 91 PASS not_null_int_trip_metrics_pickup_date ............................ [[32mPASS[0m in 0.12s]
[0m12:20:24  50 of 91 START test not_null_stg_taxi_zones_borough ............................ [RUN]
[0m12:20:24  47 of 91 PASS not_null_int_trip_metrics_pickup_hour ............................ [[32mPASS[0m in 0.05s]
[0m12:20:24  51 of 91 START test not_null_stg_taxi_zones_location_id ........................ [RUN]
[0m12:20:24  49 of 91 PASS not_null_int_trip_metrics_trip_id ................................ [[32mPASS[0m in 0.05s]
[0m12:20:24  48 of 91 PASS not_null_int_trip_metrics_trip_duration_minutes .................. [[32mPASS[0m in 0.06s]
[0m12:20:24  52 of 91 START test not_null_stg_taxi_zones_zone_name .......................... [RUN]
[0m12:20:24  53 of 91 START test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:20:24  50 of 91 PASS not_null_stg_taxi_zones_borough .................................. [[32mPASS[0m in 0.06s]
[0m12:20:24  54 of 91 START test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:20:24  51 of 91 PASS not_null_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.06s]
[0m12:20:24  55 of 91 START test unique_stg_taxi_zones_location_id .......................... [RUN]
[0m12:20:24  52 of 91 PASS not_null_stg_taxi_zones_zone_name ................................ [[32mPASS[0m in 0.06s]
[0m12:20:24  56 of 91 START sql table model main_marts.dim_payment_types .................... [RUN]
[0m12:20:24  53 of 91 PASS relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.06s]
[0m12:20:24  54 of 91 PASS relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.04s]
[0m12:20:24  57 of 91 START sql view model main_intermediate.int_daily_summary .............. [RUN]
[0m12:20:24  58 of 91 START sql view model main_intermediate.int_hourly_patterns ............ [RUN]
[0m12:20:24  55 of 91 PASS unique_stg_taxi_zones_location_id ................................ [[32mPASS[0m in 0.06s]
[0m12:20:24  59 of 91 START sql table model main_marts.dim_locations ........................ [RUN]
[0m12:20:24  58 of 91 OK created sql view model main_intermediate.int_hourly_patterns ....... [[32mOK[0m in 0.07s]
[0m12:20:24  57 of 91 OK created sql view model main_intermediate.int_daily_summary ......... [[32mOK[0m in 0.07s]
[0m12:20:24  60 of 91 START test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [RUN]
[0m12:20:24  61 of 91 START test not_null_int_hourly_patterns_pickup_date ................... [RUN]
[0m12:20:24  56 of 91 OK created sql table model main_marts.dim_payment_types ............... [[32mOK[0m in 0.10s]
[0m12:20:24  62 of 91 START test not_null_int_hourly_patterns_pickup_hour ................... [RUN]
[0m12:20:24  59 of 91 OK created sql table model main_marts.dim_locations ................... [[32mOK[0m in 0.08s]
[0m12:20:24  63 of 91 START test not_null_int_hourly_patterns_total_trips ................... [RUN]
[0m12:20:24  61 of 91 PASS not_null_int_hourly_patterns_pickup_date ......................... [[32mPASS[0m in 0.05s]
[0m12:20:24  60 of 91 PASS dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0 .. [[32mPASS[0m in 0.06s]
[0m12:20:24  64 of 91 START test dbt_utils_accepted_range_int_daily_summary_total_trips__0 .. [RUN]
[0m12:20:24  65 of 91 START test not_null_int_daily_summary_pickup_date ..................... [RUN]
[0m12:20:24  62 of 91 PASS not_null_int_hourly_patterns_pickup_hour ......................... [[32mPASS[0m in 0.06s]
[0m12:20:24  66 of 91 START test not_null_int_daily_summary_total_revenue ................... [RUN]
[0m12:20:24  63 of 91 PASS not_null_int_hourly_patterns_total_trips ......................... [[32mPASS[0m in 0.05s]
[0m12:20:24  67 of 91 START test not_null_int_daily_summary_total_trips ..................... [RUN]
[0m12:20:24  65 of 91 PASS not_null_int_daily_summary_pickup_date ........................... [[32mPASS[0m in 0.05s]
[0m12:20:24  68 of 91 START test unique_int_daily_summary_pickup_date ....................... [RUN]
[0m12:20:24  64 of 91 PASS dbt_utils_accepted_range_int_daily_summary_total_trips__0 ........ [[32mPASS[0m in 0.05s]
[0m12:20:24  69 of 91 START test not_null_dim_payment_types_payment_type_id ................. [RUN]
[0m12:20:24  66 of 91 PASS not_null_int_daily_summary_total_revenue ......................... [[32mPASS[0m in 0.05s]
[0m12:20:24  70 of 91 START test not_null_dim_payment_types_payment_type_name ............... [RUN]
[0m12:20:24  67 of 91 PASS not_null_int_daily_summary_total_trips ........................... [[32mPASS[0m in 0.06s]
[0m12:20:24  71 of 91 START test unique_dim_payment_types_payment_type_id ................... [RUN]
[0m12:20:24  70 of 91 PASS not_null_dim_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.04s]
[0m12:20:24  68 of 91 PASS unique_int_daily_summary_pickup_date ............................. [[32mPASS[0m in 0.06s]
[0m12:20:24  72 of 91 START test not_null_dim_locations_borough ............................. [RUN]
[0m12:20:24  69 of 91 PASS not_null_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.05s]
[0m12:20:24  73 of 91 START test not_null_dim_locations_location_id ......................... [RUN]
[0m12:20:24  74 of 91 START test not_null_dim_locations_zone_name ........................... [RUN]
[0m12:20:24  71 of 91 PASS unique_dim_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m12:20:24  72 of 91 PASS not_null_dim_locations_borough ................................... [[32mPASS[0m in 0.04s]
[0m12:20:24  75 of 91 START test unique_dim_locations_location_id ........................... [RUN]
[0m12:20:24  76 of 91 START sql table model main_marts.mart_hourly_demand ................... [RUN]
[0m12:20:24  73 of 91 PASS not_null_dim_locations_location_id ............................... [[32mPASS[0m in 0.05s]
[0m12:20:24  74 of 91 PASS not_null_dim_locations_zone_name ................................. [[32mPASS[0m in 0.05s]
[0m12:20:24  77 of 91 START sql table model main_marts.mart_daily_revenue ................... [RUN]
[0m12:20:24  75 of 91 PASS unique_dim_locations_location_id ................................. [[32mPASS[0m in 0.05s]
[0m12:20:24  78 of 91 START sql incremental model main_marts.fct_trips ...................... [RUN]
[0m12:20:24  76 of 91 OK created sql table model main_marts.mart_hourly_demand .............. [[32mOK[0m in 0.10s]
[0m12:20:24  77 of 91 OK created sql table model main_marts.mart_daily_revenue .............. [[32mOK[0m in 0.10s]
[0m12:20:24  79 of 91 START test not_null_mart_hourly_demand_is_weekend ..................... [RUN]
[0m12:20:24  80 of 91 START test not_null_mart_hourly_demand_pickup_hour .................... [RUN]
[0m12:20:24  81 of 91 START test not_null_mart_daily_revenue_date_key ....................... [RUN]
[0m12:20:24  78 of 91 OK created sql incremental model main_marts.fct_trips ................. [[32mOK[0m in 0.11s]
[0m12:20:24  82 of 91 START test not_null_mart_daily_revenue_total_revenue .................. [RUN]
[0m12:20:24  79 of 91 PASS not_null_mart_hourly_demand_is_weekend ........................... [[32mPASS[0m in 0.06s]
[0m12:20:24  83 of 91 START test unique_mart_daily_revenue_date_key ......................... [RUN]
[0m12:20:24  80 of 91 PASS not_null_mart_hourly_demand_pickup_hour .......................... [[32mPASS[0m in 0.06s]
[0m12:20:24  84 of 91 START test not_null_fct_trips_pickup_datetime ......................... [RUN]
[0m12:20:24  81 of 91 PASS not_null_mart_daily_revenue_date_key ............................. [[32mPASS[0m in 0.07s]
[0m12:20:24  85 of 91 START test not_null_fct_trips_total_amount ............................ [RUN]
[0m12:20:24  82 of 91 PASS not_null_mart_daily_revenue_total_revenue ........................ [[32mPASS[0m in 0.05s]
[0m12:20:24  86 of 91 START test not_null_fct_trips_trip_id ................................. [RUN]
[0m12:20:24  83 of 91 PASS unique_mart_daily_revenue_date_key ............................... [[32mPASS[0m in 0.04s]
[0m12:20:24  87 of 91 START test unique_fct_trips_trip_id ................................... [RUN]
[0m12:20:24  85 of 91 PASS not_null_fct_trips_total_amount .................................. [[32mPASS[0m in 0.04s]
[0m12:20:24  84 of 91 PASS not_null_fct_trips_pickup_datetime ............................... [[32mPASS[0m in 0.05s]
[0m12:20:24  86 of 91 PASS not_null_fct_trips_trip_id ....................................... [[32mPASS[0m in 0.05s]
[0m12:20:24  87 of 91 PASS unique_fct_trips_trip_id ......................................... [[32mPASS[0m in 0.04s]
[0m12:20:24  88 of 91 START sql table model main_marts.mart_location_performance ............ [RUN]
[0m12:20:25  88 of 91 OK created sql table model main_marts.mart_location_performance ....... [[32mOK[0m in 0.07s]
[0m12:20:25  89 of 91 START test not_null_mart_location_performance_pickup_location_id ...... [RUN]
[0m12:20:25  90 of 91 START test not_null_mart_location_performance_total_pickups ........... [RUN]
[0m12:20:25  91 of 91 START test unique_mart_location_performance_pickup_location_id ........ [RUN]
[0m12:20:25  89 of 91 PASS not_null_mart_location_performance_pickup_location_id ............ [[32mPASS[0m in 0.05s]
[0m12:20:25  90 of 91 PASS not_null_mart_location_performance_total_pickups ................. [[32mPASS[0m in 0.05s]
[0m12:20:25  91 of 91 PASS unique_mart_location_performance_pickup_location_id .............. [[32mPASS[0m in 0.05s]
[0m12:20:25  
[0m12:20:25  Finished running 1 incremental model, 3 seeds, 7 table models, 74 data tests, 6 view models in 0 hours 0 minutes and 6.29 seconds (6.29s).
[0m12:20:25  
[0m12:20:25  [32mCompleted successfully[0m
[0m12:20:25  
[0m12:20:25  Done. PASS=91 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=91
dbt build complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'

============================================================
  BENCHMARK COMPLETE
  Total elapsed: 79s
============================================================
Results saved to benchmark_results/latest.json
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
cd astro-project && astro dev stop
docker compose --profile generator --profile dbt down -v
 Container p08-flink-taskmanager  Stopping
 Container p08-flink-taskmanager  Stopped
 Container p08-flink-taskmanager  Removing
 Container p08-flink-taskmanager  Removed
 Container p08-flink-jobmanager  Stopping
 Container p08-flink-jobmanager  Stopped
 Container p08-flink-jobmanager  Removing
 Container p08-flink-jobmanager  Removed
 Container p08-mc-init  Stopping
 Container p08-kafka  Stopping
 Container p08-mc-init  Stopped
 Container p08-mc-init  Removing
 Container p08-mc-init  Removed
 Container p08-minio  Stopping
 Container p08-minio  Stopped
 Container p08-minio  Removing
 Container p08-minio  Removed
 Container p08-kafka  Stopped
 Container p08-kafka  Removing
 Container p08-kafka  Removed
 Volume 08-airflow-orchestrated_minio-data  Removing
 Volume 08-airflow-orchestrated_flink-checkpoints  Removing
 Network p08-pipeline-net  Removing
 Volume 08-airflow-orchestrated_flink-checkpoints  Removed
 Volume 08-airflow-orchestrated_minio-data  Removed
 Network p08-pipeline-net  Removed
Pipeline 08 stopped and volumes removed.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/08-airflow-orchestrated'
cd astro-project && astro dev stop
docker compose --profile generator --profile dbt down -v
Pipeline 08 stopped and volumes removed.
cd astro-project && astro dev stop || true
docker compose --profile generator --profile dbt down -v --remove-orphans
docker network rm p08-pipeline-net 2>/dev/null || true
Pipeline 08 fully cleaned.
