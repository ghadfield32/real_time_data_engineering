docker compose --profile generate down -v --remove-orphans
docker compose up -d kafka minio mc-init spark-master spark-worker dbt
 Network 02-kafka-spark-iceberg_pipeline-net  Creating
 Network 02-kafka-spark-iceberg_pipeline-net  Created
 Volume 02-kafka-spark-iceberg_kafka-data  Creating
 Volume 02-kafka-spark-iceberg_kafka-data  Created
 Volume 02-kafka-spark-iceberg_minio-data  Creating
 Volume 02-kafka-spark-iceberg_minio-data  Created
 Container p02-spark-master  Creating
 Container p02-kafka  Creating
 Container p02-minio  Creating
 Container p02-kafka  Created
 Container p02-minio  Created
 Container p02-mc-init  Creating
 Container p02-spark-master  Created
 Container p02-spark-worker  Creating
 Container p02-mc-init  Created
 Container p02-dbt  Creating
 Container p02-spark-worker  Created
 Container p02-dbt  Created
 Container p02-kafka  Starting
 Container p02-spark-master  Starting
 Container p02-minio  Starting
 Container p02-kafka  Started
 Container p02-spark-master  Started
 Container p02-spark-master  Waiting
 Container p02-minio  Started
 Container p02-minio  Waiting
 Container p02-spark-master  Healthy
 Container p02-spark-worker  Starting
 Container p02-minio  Healthy
 Container p02-mc-init  Starting
 Container p02-mc-init  Started
 Container p02-mc-init  Waiting
 Container p02-spark-worker  Started
 Container p02-mc-init  Exited
 Container p02-dbt  Starting
 Container p02-dbt  Started
Waiting for services to be healthy...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
=== Pipeline 02 Benchmark ===
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
docker compose --profile generate down -v --remove-orphans
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
docker compose up -d kafka minio mc-init spark-master spark-worker dbt
 Network 02-kafka-spark-iceberg_pipeline-net  Creating
 Network 02-kafka-spark-iceberg_pipeline-net  Created
 Volume 02-kafka-spark-iceberg_minio-data  Creating
 Volume 02-kafka-spark-iceberg_minio-data  Created
 Volume 02-kafka-spark-iceberg_kafka-data  Creating
 Volume 02-kafka-spark-iceberg_kafka-data  Created
 Container p02-kafka  Creating
 Container p02-spark-master  Creating
 Container p02-minio  Creating
 Container p02-kafka  Created
 Container p02-minio  Created
 Container p02-mc-init  Creating
 Container p02-spark-master  Created
 Container p02-spark-worker  Creating
 Container p02-mc-init  Created
 Container p02-dbt  Creating
 Container p02-spark-worker  Created
 Container p02-dbt  Created
 Container p02-kafka  Starting
 Container p02-spark-master  Starting
 Container p02-minio  Starting
 Container p02-minio  Started
 Container p02-minio  Waiting
 Container p02-kafka  Started
 Container p02-spark-master  Started
 Container p02-spark-master  Waiting
 Container p02-minio  Healthy
 Container p02-mc-init  Starting
 Container p02-spark-master  Healthy
 Container p02-spark-worker  Starting
 Container p02-mc-init  Started
 Container p02-mc-init  Waiting
 Container p02-spark-worker  Started
 Container p02-mc-init  Exited
 Container p02-dbt  Starting
 Container p02-dbt  Started
Waiting for services to be healthy...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[2]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[2]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
MSYS_NO_PATHCONV=1 docker compose run --rm \
	-e MODE=burst \
	-e MAX_EVENTS=10000 \
	-e METRICS_PATH=/benchmarks/generator_metrics.json \
	data-generator
 Container p02-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.36s
  Rate:    28,017 events/sec
============================================================
  Metrics written to /benchmarks/generator_metrics.json
Sample generation complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/bronze_ingest.py
============================================================
  Pipeline 02 â€” Bronze Ingest (Kafka -> Iceberg)
============================================================
26/02/21 14:44:35 INFO SparkContext: Running Spark version 3.3.3
26/02/21 14:44:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/21 14:44:35 INFO ResourceUtils: ==============================================================
26/02/21 14:44:35 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/21 14:44:35 INFO ResourceUtils: ==============================================================
26/02/21 14:44:35 INFO SparkContext: Submitted application: Pipeline02_BronzeIngest
26/02/21 14:44:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/21 14:44:35 INFO ResourceProfile: Limiting resource is cpu
26/02/21 14:44:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/21 14:44:36 INFO SecurityManager: Changing view acls to: spark
26/02/21 14:44:36 INFO SecurityManager: Changing modify acls to: spark
26/02/21 14:44:36 INFO SecurityManager: Changing view acls groups to: 
26/02/21 14:44:36 INFO SecurityManager: Changing modify acls groups to: 
26/02/21 14:44:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/21 14:44:37 INFO Utils: Successfully started service 'sparkDriver' on port 40499.
26/02/21 14:44:37 INFO SparkEnv: Registering MapOutputTracker
26/02/21 14:44:37 INFO SparkEnv: Registering BlockManagerMaster
26/02/21 14:44:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/21 14:44:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/21 14:44:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/21 14:44:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-980ffd11-4b13-438f-ac58-cdd09c49a32f
26/02/21 14:44:37 INFO MemoryStore: MemoryStore started with capacity 127.2 MiB
26/02/21 14:44:37 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/21 14:44:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/21 14:44:37 INFO Executor: Starting executor ID driver on host spark-master
26/02/21 14:44:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/21 14:44:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42225.
26/02/21 14:44:37 INFO NettyBlockTransferService: Server created on spark-master:42225
26/02/21 14:44:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/21 14:44:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 42225, None)
26/02/21 14:44:37 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:42225 with 127.2 MiB RAM, BlockManagerId(driver, spark-master, 42225, None)
26/02/21 14:44:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 42225, None)
26/02/21 14:44:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 42225, None)
26/02/21 14:44:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Bronze table warehouse.bronze.raw_trips is ready.
26/02/21 14:44:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
Bronze ingest complete. Total rows in warehouse.bronze.raw_trips: 10,000
============================================================
  Bronze Ingest COMPLETE
============================================================
Bronze ingest complete.
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/silver_transform.py
============================================================
  Pipeline 02 â€” Silver Transform (Bronze -> Silver Iceberg)
  Column names preserved for dbt-duckdb consumption
============================================================
26/02/21 14:44:50 INFO SparkContext: Running Spark version 3.3.3
26/02/21 14:44:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/21 14:44:50 INFO ResourceUtils: ==============================================================
26/02/21 14:44:50 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/21 14:44:50 INFO ResourceUtils: ==============================================================
26/02/21 14:44:50 INFO SparkContext: Submitted application: Pipeline02_SilverTransform
26/02/21 14:44:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/21 14:44:50 INFO ResourceProfile: Limiting resource is cpu
26/02/21 14:44:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/21 14:44:51 INFO SecurityManager: Changing view acls to: spark
26/02/21 14:44:51 INFO SecurityManager: Changing modify acls to: spark
26/02/21 14:44:51 INFO SecurityManager: Changing view acls groups to: 
26/02/21 14:44:51 INFO SecurityManager: Changing modify acls groups to: 
26/02/21 14:44:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/21 14:44:51 INFO Utils: Successfully started service 'sparkDriver' on port 39055.
26/02/21 14:44:51 INFO SparkEnv: Registering MapOutputTracker
26/02/21 14:44:51 INFO SparkEnv: Registering BlockManagerMaster
26/02/21 14:44:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/21 14:44:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/21 14:44:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/21 14:44:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-95baa9b3-09a3-4e63-9dae-7648de3eb62c
26/02/21 14:44:51 INFO MemoryStore: MemoryStore started with capacity 127.2 MiB
26/02/21 14:44:51 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/21 14:44:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/21 14:44:51 INFO Executor: Starting executor ID driver on host spark-master
26/02/21 14:44:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/21 14:44:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33113.
26/02/21 14:44:51 INFO NettyBlockTransferService: Server created on spark-master:33113
26/02/21 14:44:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/21 14:44:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 33113, None)
26/02/21 14:44:51 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:33113 with 127.2 MiB RAM, BlockManagerId(driver, spark-master, 33113, None)
26/02/21 14:44:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 33113, None)
26/02/21 14:44:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 33113, None)
26/02/21 14:44:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Silver table warehouse.silver.cleaned_trips is ready.
Bronze rows read: 10,000
Silver transform complete. Rows written: 9,855
Rows filtered out: 145
============================================================
  Silver Transform COMPLETE
============================================================
Silver transform complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --profiles-dir ."
 Container p02-minio  Running
 Container p02-minio  Waiting
 Container p02-minio  Healthy
 Container p02-mc-init  Starting
 Container p02-mc-init  Started
[0m14:44:59  Running with dbt=1.11.5
[0m14:45:00  Installing dbt-labs/dbt_utils
[0m14:45:03  Installed from version 1.3.3
[0m14:45:03  Up to date!
[0m14:45:04  Running with dbt=1.11.5
[0m14:45:05  Registered adapter: duckdb=1.10.0
[0m14:45:05  Found 14 models, 74 data tests, 3 seeds, 1 source, 606 macros
[0m14:45:05  
[0m14:45:05  Concurrency: 4 threads (target='dev')
[0m14:45:05  
[0m14:45:11  1 of 91 START sql table model main.dim_dates ................................... [RUN]
[0m14:45:11  2 of 91 START sql view model main.stg_yellow_trips ............................. [RUN]
[0m14:45:11  3 of 91 START seed file main_main.payment_type_lookup .......................... [RUN]
[0m14:45:11  4 of 91 START seed file main_main.rate_code_lookup ............................. [RUN]
[0m14:45:11  3 of 91 OK loaded seed file main_main.payment_type_lookup ...................... [[32mINSERT 6[0m in 0.14s]
[0m14:45:11  4 of 91 OK loaded seed file main_main.rate_code_lookup ......................... [[32mINSERT 7[0m in 0.14s]
[0m14:45:11  5 of 91 START seed file main_main.taxi_zone_lookup ............................. [RUN]
[0m14:45:11  6 of 91 START test not_null_payment_type_lookup_payment_type_id ................ [RUN]
[0m14:45:11  2 of 91 OK created sql view model main.stg_yellow_trips ........................ [[32mOK[0m in 0.16s]
[0m14:45:11  7 of 91 START test unique_payment_type_lookup_payment_type_id .................. [RUN]
[0m14:45:11  1 of 91 OK created sql table model main.dim_dates .............................. [[32mOK[0m in 0.21s]
[0m14:45:11  8 of 91 START test not_null_rate_code_lookup_rate_code_id ...................... [RUN]
[0m14:45:11  6 of 91 PASS not_null_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.07s]
[0m14:45:11  9 of 91 START test unique_rate_code_lookup_rate_code_id ........................ [RUN]
[0m14:45:11  7 of 91 PASS unique_payment_type_lookup_payment_type_id ........................ [[32mPASS[0m in 0.06s]
[0m14:45:11  10 of 91 START test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [RUN]
[0m14:45:11  5 of 91 OK loaded seed file main_main.taxi_zone_lookup ......................... [[32mINSERT 265[0m in 0.09s]
[0m14:45:11  11 of 91 START test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [RUN]
[0m14:45:11  8 of 91 PASS not_null_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.06s]
[0m14:45:11  12 of 91 START test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ........ [RUN]
[0m14:45:11  9 of 91 PASS unique_rate_code_lookup_rate_code_id .............................. [[32mPASS[0m in 0.06s]
[0m14:45:11  13 of 91 START test assert_fare_not_exceeds_total .............................. [RUN]
[0m14:45:11  10 of 91 PASS accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[32mPASS[0m in 0.08s]
[0m14:45:11  11 of 91 PASS accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[32mPASS[0m in 0.07s]
[0m14:45:11  12 of 91 PASS accepted_values_stg_yellow_trips_vendor_id__1__2__6 .............. [[32mPASS[0m in 0.05s]
[0m14:45:11  14 of 91 START test not_null_stg_yellow_trips_dropoff_datetime ................. [RUN]
[0m14:45:11  15 of 91 START test not_null_stg_yellow_trips_dropoff_location_id .............. [RUN]
[0m14:45:11  16 of 91 START test not_null_stg_yellow_trips_fare_amount ...................... [RUN]
[0m14:45:11  13 of 91 PASS assert_fare_not_exceeds_total .................................... [[32mPASS[0m in 0.07s]
[0m14:45:11  17 of 91 START test not_null_stg_yellow_trips_pickup_datetime .................. [RUN]
[0m14:45:11  16 of 91 PASS not_null_stg_yellow_trips_fare_amount ............................ [[32mPASS[0m in 0.06s]
[0m14:45:11  15 of 91 PASS not_null_stg_yellow_trips_dropoff_location_id .................... [[32mPASS[0m in 0.06s]
[0m14:45:11  14 of 91 PASS not_null_stg_yellow_trips_dropoff_datetime ....................... [[32mPASS[0m in 0.06s]
[0m14:45:11  18 of 91 START test not_null_stg_yellow_trips_pickup_location_id ............... [RUN]
[0m14:45:11  19 of 91 START test not_null_stg_yellow_trips_total_amount ..................... [RUN]
[0m14:45:11  20 of 91 START test not_null_stg_yellow_trips_trip_distance_miles .............. [RUN]
[0m14:45:11  17 of 91 PASS not_null_stg_yellow_trips_pickup_datetime ........................ [[32mPASS[0m in 0.06s]
[0m14:45:11  21 of 91 START test not_null_stg_yellow_trips_trip_id .......................... [RUN]
[0m14:45:11  20 of 91 PASS not_null_stg_yellow_trips_trip_distance_miles .................... [[32mPASS[0m in 0.05s]
[0m14:45:11  18 of 91 PASS not_null_stg_yellow_trips_pickup_location_id ..................... [[32mPASS[0m in 0.06s]
[0m14:45:11  19 of 91 PASS not_null_stg_yellow_trips_total_amount ........................... [[32mPASS[0m in 0.06s]
[0m14:45:11  22 of 91 START test not_null_stg_yellow_trips_vendor_id ........................ [RUN]
[0m14:45:11  23 of 91 START test unique_stg_yellow_trips_trip_id ............................ [RUN]
[0m14:45:11  24 of 91 START test not_null_dim_dates_date_key ................................ [RUN]
[0m14:45:11  21 of 91 PASS not_null_stg_yellow_trips_trip_id ................................ [[32mPASS[0m in 0.07s]
[0m14:45:11  25 of 91 START test not_null_dim_dates_day_of_week_name ........................ [RUN]
[0m14:45:11  24 of 91 PASS not_null_dim_dates_date_key ...................................... [[32mPASS[0m in 0.06s]
[0m14:45:11  26 of 91 START test not_null_dim_dates_is_holiday .............................. [RUN]
[0m14:45:11  22 of 91 PASS not_null_stg_yellow_trips_vendor_id .............................. [[32mPASS[0m in 0.07s]
[0m14:45:11  27 of 91 START test not_null_dim_dates_is_weekend .............................. [RUN]
[0m14:45:11  23 of 91 PASS unique_stg_yellow_trips_trip_id .................................. [[32mPASS[0m in 0.09s]
[0m14:45:11  28 of 91 START test unique_dim_dates_date_key .................................. [RUN]
[0m14:45:11  26 of 91 PASS not_null_dim_dates_is_holiday .................................... [[32mPASS[0m in 0.04s]
[0m14:45:11  29 of 91 START sql view model main.stg_payment_types ........................... [RUN]
[0m14:45:11  25 of 91 PASS not_null_dim_dates_day_of_week_name .............................. [[32mPASS[0m in 0.06s]
[0m14:45:11  30 of 91 START test not_null_taxi_zone_lookup_Borough .......................... [RUN]
[0m14:45:11  27 of 91 PASS not_null_dim_dates_is_weekend .................................... [[32mPASS[0m in 0.05s]
[0m14:45:11  31 of 91 START test not_null_taxi_zone_lookup_LocationID ....................... [RUN]
[0m14:45:11  28 of 91 PASS unique_dim_dates_date_key ........................................ [[32mPASS[0m in 0.05s]
[0m14:45:11  32 of 91 START test unique_taxi_zone_lookup_LocationID ......................... [RUN]
[0m14:45:11  29 of 91 OK created sql view model main.stg_payment_types ...................... [[32mOK[0m in 0.06s]
[0m14:45:11  30 of 91 PASS not_null_taxi_zone_lookup_Borough ................................ [[32mPASS[0m in 0.05s]
[0m14:45:11  33 of 91 START sql view model main.stg_rate_codes .............................. [RUN]
[0m14:45:11  34 of 91 START sql view model main.int_trip_metrics ............................ [RUN]
[0m14:45:11  31 of 91 PASS not_null_taxi_zone_lookup_LocationID ............................. [[32mPASS[0m in 0.06s]
[0m14:45:11  35 of 91 START test not_null_stg_payment_types_payment_type_id ................. [RUN]
[0m14:45:11  32 of 91 PASS unique_taxi_zone_lookup_LocationID ............................... [[32mPASS[0m in 0.05s]
[0m14:45:11  36 of 91 START test not_null_stg_payment_types_payment_type_name ............... [RUN]
[0m14:45:11  35 of 91 PASS not_null_stg_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.05s]
[0m14:45:11  33 of 91 OK created sql view model main.stg_rate_codes ......................... [[32mOK[0m in 0.07s]
[0m14:45:11  37 of 91 START test unique_stg_payment_types_payment_type_id ................... [RUN]
[0m14:45:11  38 of 91 START sql view model main.stg_taxi_zones .............................. [RUN]
[0m14:45:11  34 of 91 OK created sql view model main.int_trip_metrics ....................... [[32mOK[0m in 0.08s]
[0m14:45:11  39 of 91 START test not_null_stg_rate_codes_rate_code_id ....................... [RUN]
[0m14:45:11  36 of 91 PASS not_null_stg_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.06s]
[0m14:45:11  40 of 91 START test not_null_stg_rate_codes_rate_code_name ..................... [RUN]
[0m14:45:12  37 of 91 PASS unique_stg_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.11s]
[0m14:45:12  39 of 91 PASS not_null_stg_rate_codes_rate_code_id ............................. [[32mPASS[0m in 0.10s]
[0m14:45:12  41 of 91 START test unique_stg_rate_codes_rate_code_id ......................... [RUN]
[0m14:45:12  42 of 91 START test assert_trip_duration_positive .............................. [RUN]
[0m14:45:12  38 of 91 OK created sql view model main.stg_taxi_zones ......................... [[32mOK[0m in 0.12s]
[0m14:45:12  43 of 91 START test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [RUN]
[0m14:45:12  40 of 91 PASS not_null_stg_rate_codes_rate_code_name ........................... [[32mPASS[0m in 0.11s]
[0m14:45:12  44 of 91 START test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [RUN]
[0m14:45:12  41 of 91 PASS unique_stg_rate_codes_rate_code_id ............................... [[32mPASS[0m in 0.05s]
[0m14:45:12  42 of 91 PASS assert_trip_duration_positive .................................... [[32mPASS[0m in 0.05s]
[0m14:45:12  45 of 91 START test not_null_int_trip_metrics_is_weekend ....................... [RUN]
[0m14:45:12  46 of 91 START test not_null_int_trip_metrics_pickup_date ...................... [RUN]
[0m14:45:12  43 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0 ..... [[32mPASS[0m in 0.06s]
[0m14:45:12  47 of 91 START test not_null_int_trip_metrics_pickup_hour ...................... [RUN]
[0m14:45:12  44 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[32mPASS[0m in 0.06s]
[0m14:45:12  48 of 91 START test not_null_int_trip_metrics_trip_duration_minutes ............ [RUN]
[0m14:45:12  45 of 91 PASS not_null_int_trip_metrics_is_weekend ............................. [[32mPASS[0m in 0.06s]
[0m14:45:12  49 of 91 START test not_null_int_trip_metrics_trip_id .......................... [RUN]
[0m14:45:12  46 of 91 PASS not_null_int_trip_metrics_pickup_date ............................ [[32mPASS[0m in 0.07s]
[0m14:45:12  50 of 91 START sql table model main.dim_payment_types .......................... [RUN]
[0m14:45:12  47 of 91 PASS not_null_int_trip_metrics_pickup_hour ............................ [[32mPASS[0m in 0.06s]
[0m14:45:12  51 of 91 START test not_null_stg_taxi_zones_borough ............................ [RUN]
[0m14:45:12  48 of 91 PASS not_null_int_trip_metrics_trip_duration_minutes .................. [[32mPASS[0m in 0.06s]
[0m14:45:12  52 of 91 START test not_null_stg_taxi_zones_location_id ........................ [RUN]
[0m14:45:12  49 of 91 PASS not_null_int_trip_metrics_trip_id ................................ [[32mPASS[0m in 0.07s]
[0m14:45:12  53 of 91 START test not_null_stg_taxi_zones_zone_name .......................... [RUN]
[0m14:45:12  51 of 91 PASS not_null_stg_taxi_zones_borough .................................. [[32mPASS[0m in 0.05s]
[0m14:45:12  52 of 91 PASS not_null_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.04s]
[0m14:45:12  54 of 91 START test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m14:45:12  55 of 91 START test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m14:45:12  50 of 91 OK created sql table model main.dim_payment_types ..................... [[32mOK[0m in 0.08s]
[0m14:45:12  56 of 91 START test unique_stg_taxi_zones_location_id .......................... [RUN]
[0m14:45:12  53 of 91 PASS not_null_stg_taxi_zones_zone_name ................................ [[32mPASS[0m in 0.04s]
[0m14:45:12  57 of 91 START sql view model main.int_daily_summary ........................... [RUN]
[0m14:45:12  54 of 91 PASS relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.05s]
[0m14:45:12  58 of 91 START sql view model main.int_hourly_patterns ......................... [RUN]
[0m14:45:12  55 of 91 PASS relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.05s]
[0m14:45:12  56 of 91 PASS unique_stg_taxi_zones_location_id ................................ [[32mPASS[0m in 0.04s]
[0m14:45:12  59 of 91 START test not_null_dim_payment_types_payment_type_id ................. [RUN]
[0m14:45:12  60 of 91 START test not_null_dim_payment_types_payment_type_name ............... [RUN]
[0m14:45:12  60 of 91 PASS not_null_dim_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m14:45:12  61 of 91 START test unique_dim_payment_types_payment_type_id ................... [RUN]
[0m14:45:12  59 of 91 PASS not_null_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m14:45:12  57 of 91 OK created sql view model main.int_daily_summary ...................... [[32mOK[0m in 0.08s]
[0m14:45:12  58 of 91 OK created sql view model main.int_hourly_patterns .................... [[32mOK[0m in 0.07s]
[0m14:45:12  62 of 91 START sql table model main.dim_locations .............................. [RUN]
[0m14:45:12  63 of 91 START test dbt_utils_accepted_range_int_daily_summary_total_trips__0 .. [RUN]
[0m14:45:12  64 of 91 START test not_null_int_daily_summary_pickup_date ..................... [RUN]
[0m14:45:12  61 of 91 PASS unique_dim_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m14:45:12  65 of 91 START test not_null_int_daily_summary_total_revenue ................... [RUN]
[0m14:45:12  64 of 91 PASS not_null_int_daily_summary_pickup_date ........................... [[32mPASS[0m in 0.07s]
[0m14:45:12  62 of 91 OK created sql table model main.dim_locations ......................... [[32mOK[0m in 0.07s]
[0m14:45:12  66 of 91 START test not_null_int_daily_summary_total_trips ..................... [RUN]
[0m14:45:12  63 of 91 PASS dbt_utils_accepted_range_int_daily_summary_total_trips__0 ........ [[32mPASS[0m in 0.08s]
[0m14:45:12  67 of 91 START test unique_int_daily_summary_pickup_date ....................... [RUN]
[0m14:45:12  68 of 91 START test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [RUN]
[0m14:45:12  65 of 91 PASS not_null_int_daily_summary_total_revenue ......................... [[32mPASS[0m in 0.08s]
[0m14:45:12  69 of 91 START test not_null_int_hourly_patterns_pickup_date ................... [RUN]
[0m14:45:12  68 of 91 PASS dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0 .. [[32mPASS[0m in 0.06s]
[0m14:45:12  70 of 91 START test not_null_int_hourly_patterns_pickup_hour ................... [RUN]
[0m14:45:12  67 of 91 PASS unique_int_daily_summary_pickup_date ............................. [[32mPASS[0m in 0.07s]
[0m14:45:12  66 of 91 PASS not_null_int_daily_summary_total_trips ........................... [[32mPASS[0m in 0.07s]
[0m14:45:12  71 of 91 START test not_null_int_hourly_patterns_total_trips ................... [RUN]
[0m14:45:12  72 of 91 START test not_null_dim_locations_borough ............................. [RUN]
[0m14:45:12  69 of 91 PASS not_null_int_hourly_patterns_pickup_date ......................... [[32mPASS[0m in 0.07s]
[0m14:45:12  73 of 91 START test not_null_dim_locations_location_id ......................... [RUN]
[0m14:45:12  70 of 91 PASS not_null_int_hourly_patterns_pickup_hour ......................... [[32mPASS[0m in 0.05s]
[0m14:45:12  74 of 91 START test not_null_dim_locations_zone_name ........................... [RUN]
[0m14:45:12  72 of 91 PASS not_null_dim_locations_borough ................................... [[32mPASS[0m in 0.05s]
[0m14:45:12  75 of 91 START test unique_dim_locations_location_id ........................... [RUN]
[0m14:45:12  71 of 91 PASS not_null_int_hourly_patterns_total_trips ......................... [[32mPASS[0m in 0.06s]
[0m14:45:12  76 of 91 START sql table model main.mart_daily_revenue ......................... [RUN]
[0m14:45:12  74 of 91 PASS not_null_dim_locations_zone_name ................................. [[32mPASS[0m in 0.05s]
[0m14:45:12  77 of 91 START sql table model main.mart_hourly_demand ......................... [RUN]
[0m14:45:12  73 of 91 PASS not_null_dim_locations_location_id ............................... [[32mPASS[0m in 0.07s]
[0m14:45:12  75 of 91 PASS unique_dim_locations_location_id ................................. [[32mPASS[0m in 0.06s]
[0m14:45:12  78 of 91 START sql incremental model main.fct_trips ............................ [RUN]
[0m14:45:12  76 of 91 OK created sql table model main.mart_daily_revenue .................... [[32mOK[0m in 0.12s]
[0m14:45:12  79 of 91 START test not_null_mart_daily_revenue_date_key ....................... [RUN]
[0m14:45:12  80 of 91 START test not_null_mart_daily_revenue_total_revenue .................. [RUN]
[0m14:45:12  77 of 91 OK created sql table model main.mart_hourly_demand .................... [[32mOK[0m in 0.11s]
[0m14:45:12  81 of 91 START test unique_mart_daily_revenue_date_key ......................... [RUN]
[0m14:45:12  78 of 91 OK created sql incremental model main.fct_trips ....................... [[32mOK[0m in 0.13s]
[0m14:45:12  82 of 91 START test not_null_mart_hourly_demand_is_weekend ..................... [RUN]
[0m14:45:12  80 of 91 PASS not_null_mart_daily_revenue_total_revenue ........................ [[32mPASS[0m in 0.06s]
[0m14:45:12  79 of 91 PASS not_null_mart_daily_revenue_date_key ............................. [[32mPASS[0m in 0.06s]
[0m14:45:12  83 of 91 START test not_null_mart_hourly_demand_pickup_hour .................... [RUN]
[0m14:45:12  84 of 91 START test not_null_fct_trips_pickup_datetime ......................... [RUN]
[0m14:45:12  81 of 91 PASS unique_mart_daily_revenue_date_key ............................... [[32mPASS[0m in 0.05s]
[0m14:45:12  85 of 91 START test not_null_fct_trips_total_amount ............................ [RUN]
[0m14:45:12  82 of 91 PASS not_null_mart_hourly_demand_is_weekend ........................... [[32mPASS[0m in 0.04s]
[0m14:45:12  86 of 91 START test not_null_fct_trips_trip_id ................................. [RUN]
[0m14:45:12  83 of 91 PASS not_null_mart_hourly_demand_pickup_hour .......................... [[32mPASS[0m in 0.05s]
[0m14:45:12  87 of 91 START test unique_fct_trips_trip_id ................................... [RUN]
[0m14:45:12  84 of 91 PASS not_null_fct_trips_pickup_datetime ............................... [[32mPASS[0m in 0.05s]
[0m14:45:12  85 of 91 PASS not_null_fct_trips_total_amount .................................. [[32mPASS[0m in 0.05s]
[0m14:45:12  86 of 91 PASS not_null_fct_trips_trip_id ....................................... [[32mPASS[0m in 0.04s]
[0m14:45:12  87 of 91 PASS unique_fct_trips_trip_id ......................................... [[32mPASS[0m in 0.03s]
[0m14:45:12  88 of 91 START sql table model main.mart_location_performance .................. [RUN]
[0m14:45:12  88 of 91 OK created sql table model main.mart_location_performance ............. [[32mOK[0m in 0.06s]
[0m14:45:12  89 of 91 START test not_null_mart_location_performance_pickup_location_id ...... [RUN]
[0m14:45:12  90 of 91 START test not_null_mart_location_performance_total_pickups ........... [RUN]
[0m14:45:12  91 of 91 START test unique_mart_location_performance_pickup_location_id ........ [RUN]
[0m14:45:12  89 of 91 PASS not_null_mart_location_performance_pickup_location_id ............ [[32mPASS[0m in 0.05s]
[0m14:45:12  91 of 91 PASS unique_mart_location_performance_pickup_location_id .............. [[32mPASS[0m in 0.05s]
[0m14:45:12  90 of 91 PASS not_null_mart_location_performance_total_pickups ................. [[32mPASS[0m in 0.05s]
[0m14:45:12  
[0m14:45:12  Finished running 1 incremental model, 3 seeds, 6 table models, 74 data tests, 7 view models in 0 hours 0 minutes and 7.02 seconds (7.02s).
[0m14:45:13  
[0m14:45:13  [32mCompleted successfully[0m
[0m14:45:13  
[0m14:45:13  Done. PASS=91 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=91
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/02-kafka-spark-iceberg'
=== Benchmark complete (Total: 64s) ===
docker compose --profile generate down -v --remove-orphans
 Container p02-kafka  Stopping
 Container p02-spark-worker  Stopping
 Container p02-dbt  Stopping
 Container p02-kafka  Stopped
 Container p02-kafka  Removing
 Container p02-spark-worker  Stopped
 Container p02-spark-worker  Removing
 Container p02-kafka  Removed
 Container p02-spark-worker  Removed
 Container p02-spark-master  Stopping
 Container p02-dbt  Stopped
 Container p02-dbt  Removing
 Container p02-dbt  Removed
 Container p02-mc-init  Stopping
 Container p02-mc-init  Stopped
 Container p02-mc-init  Removing
 Container p02-mc-init  Removed
 Container p02-minio  Stopping
 Container p02-minio  Stopped
 Container p02-minio  Removing
 Container p02-minio  Removed
 Container p02-spark-master  Stopped
 Container p02-spark-master  Removing
 Container p02-spark-master  Removed
 Volume 02-kafka-spark-iceberg_minio-data  Removing
 Network 02-kafka-spark-iceberg_pipeline-net  Removing
 Volume 02-kafka-spark-iceberg_kafka-data  Removing
 Volume 02-kafka-spark-iceberg_kafka-data  Removed
 Volume 02-kafka-spark-iceberg_minio-data  Removed
 Network 02-kafka-spark-iceberg_pipeline-net  Removed
docker compose --profile generate down -v --remove-orphans
rm -rf benchmark_results/*.json
