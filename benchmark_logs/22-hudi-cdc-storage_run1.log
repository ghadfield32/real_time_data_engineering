docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-kafka  Creating
 Container p22-minio  Creating
 Container p22-kafka  Created
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-minio  Starting
 Container p22-kafka  Starting
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-kafka  Started
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-mc-init  Waiting
 Container p22-kafka  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
============================================================
  Pipeline 22 Benchmark: Kafka + Spark + Apache Hudi
============================================================
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-minio  Creating
 Container p22-kafka  Creating
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-kafka  Created
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-minio  Starting
 Container p22-kafka  Starting
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-kafka  Started
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-mc-init  Waiting
 Container p22-kafka  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[2]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[2]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm \
	-e MODE=burst \
	-e MAX_EVENTS=10000 \
	data-generator
 Container p22-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.47s
  Rate:    21,372 events/sec
============================================================
  Metrics written to /tmp/generator_metrics.json
Limited data generation complete (10k events).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/bronze_ingest.py
26/02/21 15:16:48 INFO SparkContext: Running Spark version 3.3.3
26/02/21 15:16:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/21 15:16:49 INFO ResourceUtils: ==============================================================
26/02/21 15:16:49 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/21 15:16:49 INFO ResourceUtils: ==============================================================
26/02/21 15:16:49 INFO SparkContext: Submitted application: P22-Bronze-Ingest-Hudi
26/02/21 15:16:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/21 15:16:49 INFO ResourceProfile: Limiting resource is cpu
26/02/21 15:16:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/21 15:16:49 INFO SecurityManager: Changing view acls to: spark
26/02/21 15:16:49 INFO SecurityManager: Changing modify acls to: spark
26/02/21 15:16:49 INFO SecurityManager: Changing view acls groups to: 
26/02/21 15:16:49 INFO SecurityManager: Changing modify acls groups to: 
26/02/21 15:16:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/21 15:16:49 INFO Utils: Successfully started service 'sparkDriver' on port 45389.
26/02/21 15:16:49 INFO SparkEnv: Registering MapOutputTracker
26/02/21 15:16:49 INFO SparkEnv: Registering BlockManagerMaster
26/02/21 15:16:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/21 15:16:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/21 15:16:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/21 15:16:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0c9017a4-4823-4ee0-a0ff-e0ab52cebb76
26/02/21 15:16:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/21 15:16:49 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/21 15:16:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/21 15:16:49 INFO Executor: Starting executor ID driver on host spark-master
26/02/21 15:16:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/21 15:16:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42319.
26/02/21 15:16:49 INFO NettyBlockTransferService: Server created on spark-master:42319
26/02/21 15:16:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/21 15:16:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 42319, None)
26/02/21 15:16:49 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:42319 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 42319, None)
26/02/21 15:16:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 42319, None)
26/02/21 15:16:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 42319, None)
26/02/21 15:16:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/21 15:16:49 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/21 15:16:51 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/21 15:16:51 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/21 15:16:51 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/21 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:52 WARN HoodieStreamingSink: Ignore TableNotFoundException as it is first microbatch.
26/02/21 15:16:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/21 15:16:52 INFO ResolveWriteToStream: Checkpoint root s3a://warehouse/checkpoints/bronze resolved to s3a://warehouse/checkpoints/bronze.
26/02/21 15:16:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/21 15:16:52 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
26/02/21 15:16:52 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/metadata using temp file s3a://warehouse/checkpoints/bronze/.metadata.1a83f1a5-d697-4b9a-a167-e2ac838a784f.tmp
26/02/21 15:16:52 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/.metadata.1a83f1a5-d697-4b9a-a167-e2ac838a784f.tmp to s3a://warehouse/checkpoints/bronze/metadata
26/02/21 15:16:52 INFO MicroBatchExecution: Starting [id = 509855b7-f33b-40d0-b9f0-d11b12b8ba06, runId = b3a71ae5-2e7a-4252-adf7-fd6a39c628a2]. Use s3a://warehouse/checkpoints/bronze to store the query checkpoint.
26/02/21 15:16:52 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@73c9ca9b] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@17a32195]
26/02/21 15:16:52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/21 15:16:52 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/21 15:16:52 INFO AppInfoParser: Kafka version: 3.7.1
26/02/21 15:16:52 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/21 15:16:52 INFO AppInfoParser: Kafka startTimeMs: 1771687012600
26/02/21 15:16:52 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Subscribed to topic(s): taxi.raw_trips
26/02/21 15:16:52 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Cluster ID: p22-hudi-cdc-storage-001
26/02/21 15:16:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] (Re-)joining group
26/02/21 15:16:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/21 15:16:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/21 15:16:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/21 15:16:53 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] (Re-)joining group
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/21 15:16:53 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] (Re-)joining group
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/21 15:16:53 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] (Re-)joining group
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Request joining group due to: need to re-join with the given member-id: consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1-971746a6-6ef6-4655-837f-f5e42efa4935
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] (Re-)joining group
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1-971746a6-6ef6-4655-837f-f5e42efa4935', protocol='range'}
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1-971746a6-6ef6-4655-837f-f5e42efa4935=Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])}
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1-971746a6-6ef6-4655-837f-f5e42efa4935', protocol='range'}
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Notifying assignor about the new Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])
26/02/21 15:16:53 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Adding newly assigned partitions: taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-0
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-1
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-2
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-3
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-4
26/02/21 15:16:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-5
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/sources/0/0 using temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.e178a901-2745-4458-9c41-5fae616dbf0d.tmp
26/02/21 15:16:53 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.e178a901-2745-4458-9c41-5fae616dbf0d.tmp to s3a://warehouse/checkpoints/bronze/sources/0/0
26/02/21 15:16:53 INFO KafkaMicroBatchStream: Initial offsets: {"taxi.raw_trips":{"2":0,"5":0,"4":0,"1":0,"3":0,"0":0}}
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-0
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-1
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-2
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-3
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-4
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-5
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:53 INFO MicroBatchExecution: Starting new streaming query.
26/02/21 15:16:53 INFO MicroBatchExecution: Stream started from {}
26/02/21 15:16:53 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/offsets/0 using temp file s3a://warehouse/checkpoints/bronze/offsets/.0.1655282b-e595-478a-bd77-27408d4114f1.tmp
26/02/21 15:16:53 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/offsets/.0.1655282b-e595-478a-bd77-27408d4114f1.tmp to s3a://warehouse/checkpoints/bronze/offsets/0
26/02/21 15:16:53 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1771687013725,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
26/02/21 15:16:53 INFO IncrementalExecution: Current batch timestamp = 1771687013725
26/02/21 15:16:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/21 15:16:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/21 15:16:54 INFO IncrementalExecution: Current batch timestamp = 1771687013725
26/02/21 15:16:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/21 15:16:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/21 15:16:54 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/21 15:16:54 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/21 15:16:54 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips as hoodie table s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:16:54 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark
26/02/21 15:16:54 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:16:54 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:16:54 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/21 15:16:54 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:16:54 INFO CleanerUtils: Cleaned failed attempts if any
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:54 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:54 INFO BaseHoodieWriteClient: Generate a new instant time: 20260221151654294 action: commit
26/02/21 15:16:54 INFO HoodieActiveTimeline: Creating a new instant [==>20260221151654294__commit__REQUESTED]
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151654294__commit__REQUESTED__20260221151654681]}
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:54 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/21 15:16:54 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151654294__commit__REQUESTED__20260221151654681]}
26/02/21 15:16:54 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips/.hoodie/metadata as hoodie table s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:54 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:54 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:54 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:54 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
26/02/21 15:16:54 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:54 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.5 KiB, free 434.3 MiB)
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:42319 (size: 36.2 KiB, free: 434.4 MiB)
26/02/21 15:16:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/21 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4401 bytes) taskResourceAssignments Map()
26/02/21 15:16:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/21 15:16:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 926 bytes result sent to driver
26/02/21 15:16:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 88 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/21 15:16:55 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 0.317 s
26/02/21 15:16:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/21 15:16:55 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 0.337531 s
26/02/21 15:16:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:16:55 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/21 15:16:55 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/21 15:16:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:55 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:55 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
26/02/21 15:16:55 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:55 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:55 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KiB, free 434.3 MiB)
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.3 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:42319 (size: 1866.0 B, free: 434.4 MiB)
26/02/21 15:16:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/21 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/21 15:16:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/21 15:16:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 803 bytes result sent to driver
26/02/21 15:16:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/21 15:16:55 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 0.017 s
26/02/21 15:16:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/21 15:16:55 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 0.020324 s
26/02/21 15:16:55 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/21 15:16:55 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/21 15:16:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:55 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:55 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
26/02/21 15:16:55 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:55 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:55 INFO DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 393.8 KiB, free 433.9 MiB)
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 129.8 KiB, free 433.7 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:42319 (size: 129.8 KiB, free: 434.2 MiB)
26/02/21 15:16:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/21 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/21 15:16:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
26/02/21 15:16:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:42319 in memory (size: 1866.0 B, free: 434.2 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:42319 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/21 15:16:55 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/21 15:16:55 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/21 15:16:55 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/21 15:16:55 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/21 15:16:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 846 bytes result sent to driver
26/02/21 15:16:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 71 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/21 15:16:55 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 0.098 s
26/02/21 15:16:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
26/02/21 15:16:55 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 0.101816 s
26/02/21 15:16:55 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:55 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:55 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/21 15:16:55 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/21 15:16:55 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:16:55 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:16:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:55 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:55 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:55 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:16:55 INFO CleanerUtils: Cleaned failed attempts if any
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:16:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:55 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:55 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:55 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/21 15:16:55 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:55 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260221151655526]}
26/02/21 15:16:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:55 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:55 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:55 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/21 15:16:55 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/21 15:16:55 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/21 15:16:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:55 INFO DAGScheduler: Registering RDD 14 (start at <unknown>:0) as input to shuffle 0
26/02/21 15:16:55 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:55 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
26/02/21 15:16:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/21 15:16:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
26/02/21 15:16:55 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.5 KiB, free 433.9 MiB)
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.9 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:42319 (size: 4.7 KiB, free: 434.3 MiB)
26/02/21 15:16:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/21 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/21 15:16:55 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
26/02/21 15:16:55 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 964 bytes result sent to driver
26/02/21 15:16:55 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 30 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:55 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/21 15:16:55 INFO DAGScheduler: ShuffleMapStage 3 (start at <unknown>:0) finished in 0.050 s
26/02/21 15:16:55 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:16:55 INFO DAGScheduler: running: Set()
26/02/21 15:16:55 INFO DAGScheduler: waiting: Set(ResultStage 4)
26/02/21 15:16:55 INFO DAGScheduler: failed: Set()
26/02/21 15:16:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.3 KiB, free 433.9 MiB)
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.9 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:42319 (size: 3.9 KiB, free: 434.3 MiB)
26/02/21 15:16:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:55 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/21 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:55 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
26/02/21 15:16:55 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
26/02/21 15:16:55 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1246 bytes result sent to driver
26/02/21 15:16:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 60 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:55 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/21 15:16:55 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.067 s
26/02/21 15:16:55 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/21 15:16:55 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.143033 s
26/02/21 15:16:55 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/21 15:16:55 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/21 15:16:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:55 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:55 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
26/02/21 15:16:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
26/02/21 15:16:55 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:55 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 267.1 KiB, free 433.6 MiB)
26/02/21 15:16:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 433.5 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:42319 (size: 91.4 KiB, free: 434.2 MiB)
26/02/21 15:16:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:55 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/21 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:55 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/21 15:16:55 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:55 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/21 15:16:55 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE
26/02/21 15:16:55 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE in 29 ms
26/02/21 15:16:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:42319 in memory (size: 4.7 KiB, free: 434.2 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:42319 in memory (size: 129.8 KiB, free: 434.3 MiB)
26/02/21 15:16:55 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:42319 in memory (size: 3.9 KiB, free: 434.3 MiB)
26/02/21 15:16:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/21 15:16:56 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/21 15:16:56 INFO MetricsSystemImpl: HBase metrics system started
26/02/21 15:16:56 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/21 15:16:56 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:16:56 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:16:56 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/21 15:16:56 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/21 15:16:56 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 307 ms.
26/02/21 15:16:56 INFO MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 271.0 B, free 434.0 MiB)
26/02/21 15:16:56 INFO BlockManagerInfo: Added rdd_19_0 in memory on spark-master:42319 (size: 271.0 B, free: 434.3 MiB)
26/02/21 15:16:56 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 1481 bytes result sent to driver
26/02/21 15:16:56 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 339 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:56 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/21 15:16:56 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.351 s
26/02/21 15:16:56 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/21 15:16:56 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.356269 s
26/02/21 15:16:56 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/21 15:16:56 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/21 15:16:56 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:56 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:56 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
26/02/21 15:16:56 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:56 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:56 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:56 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 101.2 KiB, free 434.0 MiB)
26/02/21 15:16:56 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.9 MiB)
26/02/21 15:16:56 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:42319 (size: 36.1 KiB, free: 434.3 MiB)
26/02/21 15:16:56 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:56 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/21 15:16:56 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/21 15:16:56 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 854 bytes result sent to driver
26/02/21 15:16:56 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 16 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:56 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/21 15:16:56 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.024 s
26/02/21 15:16:56 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/21 15:16:56 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.026931 s
26/02/21 15:16:56 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/21 15:16:56 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/21 15:16:56 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/21 15:16:56 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/21 15:16:56 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:56 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:56 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
26/02/21 15:16:56 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:56 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:56 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:56 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 101.3 KiB, free 433.8 MiB)
26/02/21 15:16:56 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.8 MiB)
26/02/21 15:16:56 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:42319 (size: 36.2 KiB, free: 434.2 MiB)
26/02/21 15:16:56 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/21 15:16:56 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/21 15:16:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 900 bytes result sent to driver
26/02/21 15:16:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 37 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/21 15:16:56 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.045 s
26/02/21 15:16:56 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/21 15:16:56 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.047719 s
26/02/21 15:16:56 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: MDT s3a://warehouse/bronze/raw_trips partition FILES has been enabled
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:56 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 1083 in ms
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO BaseHoodieWriteClient: Cleaner started
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/21 15:16:56 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/21 15:16:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:56 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/21 15:16:56 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151654294__commit__REQUESTED__20260221151654681]}
26/02/21 15:16:56 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/21 15:16:56 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151654294__commit__REQUESTED__20260221151654681]}
26/02/21 15:16:56 INFO HoodieTimelineArchiver: No Instants to archive
26/02/21 15:16:56 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:56 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/21 15:16:56 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/21 15:16:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:56 INFO BlockManager: Removing RDD 19
26/02/21 15:16:56 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:56 INFO DAGScheduler: Registering RDD 26 (start at <unknown>:0) as input to shuffle 1
26/02/21 15:16:56 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 6 output partitions
26/02/21 15:16:56 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
26/02/21 15:16:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/21 15:16:56 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
26/02/21 15:16:56 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:56 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:42319 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/21 15:16:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:42319 in memory (size: 91.4 KiB, free: 434.4 MiB)
26/02/21 15:16:56 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:42319 in memory (size: 36.1 KiB, free: 434.4 MiB)
26/02/21 15:16:56 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 90.9 KiB, free 434.3 MiB)
26/02/21 15:16:56 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 434.3 MiB)
26/02/21 15:16:56 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:42319 (size: 31.0 KiB, free: 434.4 MiB)
26/02/21 15:16:56 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/21 15:16:56 INFO TaskSchedulerImpl: Adding task set 9.0 with 6 tasks resource profile 0
26/02/21 15:16:56 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 9) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 10) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 11) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 12) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 13) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:56 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
26/02/21 15:16:56 INFO Executor: Running task 1.0 in stage 9.0 (TID 9)
26/02/21 15:16:56 INFO Executor: Running task 2.0 in stage 9.0 (TID 10)
26/02/21 15:16:56 INFO Executor: Running task 5.0 in stage 9.0 (TID 13)
26/02/21 15:16:56 INFO Executor: Running task 3.0 in stage 9.0 (TID 11)
26/02/21 15:16:56 INFO Executor: Running task 4.0 in stage 9.0 (TID 12)
26/02/21 15:16:56 INFO CodeGenerator: Code generated in 125.798843 ms
26/02/21 15:16:56 INFO CodeGenerator: Code generated in 11.64425 ms
26/02/21 15:16:56 INFO CodeGenerator: Code generated in 11.287509 ms
26/02/21 15:16:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/21 15:16:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/21 15:16:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/21 15:16:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/21 15:16:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/21 15:16:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/21 15:16:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/21 15:16:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/21 15:16:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/21 15:16:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/21 15:16:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/21 15:16:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/21 15:16:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/21 15:16:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/21 15:16:56 INFO AppInfoParser: Kafka startTimeMs: 1771687016981
26/02/21 15:16:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/21 15:16:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Assigned to partition(s): taxi.raw_trips-2
26/02/21 15:16:56 INFO AppInfoParser: Kafka startTimeMs: 1771687016981
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Assigned to partition(s): taxi.raw_trips-0
26/02/21 15:16:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/21 15:16:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/21 15:16:56 INFO AppInfoParser: Kafka startTimeMs: 1771687016981
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Assigned to partition(s): taxi.raw_trips-3
26/02/21 15:16:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/21 15:16:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/21 15:16:56 INFO AppInfoParser: Kafka startTimeMs: 1771687016983
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Assigned to partition(s): taxi.raw_trips-5
26/02/21 15:16:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/21 15:16:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/21 15:16:56 INFO AppInfoParser: Kafka startTimeMs: 1771687016983
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Assigned to partition(s): taxi.raw_trips-1
26/02/21 15:16:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/21 15:16:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/21 15:16:56 INFO AppInfoParser: Kafka startTimeMs: 1771687016984
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Assigned to partition(s): taxi.raw_trips-4
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-0
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-2
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-4
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-5
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-1
26/02/21 15:16:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-3
26/02/21 15:16:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/21 15:16:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/21 15:16:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/21 15:16:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/21 15:16:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/21 15:16:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-5
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-2
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-1
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-0
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-2
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-5
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-4
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-3
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 2000 for partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to offset 2500 for partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO MemoryStore: Block rdd_6_3 stored as values in memory (estimated size 49.3 KiB, free 434.2 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added rdd_6_3 in memory on spark-master:42319 (size: 49.3 KiB, free: 434.3 MiB)
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO Executor: Finished task 3.0 in stage 9.0 (TID 11). 1753 bytes result sent to driver
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 11) in 1140 ms on spark-master (executor driver) (1/6)
26/02/21 15:16:57 INFO MemoryStore: Block rdd_6_2 stored as values in memory (estimated size 79.0 KiB, free 434.2 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added rdd_6_2 in memory on spark-master:42319 (size: 79.0 KiB, free: 434.2 MiB)
26/02/21 15:16:57 INFO MemoryStore: Block rdd_6_1 stored as values in memory (estimated size 94.1 KiB, free 434.1 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added rdd_6_1 in memory on spark-master:42319 (size: 94.1 KiB, free: 434.2 MiB)
26/02/21 15:16:57 INFO Executor: Finished task 2.0 in stage 9.0 (TID 10). 1753 bytes result sent to driver
26/02/21 15:16:57 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 10) in 1153 ms on spark-master (executor driver) (2/6)
26/02/21 15:16:57 INFO Executor: Finished task 1.0 in stage 9.0 (TID 9). 1753 bytes result sent to driver
26/02/21 15:16:57 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 9) in 1159 ms on spark-master (executor driver) (3/6)
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO MemoryStore: Block rdd_6_0 stored as values in memory (estimated size 126.8 KiB, free 433.9 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added rdd_6_0 in memory on spark-master:42319 (size: 126.8 KiB, free: 434.0 MiB)
26/02/21 15:16:57 INFO MemoryStore: Block rdd_6_5 stored as values in memory (estimated size 132.6 KiB, free 433.8 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added rdd_6_5 in memory on spark-master:42319 (size: 132.6 KiB, free: 433.9 MiB)
26/02/21 15:16:57 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1753 bytes result sent to driver
26/02/21 15:16:57 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 1189 ms on spark-master (executor driver) (4/6)
26/02/21 15:16:57 INFO Executor: Finished task 5.0 in stage 9.0 (TID 13). 1753 bytes result sent to driver
26/02/21 15:16:57 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 13) in 1192 ms on spark-master (executor driver) (5/6)
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/21 15:16:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/21 15:16:57 INFO MemoryStore: Block rdd_6_4 stored as values in memory (estimated size 191.4 KiB, free 433.6 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added rdd_6_4 in memory on spark-master:42319 (size: 191.4 KiB, free: 433.7 MiB)
26/02/21 15:16:57 INFO Executor: Finished task 4.0 in stage 9.0 (TID 12). 1753 bytes result sent to driver
26/02/21 15:16:57 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 12) in 1222 ms on spark-master (executor driver) (6/6)
26/02/21 15:16:57 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
26/02/21 15:16:57 INFO DAGScheduler: ShuffleMapStage 9 (start at <unknown>:0) finished in 1.259 s
26/02/21 15:16:57 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:16:57 INFO DAGScheduler: running: Set()
26/02/21 15:16:57 INFO DAGScheduler: waiting: Set(ResultStage 10)
26/02/21 15:16:57 INFO DAGScheduler: failed: Set()
26/02/21 15:16:57 INFO DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:57 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.5 KiB, free 433.6 MiB)
26/02/21 15:16:57 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.6 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:42319 (size: 3.1 KiB, free: 433.7 MiB)
26/02/21 15:16:57 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:57 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/21 15:16:57 INFO TaskSchedulerImpl: Adding task set 10.0 with 6 tasks resource profile 0
26/02/21 15:16:57 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14) (spark-master, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:57 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 15) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:57 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 16) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:57 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 17) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:57 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 18) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:57 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 19) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:57 INFO Executor: Running task 2.0 in stage 10.0 (TID 16)
26/02/21 15:16:57 INFO Executor: Running task 1.0 in stage 10.0 (TID 14)
26/02/21 15:16:57 INFO Executor: Running task 3.0 in stage 10.0 (TID 17)
26/02/21 15:16:57 INFO Executor: Running task 0.0 in stage 10.0 (TID 15)
26/02/21 15:16:57 INFO Executor: Running task 5.0 in stage 10.0 (TID 19)
26/02/21 15:16:57 INFO Executor: Running task 4.0 in stage 10.0 (TID 18)
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Getting 6 (528.0 B) non-empty blocks including 6 (528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:57 INFO Executor: Finished task 5.0 in stage 10.0 (TID 19). 1235 bytes result sent to driver
26/02/21 15:16:57 INFO Executor: Finished task 1.0 in stage 10.0 (TID 14). 1284 bytes result sent to driver
26/02/21 15:16:57 INFO Executor: Finished task 4.0 in stage 10.0 (TID 18). 1235 bytes result sent to driver
26/02/21 15:16:57 INFO Executor: Finished task 3.0 in stage 10.0 (TID 17). 1235 bytes result sent to driver
26/02/21 15:16:57 INFO Executor: Finished task 2.0 in stage 10.0 (TID 16). 1235 bytes result sent to driver
26/02/21 15:16:57 INFO Executor: Finished task 0.0 in stage 10.0 (TID 15). 1235 bytes result sent to driver
26/02/21 15:16:57 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 19) in 16 ms on spark-master (executor driver) (1/6)
26/02/21 15:16:57 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 18) in 16 ms on spark-master (executor driver) (2/6)
26/02/21 15:16:57 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 16) in 16 ms on spark-master (executor driver) (3/6)
26/02/21 15:16:57 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 17) in 17 ms on spark-master (executor driver) (4/6)
26/02/21 15:16:57 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 15) in 18 ms on spark-master (executor driver) (5/6)
26/02/21 15:16:57 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 20 ms on spark-master (executor driver) (6/6)
26/02/21 15:16:57 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/21 15:16:57 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.027 s
26/02/21 15:16:57 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/21 15:16:57 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 1.294998 s
26/02/21 15:16:57 INFO BaseSparkCommitActionExecutor: Source read and index timer 1323
26/02/21 15:16:57 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/21 15:16:57 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:57 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:57 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
26/02/21 15:16:57 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:57 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:57 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:57 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 269.4 KiB, free 433.4 MiB)
26/02/21 15:16:57 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 93.5 KiB, free 433.3 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:42319 (size: 93.5 KiB, free: 433.6 MiB)
26/02/21 15:16:57 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:42319 in memory (size: 3.1 KiB, free: 433.6 MiB)
26/02/21 15:16:57 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:57 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/21 15:16:57 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/21 15:16:57 INFO Executor: Running task 0.0 in stage 11.0 (TID 20)
26/02/21 15:16:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:42319 in memory (size: 31.0 KiB, free: 433.7 MiB)
26/02/21 15:16:57 INFO Executor: Finished task 0.0 in stage 11.0 (TID 20). 833 bytes result sent to driver
26/02/21 15:16:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 20) in 30 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/21 15:16:57 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.056 s
26/02/21 15:16:57 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/21 15:16:57 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.058960 s
26/02/21 15:16:57 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:57 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:57 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/21 15:16:57 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 10000, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/21 15:16:57 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/21 15:16:57 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/21 15:16:58 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260221151654294.inflight
26/02/21 15:16:58 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/21 15:16:58 INFO BaseCommitActionExecutor: Auto commit disabled for 20260221151654294
26/02/21 15:16:58 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:58 INFO DAGScheduler: Registering RDD 30 (start at <unknown>:0) as input to shuffle 2
26/02/21 15:16:58 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:58 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
26/02/21 15:16:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
26/02/21 15:16:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
26/02/21 15:16:58 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:58 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 299.7 KiB, free 433.1 MiB)
26/02/21 15:16:58 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.0 KiB, free 433.0 MiB)
26/02/21 15:16:58 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:42319 (size: 106.0 KiB, free: 433.5 MiB)
26/02/21 15:16:58 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:58 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/21 15:16:58 INFO TaskSchedulerImpl: Adding task set 12.0 with 6 tasks resource profile 0
26/02/21 15:16:58 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 21) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 22) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 23) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 24) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 25) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 26) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO Executor: Running task 0.0 in stage 12.0 (TID 21)
26/02/21 15:16:58 INFO Executor: Running task 4.0 in stage 12.0 (TID 25)
26/02/21 15:16:58 INFO Executor: Running task 5.0 in stage 12.0 (TID 26)
26/02/21 15:16:58 INFO Executor: Running task 2.0 in stage 12.0 (TID 23)
26/02/21 15:16:58 INFO Executor: Running task 3.0 in stage 12.0 (TID 24)
26/02/21 15:16:58 INFO Executor: Running task 1.0 in stage 12.0 (TID 22)
26/02/21 15:16:58 INFO BlockManager: Found block rdd_6_4 locally
26/02/21 15:16:58 INFO BlockManager: Found block rdd_6_1 locally
26/02/21 15:16:58 INFO BlockManager: Found block rdd_6_0 locally
26/02/21 15:16:58 INFO BlockManager: Found block rdd_6_5 locally
26/02/21 15:16:58 INFO BlockManager: Found block rdd_6_3 locally
26/02/21 15:16:58 INFO BlockManager: Found block rdd_6_2 locally
26/02/21 15:16:58 INFO Executor: Finished task 3.0 in stage 12.0 (TID 24). 1619 bytes result sent to driver
26/02/21 15:16:58 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 24) in 47 ms on spark-master (executor driver) (1/6)
26/02/21 15:16:58 INFO Executor: Finished task 2.0 in stage 12.0 (TID 23). 1619 bytes result sent to driver
26/02/21 15:16:58 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 23) in 52 ms on spark-master (executor driver) (2/6)
26/02/21 15:16:58 INFO Executor: Finished task 1.0 in stage 12.0 (TID 22). 1619 bytes result sent to driver
26/02/21 15:16:58 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 22) in 56 ms on spark-master (executor driver) (3/6)
26/02/21 15:16:58 INFO Executor: Finished task 5.0 in stage 12.0 (TID 26). 1619 bytes result sent to driver
26/02/21 15:16:58 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 26) in 60 ms on spark-master (executor driver) (4/6)
26/02/21 15:16:58 INFO Executor: Finished task 0.0 in stage 12.0 (TID 21). 1619 bytes result sent to driver
26/02/21 15:16:58 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 21) in 64 ms on spark-master (executor driver) (5/6)
26/02/21 15:16:58 INFO Executor: Finished task 4.0 in stage 12.0 (TID 25). 1619 bytes result sent to driver
26/02/21 15:16:58 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 25) in 69 ms on spark-master (executor driver) (6/6)
26/02/21 15:16:58 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/21 15:16:58 INFO DAGScheduler: ShuffleMapStage 12 (start at <unknown>:0) finished in 0.084 s
26/02/21 15:16:58 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:16:58 INFO DAGScheduler: running: Set()
26/02/21 15:16:58 INFO DAGScheduler: waiting: Set(ResultStage 13)
26/02/21 15:16:58 INFO DAGScheduler: failed: Set()
26/02/21 15:16:58 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:58 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 302.5 KiB, free 432.8 MiB)
26/02/21 15:16:58 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:42319 in memory (size: 93.5 KiB, free: 433.6 MiB)
26/02/21 15:16:58 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 107.4 KiB, free 432.9 MiB)
26/02/21 15:16:58 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:42319 (size: 107.4 KiB, free: 433.5 MiB)
26/02/21 15:16:58 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:58 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/21 15:16:58 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO Executor: Running task 0.0 in stage 13.0 (TID 27)
26/02/21 15:16:58 INFO ShuffleBlockFetcherIterator: Getting 6 (837.5 KiB) non-empty blocks including 6 (837.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:58 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/21 15:16:58 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260221151654294/a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0_0-13-27_20260221151654294.parquet.marker.CREATE
26/02/21 15:16:58 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260221151654294/a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0_0-13-27_20260221151654294.parquet.marker.CREATE in 25 ms
26/02/21 15:16:58 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:16:58 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0
26/02/21 15:16:58 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:42319 in memory (size: 106.0 KiB, free: 433.6 MiB)
26/02/21 15:16:58 INFO HoodieCreateHandle: Closing the file a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0 as we are done with all the records 10000
26/02/21 15:16:58 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0, took 732 ms.
26/02/21 15:16:58 INFO MemoryStore: Block rdd_34_0 stored as values in memory (estimated size 295.0 B, free 433.3 MiB)
26/02/21 15:16:58 INFO BlockManagerInfo: Added rdd_34_0 in memory on spark-master:42319 (size: 295.0 B, free: 433.6 MiB)
26/02/21 15:16:58 INFO Executor: Finished task 0.0 in stage 13.0 (TID 27). 1716 bytes result sent to driver
26/02/21 15:16:58 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 27) in 779 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:58 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/21 15:16:58 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.812 s
26/02/21 15:16:58 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
26/02/21 15:16:58 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.902055 s
26/02/21 15:16:58 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/21 15:16:58 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:58 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:58 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
26/02/21 15:16:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/21 15:16:58 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:58 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:58 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 303.2 KiB, free 433.0 MiB)
26/02/21 15:16:58 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 107.8 KiB, free 432.9 MiB)
26/02/21 15:16:58 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:42319 (size: 107.8 KiB, free: 433.5 MiB)
26/02/21 15:16:58 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:58 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/21 15:16:58 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:58 INFO Executor: Running task 0.0 in stage 15.0 (TID 28)
26/02/21 15:16:59 INFO BlockManager: Found block rdd_34_0 locally
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:42319 in memory (size: 107.4 KiB, free: 433.6 MiB)
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 15.0 (TID 28). 1720 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 28) in 31 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.050 s
26/02/21 15:16:59 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/21 15:16:59 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.053737 s
26/02/21 15:16:59 INFO BaseHoodieWriteClient: Committing 20260221151654294 action commit
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151654294__commit__INFLIGHT__20260221151657992]}
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:59 INFO CommitUtils: Creating  metadata for INSERT numWriteStats:1 numReplaceFileIds:0
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151654294__commit__INFLIGHT__20260221151657992]}
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:59 INFO BaseHoodieWriteClient: Committing 20260221151654294 action commit
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:59 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO HoodieTableMetadataUtil: Updating at 20260221151654294 from Commit/INSERT. #partitions_updated=2, #files_added=1
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/21 15:16:59 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:16:59 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:59 INFO HoodieBackedTableMetadataWriter: New commit at 20260221151654294 being applied to MDT.
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:59 INFO CleanerUtils: Cleaned failed attempts if any
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151656224]}
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:59 INFO BaseHoodieWriteClient: Generate a new instant time: 20260221151654294 action: deltacommit
26/02/21 15:16:59 INFO HoodieActiveTimeline: Creating a new instant [==>20260221151654294__deltacommit__REQUESTED]
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:16:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151654294__deltacommit__REQUESTED__20260221151659132]}
26/02/21 15:16:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:16:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:59 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/21 15:16:59 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:59 INFO DAGScheduler: Registering RDD 45 (start at <unknown>:0) as input to shuffle 3
26/02/21 15:16:59 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:59 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
26/02/21 15:16:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/21 15:16:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)
26/02/21 15:16:59 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 10.1 KiB, free 433.3 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 433.3 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:42319 (size: 5.4 KiB, free: 433.6 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:42319 in memory (size: 107.8 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 16.0 (TID 29)
26/02/21 15:16:59 INFO MemoryStore: Block rdd_43_0 stored as values in memory (estimated size 342.0 B, free 433.7 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added rdd_43_0 in memory on spark-master:42319 (size: 342.0 B, free: 433.7 MiB)
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 16.0 (TID 29). 1093 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 29) in 29 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ShuffleMapStage 16 (start at <unknown>:0) finished in 0.039 s
26/02/21 15:16:59 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:16:59 INFO DAGScheduler: running: Set()
26/02/21 15:16:59 INFO DAGScheduler: waiting: Set(ResultStage 17)
26/02/21 15:16:59 INFO DAGScheduler: failed: Set()
26/02/21 15:16:59 INFO DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KiB, free 433.7 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.7 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:42319 (size: 3.1 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 30) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 17.0 (TID 30)
26/02/21 15:16:59 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 17.0 (TID 30). 1323 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 30) in 6 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.011 s
26/02/21 15:16:59 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/21 15:16:59 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.053009 s
26/02/21 15:16:59 INFO BaseSparkCommitActionExecutor: Source read and index timer 61
26/02/21 15:16:59 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/21 15:16:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:59 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:59 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
26/02/21 15:16:59 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:59 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:59 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 262.2 KiB, free 433.5 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 433.4 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:42319 (size: 90.5 KiB, free: 433.6 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:42319 in memory (size: 5.4 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 18.0 (TID 31)
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:42319 in memory (size: 3.1 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:59 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 18.0 (TID 31). 837 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 31) in 26 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.038 s
26/02/21 15:16:59 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/21 15:16:59 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.040362 s
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/21 15:16:59 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260221151654294.deltacommit.inflight
26/02/21 15:16:59 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/21 15:16:59 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260221151654294
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:42319 in memory (size: 90.5 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:59 INFO DAGScheduler: Registering RDD 49 (start at <unknown>:0) as input to shuffle 4
26/02/21 15:16:59 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:59 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
26/02/21 15:16:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/21 15:16:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
26/02/21 15:16:59 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 266.9 KiB, free 433.5 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 93.0 KiB, free 433.4 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:42319 (size: 93.0 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 32) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 19.0 (TID 32)
26/02/21 15:16:59 INFO BlockManager: Found block rdd_43_0 locally
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 19.0 (TID 32). 1050 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 32) in 23 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ShuffleMapStage 19 (start at <unknown>:0) finished in 0.037 s
26/02/21 15:16:59 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:16:59 INFO DAGScheduler: running: Set()
26/02/21 15:16:59 INFO DAGScheduler: waiting: Set(ResultStage 20)
26/02/21 15:16:59 INFO DAGScheduler: failed: Set()
26/02/21 15:16:59 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 372.6 KiB, free 433.0 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.9 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:42319 (size: 132.2 KiB, free: 433.5 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 33) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 20.0 (TID 33)
26/02/21 15:16:59 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:16:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:16:59 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260221151654294 for file files-0000-0
26/02/21 15:16:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:16:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:16:59 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:16:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:16:59 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/21 15:16:59 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/21 15:16:59 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/21 15:16:59 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260221151654294/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND
26/02/21 15:16:59 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260221151654294/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND in 25 ms
26/02/21 15:16:59 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-20-33', fileLen=-1}
26/02/21 15:16:59 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:16:59 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:16:59 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-20-33, took 361 ms.
26/02/21 15:16:59 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 342.0 B, free 432.9 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:42319 (size: 342.0 B, free: 433.5 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:42319 in memory (size: 93.0 KiB, free: 433.6 MiB)
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 20.0 (TID 33). 1669 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 33) in 412 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.430 s
26/02/21 15:16:59 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/21 15:16:59 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.473254 s
26/02/21 15:16:59 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/21 15:16:59 INFO BaseSparkCommitActionExecutor: Committing 20260221151654294, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/21 15:16:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:59 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:59 INFO DAGScheduler: Final stage: ResultStage 21 (start at <unknown>:0)
26/02/21 15:16:59 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:59 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:59 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 101.2 KiB, free 433.1 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.1 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:42319 in memory (size: 132.2 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:42319 (size: 36.1 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 21.0 (TID 34)
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 21.0 (TID 34). 804 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 34) in 9 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ResultStage 21 (start at <unknown>:0) finished in 0.022 s
26/02/21 15:16:59 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/21 15:16:59 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.026006 s
26/02/21 15:16:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:59 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:59 INFO DAGScheduler: Final stage: ResultStage 22 (start at <unknown>:0)
26/02/21 15:16:59 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:59 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:59 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 101.1 KiB, free 433.5 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.5 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:42319 (size: 36.1 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:42319 in memory (size: 36.1 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 35) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 22.0 (TID 35)
26/02/21 15:16:59 INFO Executor: Finished task 0.0 in stage 22.0 (TID 35). 857 bytes result sent to driver
26/02/21 15:16:59 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 35) in 9 ms on spark-master (executor driver) (1/1)
26/02/21 15:16:59 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/21 15:16:59 INFO DAGScheduler: ResultStage 22 (start at <unknown>:0) finished in 0.029 s
26/02/21 15:16:59 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:16:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/21 15:16:59 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.030817 s
26/02/21 15:16:59 INFO HoodieActiveTimeline: Marking instant complete [==>20260221151654294__deltacommit__INFLIGHT]
26/02/21 15:16:59 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260221151654294.deltacommit
26/02/21 15:16:59 INFO HoodieActiveTimeline: Completed [==>20260221151654294__deltacommit__INFLIGHT]
26/02/21 15:16:59 INFO BaseSparkCommitActionExecutor: Committed 20260221151654294
26/02/21 15:16:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:16:59 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
26/02/21 15:16:59 INFO DAGScheduler: Final stage: ResultStage 23 (start at <unknown>:0)
26/02/21 15:16:59 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:16:59 INFO DAGScheduler: Missing parents: List()
26/02/21 15:16:59 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0), which has no missing parents
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 101.3 KiB, free 433.5 MiB)
26/02/21 15:16:59 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:42319 (size: 36.2 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:42319 in memory (size: 36.1 KiB, free: 433.7 MiB)
26/02/21 15:16:59 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/21 15:16:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:16:59 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
26/02/21 15:16:59 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 36) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/21 15:16:59 INFO Executor: Running task 0.0 in stage 23.0 (TID 36)
26/02/21 15:17:00 INFO Executor: Finished task 0.0 in stage 23.0 (TID 36). 900 bytes result sent to driver
26/02/21 15:17:00 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 36) in 30 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:00 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/21 15:17:00 INFO DAGScheduler: ResultStage 23 (start at <unknown>:0) finished in 0.041 s
26/02/21 15:17:00 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/21 15:17:00 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 0.042455 s
26/02/21 15:17:00 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260221151654294
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__deltacommit__COMPLETED__20260221151659944]}
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:00 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__deltacommit__COMPLETED__20260221151659944]}
26/02/21 15:17:00 INFO HoodieActiveTimeline: Marking instant complete [==>20260221151654294__commit__INFLIGHT]
26/02/21 15:17:00 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260221151654294.commit
26/02/21 15:17:00 INFO HoodieActiveTimeline: Completed [==>20260221151654294__commit__INFLIGHT]
26/02/21 15:17:00 INFO SparkContext: Starting job: start at <unknown>:0
26/02/21 15:17:00 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
26/02/21 15:17:00 INFO DAGScheduler: Final stage: ResultStage 24 (start at <unknown>:0)
26/02/21 15:17:00 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:00 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:00 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0), which has no missing parents
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 101.3 KiB, free 433.5 MiB)
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/21 15:17:00 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:42319 (size: 36.2 KiB, free: 433.7 MiB)
26/02/21 15:17:00 INFO BlockManager: Removing RDD 43
26/02/21 15:17:00 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:00 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
26/02/21 15:17:00 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 37) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/21 15:17:00 INFO Executor: Running task 0.0 in stage 24.0 (TID 37)
26/02/21 15:17:00 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:42319 in memory (size: 36.2 KiB, free: 433.7 MiB)
26/02/21 15:17:00 INFO BlockManager: Removing RDD 53
26/02/21 15:17:00 INFO Executor: Finished task 0.0 in stage 24.0 (TID 37). 964 bytes result sent to driver
26/02/21 15:17:00 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 37) in 33 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:00 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/21 15:17:00 INFO DAGScheduler: ResultStage 24 (start at <unknown>:0) finished in 0.044 s
26/02/21 15:17:00 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/21 15:17:00 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.046503 s
26/02/21 15:17:00 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260221151654294
26/02/21 15:17:00 INFO BaseHoodieWriteClient: Committed 20260221151654294
26/02/21 15:17:00 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
26/02/21 15:17:00 INFO BlockManager: Removing RDD 6
26/02/21 15:17:00 INFO MapPartitionsRDD: Removing RDD 34 from persistence list
26/02/21 15:17:00 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/21 15:17:00 INFO BlockManager: Removing RDD 34
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__deltacommit__COMPLETED__20260221151659944]}
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:00 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:00 INFO BaseHoodieWriteClient: Cleaner started
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__deltacommit__COMPLETED__20260221151659944]}
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:00 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:00 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260221151700128
26/02/21 15:17:00 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/21 15:17:00 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__deltacommit__COMPLETED__20260221151659944]}
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:00 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:00 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__deltacommit__COMPLETED__20260221151659944]}
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/21 15:17:00 INFO HoodieTimelineArchiver: No Instants to archive
26/02/21 15:17:00 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__deltacommit__COMPLETED__20260221151659944]}
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Commit 20260221151654294 successful!
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/21 15:17:00 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/21 15:17:00 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/21 15:17:00 INFO HoodieStreamingSink: Micro batch id=0 succeeded for commit=20260221151654294
26/02/21 15:17:00 INFO HoodieStreamingSink: Current value of latestCommittedBatchId: -1. Setting latestCommittedBatchId to batchId 0.
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieStreamingSink: Micro batch id=0 succeeded
26/02/21 15:17:00 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/commits/0 using temp file s3a://warehouse/checkpoints/bronze/commits/.0.685bf449-c365-4d4d-8e06-3200a6d5c397.tmp
26/02/21 15:17:00 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/commits/.0.685bf449-c365-4d4d-8e06-3200a6d5c397.tmp to s3a://warehouse/checkpoints/bronze/commits/0
26/02/21 15:17:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "509855b7-f33b-40d0-b9f0-d11b12b8ba06",
  "runId" : "b3a71ae5-2e7a-4252-adf7-fd6a39c628a2",
  "name" : null,
  "timestamp" : "2026-02-21T15:16:53.669Z",
  "batchId" : 0,
  "numInputRows" : 10000,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 1508.5231558304422,
  "durationMs" : {
    "addBatch" : 6201,
    "getBatch" : 14,
    "latestOffset" : 28,
    "queryPlanning" : 242,
    "triggerExecution" : 6628,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[taxi.raw_trips]]",
    "startOffset" : null,
    "endOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "latestOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "numInputRows" : 10000,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 1508.5231558304422,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "HoodieStreamingSink[s3a://warehouse/bronze/raw_trips]",
    "numOutputRows" : -1
  }
}
26/02/21 15:17:00 INFO MicroBatchExecution: Finished processing all available data for the trigger, terminating this Trigger.AvailableNow query
26/02/21 15:17:00 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Revoke previously assigned partitions taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/21 15:17:00 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] The pause flag in partitions [taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5] will be removed due to revocation.
26/02/21 15:17:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Member consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1-971746a6-6ef6-4655-837f-f5e42efa4935 sending LeaveGroup request to coordinator kafka:9092 (id: 2147483646 rack: null) due to the consumer is being closed
26/02/21 15:17:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/21 15:17:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0] Request joining group due to: consumer pro-actively leaving the group
26/02/21 15:17:00 INFO Metrics: Metrics scheduler closed
26/02/21 15:17:00 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/21 15:17:00 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/21 15:17:00 INFO Metrics: Metrics reporters closed
26/02/21 15:17:00 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-driver-0-1 unregistered
26/02/21 15:17:00 INFO DataSourceUtils: Getting table path..
26/02/21 15:17:00 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:00 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:00 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 7 ms
26/02/21 15:17:00 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/21 15:17:00 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/21 15:17:00 INFO DAGScheduler: Got job 18 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/21 15:17:00 INFO DAGScheduler: Final stage: ResultStage 25 (collect at HoodieSparkEngineContext.java:116)
26/02/21 15:17:00 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:00 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:00 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 101.1 KiB, free 434.2 MiB)
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.1 MiB)
26/02/21 15:17:00 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:42319 (size: 36.2 KiB, free: 434.3 MiB)
26/02/21 15:17:00 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:42319 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/21 15:17:00 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:00 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
26/02/21 15:17:00 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 38) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/21 15:17:00 INFO Executor: Running task 0.0 in stage 25.0 (TID 38)
26/02/21 15:17:00 INFO Executor: Finished task 0.0 in stage 25.0 (TID 38). 1166 bytes result sent to driver
26/02/21 15:17:00 INFO BlockManager: Removing RDD 34
26/02/21 15:17:00 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 38) in 22 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:00 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
26/02/21 15:17:00 INFO DAGScheduler: ResultStage 25 (collect at HoodieSparkEngineContext.java:116) finished in 0.034 s
26/02/21 15:17:00 INFO BlockManager: Removing RDD 6
26/02/21 15:17:00 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
26/02/21 15:17:00 INFO DAGScheduler: Job 18 finished: collect at HoodieSparkEngineContext.java:116, took 0.036587 s
26/02/21 15:17:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:00 INFO FileSourceStrategy: Pushed Filters: 
26/02/21 15:17:00 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/21 15:17:00 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/21 15:17:00 INFO CodeGenerator: Code generated in 10.120678 ms
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/21 15:17:00 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:42319 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.2 MiB)
26/02/21 15:17:00 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:42319 (size: 35.2 KiB, free: 434.4 MiB)
26/02/21 15:17:00 INFO SparkContext: Created broadcast 24 from count at <unknown>:0
26/02/21 15:17:00 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/21 15:17:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/21 15:17:00 INFO DAGScheduler: Registering RDD 68 (count at <unknown>:0) as input to shuffle 5
26/02/21 15:17:00 INFO DAGScheduler: Got map stage job 19 (count at <unknown>:0) with 1 output partitions
26/02/21 15:17:00 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (count at <unknown>:0)
26/02/21 15:17:00 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:00 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:00 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0), which has no missing parents
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.1 MiB)
26/02/21 15:17:00 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:42319 (size: 7.4 KiB, free: 434.4 MiB)
26/02/21 15:17:00 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:00 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/21 15:17:00 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 39) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/21 15:17:00 INFO Executor: Running task 0.0 in stage 26.0 (TID 39)
26/02/21 15:17:00 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0_0-13-27_20260221151654294.parquet, range: 0-681069, partition values: [empty row]
26/02/21 15:17:00 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:00 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:00 INFO Executor: Finished task 0.0 in stage 26.0 (TID 39). 2056 bytes result sent to driver
26/02/21 15:17:00 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 39) in 99 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:00 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/21 15:17:00 INFO DAGScheduler: ShuffleMapStage 26 (count at <unknown>:0) finished in 0.123 s
26/02/21 15:17:00 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:00 INFO DAGScheduler: running: Set()
26/02/21 15:17:00 INFO DAGScheduler: waiting: Set()
26/02/21 15:17:00 INFO DAGScheduler: failed: Set()
26/02/21 15:17:00 INFO CodeGenerator: Code generated in 4.071892 ms
26/02/21 15:17:00 INFO SparkContext: Starting job: count at <unknown>:0
26/02/21 15:17:00 INFO DAGScheduler: Got job 20 (count at <unknown>:0) with 1 output partitions
26/02/21 15:17:00 INFO DAGScheduler: Final stage: ResultStage 28 (count at <unknown>:0)
26/02/21 15:17:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
26/02/21 15:17:00 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:00 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0), which has no missing parents
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/21 15:17:00 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/21 15:17:00 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:42319 (size: 5.5 KiB, free: 434.4 MiB)
26/02/21 15:17:00 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:00 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:42319 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/21 15:17:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:00 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
26/02/21 15:17:00 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 40) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/21 15:17:00 INFO Executor: Running task 0.0 in stage 28.0 (TID 40)
26/02/21 15:17:00 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:00 INFO Executor: Finished task 0.0 in stage 28.0 (TID 40). 2458 bytes result sent to driver
26/02/21 15:17:00 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 40) in 22 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:00 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
26/02/21 15:17:00 INFO DAGScheduler: ResultStage 28 (count at <unknown>:0) finished in 0.032 s
26/02/21 15:17:00 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
26/02/21 15:17:00 INFO DAGScheduler: Job 20 finished: count at <unknown>:0, took 0.033574 s
Bronze ingest complete. Total rows in Hudi table: 10,000
============================================================
  Bronze Ingest (Hudi COW) COMPLETE
============================================================
26/02/21 15:17:00 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/21 15:17:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/21 15:17:00 INFO MemoryStore: MemoryStore cleared
26/02/21 15:17:00 INFO BlockManager: BlockManager stopped
26/02/21 15:17:00 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/21 15:17:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/21 15:17:00 INFO SparkContext: Successfully stopped SparkContext
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO Metrics: Metrics scheduler closed
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/21 15:17:01 INFO Metrics: Metrics reporters closed
26/02/21 15:17:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-6 unregistered
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO Metrics: Metrics scheduler closed
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/21 15:17:01 INFO Metrics: Metrics reporters closed
26/02/21 15:17:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-2 unregistered
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO Metrics: Metrics scheduler closed
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/21 15:17:01 INFO Metrics: Metrics reporters closed
26/02/21 15:17:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-7 unregistered
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO Metrics: Metrics scheduler closed
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/21 15:17:01 INFO Metrics: Metrics reporters closed
26/02/21 15:17:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-4 unregistered
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO Metrics: Metrics scheduler closed
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/21 15:17:01 INFO Metrics: Metrics reporters closed
26/02/21 15:17:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-3 unregistered
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5, groupId=spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/21 15:17:01 INFO Metrics: Metrics scheduler closed
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/21 15:17:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/21 15:17:01 INFO Metrics: Metrics reporters closed
26/02/21 15:17:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d208da2a-d33c-4977-8721-382c62055fde--1125050931-executor-5 unregistered
26/02/21 15:17:01 INFO ShutdownHookManager: Shutdown hook called
26/02/21 15:17:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-a13ac310-9f5f-4c78-922f-ab4d1fb9a223/pyspark-32361439-f08e-4bae-b565-255d331e553f
26/02/21 15:17:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-fb636c93-21b0-4193-8fb7-8e7a6d21ce1b
26/02/21 15:17:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-a13ac310-9f5f-4c78-922f-ab4d1fb9a223
Bronze ingest complete.
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/silver_transform.py
26/02/21 15:17:06 INFO SparkContext: Running Spark version 3.3.3
26/02/21 15:17:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/21 15:17:06 INFO ResourceUtils: ==============================================================
26/02/21 15:17:06 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/21 15:17:06 INFO ResourceUtils: ==============================================================
26/02/21 15:17:06 INFO SparkContext: Submitted application: P22-Silver-Transform-Hudi
26/02/21 15:17:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/21 15:17:06 INFO ResourceProfile: Limiting resource is cpu
26/02/21 15:17:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/21 15:17:06 INFO SecurityManager: Changing view acls to: spark
26/02/21 15:17:06 INFO SecurityManager: Changing modify acls to: spark
26/02/21 15:17:06 INFO SecurityManager: Changing view acls groups to: 
26/02/21 15:17:06 INFO SecurityManager: Changing modify acls groups to: 
26/02/21 15:17:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/21 15:17:07 INFO Utils: Successfully started service 'sparkDriver' on port 38937.
26/02/21 15:17:07 INFO SparkEnv: Registering MapOutputTracker
26/02/21 15:17:07 INFO SparkEnv: Registering BlockManagerMaster
26/02/21 15:17:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/21 15:17:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/21 15:17:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/21 15:17:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b5eafa73-2d81-445e-ae1f-ab44d763697a
26/02/21 15:17:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/21 15:17:07 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/21 15:17:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/21 15:17:08 INFO Executor: Starting executor ID driver on host spark-master
26/02/21 15:17:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/21 15:17:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33549.
26/02/21 15:17:08 INFO NettyBlockTransferService: Server created on spark-master:33549
26/02/21 15:17:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/21 15:17:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 33549, None)
26/02/21 15:17:08 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:33549 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 33549, None)
26/02/21 15:17:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 33549, None)
26/02/21 15:17:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 33549, None)
26/02/21 15:17:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/21 15:17:09 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/21 15:17:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/21 15:17:11 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/21 15:17:11 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/21 15:17:11 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/21 15:17:11 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/21 15:17:11 INFO DataSourceUtils: Getting table path..
26/02/21 15:17:11 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/21 15:17:11 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/21 15:17:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:11 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/21 15:17:11 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/21 15:17:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:11 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/21 15:17:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151654294__commit__COMPLETED__20260221151700040]}
26/02/21 15:17:12 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 15 ms
26/02/21 15:17:12 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/21 15:17:13 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/21 15:17:13 INFO DAGScheduler: Got job 0 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/21 15:17:13 INFO DAGScheduler: Final stage: ResultStage 0 (collect at HoodieSparkEngineContext.java:116)
26/02/21 15:17:13 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:13 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/21 15:17:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
26/02/21 15:17:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/21 15:17:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:33549 (size: 36.2 KiB, free: 434.4 MiB)
26/02/21 15:17:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/21 15:17:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/21 15:17:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/21 15:17:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1123 bytes result sent to driver
26/02/21 15:17:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 79 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/21 15:17:13 INFO DAGScheduler: ResultStage 0 (collect at HoodieSparkEngineContext.java:116) finished in 0.292 s
26/02/21 15:17:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/21 15:17:13 INFO DAGScheduler: Job 0 finished: collect at HoodieSparkEngineContext.java:116, took 0.311537 s
26/02/21 15:17:13 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:13 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:13 INFO FileSourceStrategy: Pushed Filters: 
26/02/21 15:17:13 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/21 15:17:13 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/21 15:17:13 INFO CodeGenerator: Code generated in 59.224005 ms
26/02/21 15:17:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/21 15:17:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.0 MiB)
26/02/21 15:17:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:33549 (size: 35.2 KiB, free: 434.3 MiB)
26/02/21 15:17:13 INFO SparkContext: Created broadcast 1 from count at <unknown>:0
26/02/21 15:17:13 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/21 15:17:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/21 15:17:13 INFO DAGScheduler: Registering RDD 5 (count at <unknown>:0) as input to shuffle 0
26/02/21 15:17:13 INFO DAGScheduler: Got map stage job 1 (count at <unknown>:0) with 1 output partitions
26/02/21 15:17:13 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at <unknown>:0)
26/02/21 15:17:13 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:13 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:13 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0), which has no missing parents
26/02/21 15:17:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
26/02/21 15:17:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.0 MiB)
26/02/21 15:17:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:33549 (size: 7.4 KiB, free: 434.3 MiB)
26/02/21 15:17:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/21 15:17:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/21 15:17:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/21 15:17:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:33549 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/21 15:17:13 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0_0-13-27_20260221151654294.parquet, range: 0-681069, partition values: [empty row]
26/02/21 15:17:13 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:13 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2099 bytes result sent to driver
26/02/21 15:17:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 123 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/21 15:17:13 INFO DAGScheduler: ShuffleMapStage 1 (count at <unknown>:0) finished in 0.154 s
26/02/21 15:17:13 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:13 INFO DAGScheduler: running: Set()
26/02/21 15:17:13 INFO DAGScheduler: waiting: Set()
26/02/21 15:17:13 INFO DAGScheduler: failed: Set()
26/02/21 15:17:13 INFO CodeGenerator: Code generated in 5.547667 ms
26/02/21 15:17:13 INFO SparkContext: Starting job: count at <unknown>:0
26/02/21 15:17:13 INFO DAGScheduler: Got job 2 (count at <unknown>:0) with 1 output partitions
26/02/21 15:17:13 INFO DAGScheduler: Final stage: ResultStage 3 (count at <unknown>:0)
26/02/21 15:17:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
26/02/21 15:17:13 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:13 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0), which has no missing parents
26/02/21 15:17:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/21 15:17:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/21 15:17:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:33549 (size: 5.5 KiB, free: 434.4 MiB)
26/02/21 15:17:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:14 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/21 15:17:14 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/21 15:17:14 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
26/02/21 15:17:14 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
26/02/21 15:17:14 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2458 bytes result sent to driver
26/02/21 15:17:14 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 30 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:14 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/21 15:17:14 INFO DAGScheduler: ResultStage 3 (count at <unknown>:0) finished in 0.035 s
26/02/21 15:17:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
26/02/21 15:17:14 INFO DAGScheduler: Job 2 finished: count at <unknown>:0, took 0.041229 s
Bronze rows read: 10,000
26/02/21 15:17:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/21 15:17:14 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips as hoodie table s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:14 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:17:14 INFO EmbeddedTimelineService: Overriding hostIp to (spark-master) found in spark-conf. It was null
26/02/21 15:17:14 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:14 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:14 INFO log: Logging initialized @11303ms to org.apache.hudi.org.apache.jetty.util.log.Slf4jLog
26/02/21 15:17:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:33549 in memory (size: 5.5 KiB, free: 434.4 MiB)
26/02/21 15:17:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:33549 in memory (size: 35.2 KiB, free: 434.4 MiB)
26/02/21 15:17:14 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:33549 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/21 15:17:14 INFO Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

26/02/21 15:17:14 INFO Javalin: Starting Javalin ...
26/02/21 15:17:14 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1216 days old. Consider checking for a newer version.).
26/02/21 15:17:14 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.20+8
26/02/21 15:17:14 INFO Server: Started @11513ms
26/02/21 15:17:14 INFO Javalin: Listening on http://localhost:35267/
26/02/21 15:17:14 INFO Javalin: Javalin started in 104ms \o/
26/02/21 15:17:14 INFO TimelineService: Starting Timeline server on port :35267
26/02/21 15:17:14 INFO EmbeddedTimelineService: Started embedded timeline server at spark-master:35267
26/02/21 15:17:14 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
26/02/21 15:17:14 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/21 15:17:14 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/21 15:17:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(tpep_pickup_datetime),IsNotNull(tpep_dropoff_datetime),IsNotNull(trip_distance),IsNotNull(fare_amount),GreaterThanOrEqual(trip_distance,0.0),GreaterThanOrEqual(fare_amount,0.0)
26/02/21 15:17:14 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(tpep_pickup_datetime#6),isnotnull(tpep_dropoff_datetime#7),isnotnull(trip_distance#9),isnotnull(fare_amount#15),isnotnull(cast(tpep_pickup_datetime#6 as timestamp)),isnotnull(cast(tpep_dropoff_datetime#7 as timestamp)),(trip_distance#9 >= 0.0),(fare_amount#15 >= 0.0),(cast(tpep_pickup_datetime#6 as timestamp) >= 2024-01-01 00:00:00)
26/02/21 15:17:14 INFO FileSourceStrategy: Output Data Schema: struct<VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: double, trip_distance: double ... 17 more fields>
26/02/21 15:17:14 INFO CodeGenerator: Code generated in 34.600318 ms
26/02/21 15:17:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.1 KiB, free 434.2 MiB)
26/02/21 15:17:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.2 MiB)
26/02/21 15:17:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:33549 (size: 36.1 KiB, free: 434.4 MiB)
26/02/21 15:17:14 INFO SparkContext: Created broadcast 4 from toRdd at HoodieSparkUtils.scala:111
26/02/21 15:17:14 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/21 15:17:14 INFO HoodieFileIndex: Total file slices: 1; candidate file slices after data skipping: 1; skipping percentage 0.0
26/02/21 15:17:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:14 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:17:14 INFO CleanerUtils: Cleaned failed attempts if any
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:14 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:14 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:14 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/21 15:17:14 INFO FileSystemViewManager: Creating remote first table view
26/02/21 15:17:14 INFO BaseHoodieWriteClient: Generate a new instant time: 20260221151714462 action: commit
26/02/21 15:17:14 INFO HoodieActiveTimeline: Creating a new instant [==>20260221151714462__commit__REQUESTED]
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:14 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151714462__commit__REQUESTED__20260221151714959]}
26/02/21 15:17:14 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:14 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:14 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:15 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151714462__commit__REQUESTED__20260221151714959]}
26/02/21 15:17:15 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips/.hoodie/metadata as hoodie table s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/21 15:17:15 INFO DAGScheduler: Got job 3 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/21 15:17:15 INFO DAGScheduler: Final stage: ResultStage 4 (collect at HoodieSparkEngineContext.java:116)
26/02/21 15:17:15 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:15 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:15 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 101.5 KiB, free 434.1 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 434.0 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:33549 (size: 36.3 KiB, free: 434.3 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4405 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 969 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 16 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ResultStage 4 (collect at HoodieSparkEngineContext.java:116) finished in 0.025 s
26/02/21 15:17:15 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/21 15:17:15 INFO DAGScheduler: Job 3 finished: collect at HoodieSparkEngineContext.java:116, took 0.027383 s
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:17:15 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/21 15:17:15 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/21 15:17:15 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
26/02/21 15:17:15 INFO DAGScheduler: Got job 4 (count at HoodieJavaRDD.java:115) with 1 output partitions
26/02/21 15:17:15 INFO DAGScheduler: Final stage: ResultStage 5 (count at HoodieJavaRDD.java:115)
26/02/21 15:17:15 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:15 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:15 INFO DAGScheduler: Submitting ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.0 KiB, free 434.0 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.0 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:33549 (size: 1866.0 B, free: 434.3 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 760 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 6 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ResultStage 5 (count at HoodieJavaRDD.java:115) finished in 0.010 s
26/02/21 15:17:15 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
26/02/21 15:17:15 INFO DAGScheduler: Job 4 finished: count at HoodieJavaRDD.java:115, took 0.012987 s
26/02/21 15:17:15 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/21 15:17:15 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/21 15:17:15 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
26/02/21 15:17:15 INFO DAGScheduler: Got job 5 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
26/02/21 15:17:15 INFO DAGScheduler: Final stage: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155)
26/02/21 15:17:15 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:15 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:15 INFO DAGScheduler: Submitting ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 393.9 KiB, free 433.6 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 130.0 KiB, free 433.5 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:33549 (size: 130.0 KiB, free: 434.2 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/21 15:17:15 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/21 15:17:15 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/21 15:17:15 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/21 15:17:15 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 803 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 54 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155) finished in 0.079 s
26/02/21 15:17:15 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/21 15:17:15 INFO DAGScheduler: Job 5 finished: foreach at HoodieSparkEngineContext.java:155, took 0.080673 s
26/02/21 15:17:15 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:15 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:15 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/21 15:17:15 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/21 15:17:15 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:17:15 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:15 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:17:15 INFO CleanerUtils: Cleaned failed attempts if any
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:15 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/21 15:17:15 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260221151715412]}
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:15 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/21 15:17:15 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/21 15:17:15 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/21 15:17:15 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
26/02/21 15:17:15 INFO DAGScheduler: Registering RDD 23 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 1
26/02/21 15:17:15 INFO DAGScheduler: Got job 6 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
26/02/21 15:17:15 INFO DAGScheduler: Final stage: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
26/02/21 15:17:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
26/02/21 15:17:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)
26/02/21 15:17:15 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.5 KiB, free 433.5 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.5 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:33549 (size: 4.7 KiB, free: 434.2 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 964 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 12 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ShuffleMapStage 7 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.019 s
26/02/21 15:17:15 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:15 INFO DAGScheduler: running: Set()
26/02/21 15:17:15 INFO DAGScheduler: waiting: Set(ResultStage 8)
26/02/21 15:17:15 INFO DAGScheduler: failed: Set()
26/02/21 15:17:15 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.3 KiB, free 433.5 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.5 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:33549 (size: 3.9 KiB, free: 434.2 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/21 15:17:15 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 1246 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 21 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.025 s
26/02/21 15:17:15 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/21 15:17:15 INFO DAGScheduler: Job 6 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.049030 s
26/02/21 15:17:15 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/21 15:17:15 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/21 15:17:15 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/21 15:17:15 INFO DAGScheduler: Got job 7 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/21 15:17:15 INFO DAGScheduler: Final stage: ResultStage 10 (collect at HoodieJavaRDD.java:177)
26/02/21 15:17:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/21 15:17:15 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:15 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 267.2 KiB, free 433.2 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 91.3 KiB, free 433.1 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:33549 (size: 91.3 KiB, free: 434.1 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)
26/02/21 15:17:15 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:15 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/21 15:17:15 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:33549 in memory (size: 36.3 KiB, free: 434.1 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:33549 in memory (size: 1866.0 B, free: 434.1 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:33549 in memory (size: 4.7 KiB, free: 434.1 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:33549 in memory (size: 130.0 KiB, free: 434.3 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:33549 in memory (size: 3.9 KiB, free: 434.3 MiB)
26/02/21 15:17:15 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE
26/02/21 15:17:15 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE in 29 ms
26/02/21 15:17:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/21 15:17:15 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/21 15:17:15 INFO MetricsSystemImpl: HBase metrics system started
26/02/21 15:17:15 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/21 15:17:15 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:17:15 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:17:15 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/21 15:17:15 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/21 15:17:15 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 240 ms.
26/02/21 15:17:15 INFO MemoryStore: Block rdd_28_0 stored as values in memory (estimated size 272.0 B, free 433.8 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added rdd_28_0 in memory on spark-master:33549 (size: 272.0 B, free: 434.3 MiB)
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1482 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 269 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ResultStage 10 (collect at HoodieJavaRDD.java:177) finished in 0.279 s
26/02/21 15:17:15 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/21 15:17:15 INFO DAGScheduler: Job 7 finished: collect at HoodieJavaRDD.java:177, took 0.283303 s
26/02/21 15:17:15 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/21 15:17:15 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/21 15:17:15 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/21 15:17:15 INFO DAGScheduler: Got job 8 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/21 15:17:15 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HoodieSparkEngineContext.java:150)
26/02/21 15:17:15 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:15 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:15 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 101.2 KiB, free 433.7 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.7 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:33549 (size: 36.1 KiB, free: 434.2 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 855 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 12 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ResultStage 11 (collect at HoodieSparkEngineContext.java:150) finished in 0.020 s
26/02/21 15:17:15 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/21 15:17:15 INFO DAGScheduler: Job 8 finished: collect at HoodieSparkEngineContext.java:150, took 0.022516 s
26/02/21 15:17:15 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/21 15:17:15 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/21 15:17:15 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/21 15:17:15 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/21 15:17:15 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/21 15:17:15 INFO DAGScheduler: Got job 9 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/21 15:17:15 INFO DAGScheduler: Final stage: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/21 15:17:15 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:15 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:15 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 101.3 KiB, free 433.6 MiB)
26/02/21 15:17:15 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/21 15:17:15 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:33549 (size: 36.2 KiB, free: 434.2 MiB)
26/02/21 15:17:15 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:15 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
26/02/21 15:17:15 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/21 15:17:15 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
26/02/21 15:17:15 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 904 bytes result sent to driver
26/02/21 15:17:15 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 24 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:15 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/21 15:17:15 INFO DAGScheduler: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.030 s
26/02/21 15:17:15 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
26/02/21 15:17:15 INFO DAGScheduler: Job 9 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.033471 s
26/02/21 15:17:15 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: MDT s3a://warehouse/silver/cleaned_trips partition FILES has been enabled
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:16 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:16 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 824 in ms
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:33549 in memory (size: 91.3 KiB, free: 434.3 MiB)
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:33549 in memory (size: 36.1 KiB, free: 434.3 MiB)
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO BlockManager: Removing RDD 28
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:33549 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/21 15:17:16 INFO BaseHoodieWriteClient: Cleaner started
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:16 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/21 15:17:16 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/21 15:17:16 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:16 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:16 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/21 15:17:16 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151714462__commit__REQUESTED__20260221151714959]}
26/02/21 15:17:16 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:16 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/21 15:17:16 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151714462__commit__REQUESTED__20260221151714959]}
26/02/21 15:17:16 INFO HoodieTimelineArchiver: No Instants to archive
26/02/21 15:17:16 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:16 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:16 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:16 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/21 15:17:16 INFO FileSystemViewManager: Creating remote first table view
26/02/21 15:17:16 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/21 15:17:16 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/21 15:17:16 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/21 15:17:16 INFO DAGScheduler: Registering RDD 34 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 3
26/02/21 15:17:16 INFO DAGScheduler: Registering RDD 40 (distinct at HoodieJavaRDD.java:157) as input to shuffle 2
26/02/21 15:17:16 INFO DAGScheduler: Got job 10 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/21 15:17:16 INFO DAGScheduler: Final stage: ResultStage 15 (collect at HoodieJavaRDD.java:177)
26/02/21 15:17:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/21 15:17:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 14)
26/02/21 15:17:16 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 104.0 KiB, free 434.1 MiB)
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KiB, free 434.0 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:33549 (size: 31.5 KiB, free: 434.3 MiB)
26/02/21 15:17:16 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:16 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/21 15:17:16 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/21 15:17:16 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)
26/02/21 15:17:16 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/a12ee1bc-4f4e-4fb1-a518-09f4ebccd0fb-0_0-13-27_20260221151654294.parquet, range: 0-681069, partition values: [empty row]
26/02/21 15:17:16 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:16 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(tpep_pickup_datetime, null), noteq(tpep_dropoff_datetime, null)), noteq(trip_distance, null)), noteq(fare_amount, null)), gteq(trip_distance, 0.0)), gteq(fare_amount, 0.0))
26/02/21 15:17:16 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:16 INFO CodecPool: Got brand-new decompressor [.gz]
26/02/21 15:17:16 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 1824 bytes result sent to driver
26/02/21 15:17:16 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 422 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:16 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/21 15:17:16 INFO DAGScheduler: ShuffleMapStage 13 (mapToPair at HoodieJavaRDD.java:149) finished in 0.431 s
26/02/21 15:17:16 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:16 INFO DAGScheduler: running: Set()
26/02/21 15:17:16 INFO DAGScheduler: waiting: Set(ResultStage 15, ShuffleMapStage 14)
26/02/21 15:17:16 INFO DAGScheduler: failed: Set()
26/02/21 15:17:16 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 30.1 KiB, free 434.0 MiB)
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 434.0 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:33549 (size: 13.5 KiB, free: 434.3 MiB)
26/02/21 15:17:16 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:16 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
26/02/21 15:17:16 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 12) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/21 15:17:16 INFO Executor: Running task 0.0 in stage 14.0 (TID 12)
26/02/21 15:17:16 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:16 INFO MemoryStore: Block rdd_36_0 stored as values in memory (estimated size 994.0 KiB, free 433.0 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Added rdd_36_0 in memory on spark-master:33549 (size: 994.0 KiB, free: 433.4 MiB)
26/02/21 15:17:16 INFO Executor: Finished task 0.0 in stage 14.0 (TID 12). 1394 bytes result sent to driver
26/02/21 15:17:16 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 12) in 109 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:16 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
26/02/21 15:17:16 INFO DAGScheduler: ShuffleMapStage 14 (distinct at HoodieJavaRDD.java:157) finished in 0.119 s
26/02/21 15:17:16 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:16 INFO DAGScheduler: running: Set()
26/02/21 15:17:16 INFO DAGScheduler: waiting: Set(ResultStage 15)
26/02/21 15:17:16 INFO DAGScheduler: failed: Set()
26/02/21 15:17:16 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 6.3 KiB, free 433.0 MiB)
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.0 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:33549 (size: 3.4 KiB, free: 433.3 MiB)
26/02/21 15:17:16 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:16 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/21 15:17:16 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:16 INFO Executor: Running task 0.0 in stage 15.0 (TID 13)
26/02/21 15:17:16 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:16 INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 1237 bytes result sent to driver
26/02/21 15:17:16 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 7 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:16 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/21 15:17:16 INFO DAGScheduler: ResultStage 15 (collect at HoodieJavaRDD.java:177) finished in 0.012 s
26/02/21 15:17:16 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/21 15:17:16 INFO DAGScheduler: Job 10 finished: collect at HoodieJavaRDD.java:177, took 0.567638 s
26/02/21 15:17:16 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/21 15:17:16 INFO DAGScheduler: Got job 11 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/21 15:17:16 INFO DAGScheduler: Final stage: ResultStage 16 (collect at HoodieSparkEngineContext.java:150)
26/02/21 15:17:16 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:16 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:16 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 268.6 KiB, free 432.7 MiB)
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 432.6 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:33549 (size: 93.3 KiB, free: 433.3 MiB)
26/02/21 15:17:16 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:16 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/21 15:17:16 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/21 15:17:16 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)
26/02/21 15:17:16 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 804 bytes result sent to driver
26/02/21 15:17:16 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 11 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:16 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/21 15:17:16 INFO DAGScheduler: ResultStage 16 (collect at HoodieSparkEngineContext.java:150) finished in 0.029 s
26/02/21 15:17:16 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
26/02/21 15:17:16 INFO DAGScheduler: Job 11 finished: collect at HoodieSparkEngineContext.java:150, took 0.031605 s
26/02/21 15:17:16 INFO MapPartitionsRDD: Removing RDD 36 from persistence list
26/02/21 15:17:16 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:16 INFO BlockManager: Removing RDD 36
26/02/21 15:17:16 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:16 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/21 15:17:16 INFO DAGScheduler: Registering RDD 47 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 5
26/02/21 15:17:16 INFO DAGScheduler: Registering RDD 37 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 4
26/02/21 15:17:16 INFO DAGScheduler: Registering RDD 55 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 6
26/02/21 15:17:16 INFO DAGScheduler: Got job 12 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/21 15:17:16 INFO DAGScheduler: Final stage: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105)
26/02/21 15:17:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
26/02/21 15:17:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
26/02/21 15:17:16 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 270.6 KiB, free 433.4 MiB)
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 94.4 KiB, free 433.3 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:33549 in memory (size: 13.5 KiB, free: 434.2 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:33549 (size: 94.4 KiB, free: 434.1 MiB)
26/02/21 15:17:16 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:16 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/21 15:17:16 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/21 15:17:16 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4319 bytes) taskResourceAssignments Map()
26/02/21 15:17:16 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
26/02/21 15:17:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:33549 in memory (size: 31.5 KiB, free: 434.2 MiB)
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 29.0 KiB, free 433.4 MiB)
26/02/21 15:17:16 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 433.4 MiB)
26/02/21 15:17:16 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:33549 (size: 13.1 KiB, free: 434.2 MiB)
26/02/21 15:17:16 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:16 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/21 15:17:16 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/21 15:17:16 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:33549 in memory (size: 3.4 KiB, free: 434.2 MiB)
26/02/21 15:17:16 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
26/02/21 15:17:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:33549 in memory (size: 93.3 KiB, free: 434.3 MiB)
26/02/21 15:17:16 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
26/02/21 15:17:16 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 835 bytes result sent to driver
26/02/21 15:17:16 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 32 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:16 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/21 15:17:16 INFO DAGScheduler: ShuffleMapStage 18 (mapToPair at HoodieJavaRDD.java:149) finished in 0.052 s
26/02/21 15:17:16 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:16 INFO DAGScheduler: running: Set(ShuffleMapStage 19)
26/02/21 15:17:16 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/21 15:17:16 INFO DAGScheduler: failed: Set()
26/02/21 15:17:17 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 1394 bytes result sent to driver
26/02/21 15:17:17 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 81 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:17 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/21 15:17:17 INFO DAGScheduler: ShuffleMapStage 19 (mapToPair at HoodieJavaRDD.java:149) finished in 0.089 s
26/02/21 15:17:17 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:17 INFO DAGScheduler: running: Set()
26/02/21 15:17:17 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/21 15:17:17 INFO DAGScheduler: failed: Set()
26/02/21 15:17:17 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.8 KiB, free 433.7 MiB)
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.7 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:33549 (size: 5.1 KiB, free: 434.3 MiB)
26/02/21 15:17:17 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:17 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/21 15:17:17 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/21 15:17:17 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:17 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 994.0 KiB, free 432.8 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:33549 (size: 994.0 KiB, free: 433.3 MiB)
26/02/21 15:17:17 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 1394 bytes result sent to driver
26/02/21 15:17:17 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 96 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:17 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/21 15:17:17 INFO DAGScheduler: ShuffleMapStage 20 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.101 s
26/02/21 15:17:17 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:17 INFO DAGScheduler: running: Set()
26/02/21 15:17:17 INFO DAGScheduler: waiting: Set(ResultStage 21)
26/02/21 15:17:17 INFO DAGScheduler: failed: Set()
26/02/21 15:17:17 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.5 KiB, free 432.8 MiB)
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.8 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:33549 (size: 3.1 KiB, free: 433.3 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:33549 in memory (size: 94.4 KiB, free: 433.4 MiB)
26/02/21 15:17:17 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:17 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/21 15:17:17 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:17 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
26/02/21 15:17:17 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:33549 in memory (size: 13.1 KiB, free: 433.4 MiB)
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:17 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1327 bytes result sent to driver
26/02/21 15:17:17 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 23 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:17 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/21 15:17:17 INFO DAGScheduler: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.036 s
26/02/21 15:17:17 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/21 15:17:17 INFO DAGScheduler: Job 12 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.251260 s
26/02/21 15:17:17 INFO BaseSparkCommitActionExecutor: Source read and index timer 270
26/02/21 15:17:17 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/21 15:17:17 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/21 15:17:17 INFO DAGScheduler: Got job 13 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/21 15:17:17 INFO DAGScheduler: Final stage: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285)
26/02/21 15:17:17 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:17 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:17 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 269.4 KiB, free 432.9 MiB)
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 93.7 KiB, free 432.8 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:33549 (size: 93.7 KiB, free: 433.3 MiB)
26/02/21 15:17:17 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:17 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/21 15:17:17 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/21 15:17:17 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
26/02/21 15:17:17 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 790 bytes result sent to driver
26/02/21 15:17:17 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 9 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:17 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/21 15:17:17 INFO DAGScheduler: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285) finished in 0.022 s
26/02/21 15:17:17 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/21 15:17:17 INFO DAGScheduler: Job 13 finished: collectAsMap at UpsertPartitioner.java:285, took 0.024471 s
26/02/21 15:17:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:17 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/21 15:17:17 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 9855, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/21 15:17:17 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/21 15:17:17 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/21 15:17:17 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260221151714462.inflight
26/02/21 15:17:17 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/21 15:17:17 INFO BaseCommitActionExecutor: Auto commit disabled for 20260221151714462
26/02/21 15:17:17 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1073
26/02/21 15:17:17 INFO DAGScheduler: Registering RDD 59 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 7
26/02/21 15:17:17 INFO DAGScheduler: Got job 14 (count at HoodieSparkSqlWriter.scala:1073) with 1 output partitions
26/02/21 15:17:17 INFO DAGScheduler: Final stage: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073)
26/02/21 15:17:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
26/02/21 15:17:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 26)
26/02/21 15:17:17 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 274.5 KiB, free 432.5 MiB)
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 95.3 KiB, free 432.4 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:33549 (size: 95.3 KiB, free: 433.2 MiB)
26/02/21 15:17:17 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:17 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/21 15:17:17 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/21 15:17:17 INFO Executor: Running task 0.0 in stage 26.0 (TID 20)
26/02/21 15:17:17 INFO BlockManager: Found block rdd_53_0 locally
26/02/21 15:17:17 INFO Executor: Finished task 0.0 in stage 26.0 (TID 20). 1050 bytes result sent to driver
26/02/21 15:17:17 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 38 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:17 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/21 15:17:17 INFO DAGScheduler: ShuffleMapStage 26 (mapToPair at HoodieJavaRDD.java:149) finished in 0.049 s
26/02/21 15:17:17 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:17 INFO DAGScheduler: running: Set()
26/02/21 15:17:17 INFO DAGScheduler: waiting: Set(ResultStage 27)
26/02/21 15:17:17 INFO DAGScheduler: failed: Set()
26/02/21 15:17:17 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073), which has no missing parents
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 313.8 KiB, free 432.1 MiB)
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 108.0 KiB, free 432.0 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:33549 (size: 108.0 KiB, free: 433.1 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:33549 in memory (size: 5.1 KiB, free: 433.1 MiB)
26/02/21 15:17:17 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:17 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
26/02/21 15:17:17 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:17 INFO Executor: Running task 0.0 in stage 27.0 (TID 21)
26/02/21 15:17:17 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:33549 in memory (size: 3.1 KiB, free: 433.1 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:33549 in memory (size: 93.7 KiB, free: 433.2 MiB)
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:17 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/21 15:17:17 INFO MarkerHandler: Request: create marker: 10ac6d70-892a-4e25-83fa-aa923f7e8bc7-0_0-27-21_20260221151714462.parquet.marker.CREATE
26/02/21 15:17:17 INFO TimelineServerBasedWriteMarkers: [timeline-server-based] Created marker file /10ac6d70-892a-4e25-83fa-aa923f7e8bc7-0_0-27-21_20260221151714462.parquet.marker.CREATE in 215 ms
26/02/21 15:17:17 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:17:17 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId 10ac6d70-892a-4e25-83fa-aa923f7e8bc7-0
26/02/21 15:17:17 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:33549 in memory (size: 95.3 KiB, free: 433.3 MiB)
26/02/21 15:17:17 INFO HoodieCreateHandle: Closing the file 10ac6d70-892a-4e25-83fa-aa923f7e8bc7-0 as we are done with all the records 9855
26/02/21 15:17:17 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID 10ac6d70-892a-4e25-83fa-aa923f7e8bc7-0, took 557 ms.
26/02/21 15:17:17 INFO MemoryStore: Block rdd_63_0 stored as values in memory (estimated size 295.0 B, free 432.8 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added rdd_63_0 in memory on spark-master:33549 (size: 295.0 B, free: 433.3 MiB)
26/02/21 15:17:17 INFO Executor: Finished task 0.0 in stage 27.0 (TID 21). 1749 bytes result sent to driver
26/02/21 15:17:17 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 597 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:17 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/21 15:17:17 INFO DAGScheduler: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073) finished in 0.617 s
26/02/21 15:17:17 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/21 15:17:17 INFO DAGScheduler: Job 14 finished: count at HoodieSparkSqlWriter.scala:1073, took 0.670206 s
26/02/21 15:17:17 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/21 15:17:17 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:107
26/02/21 15:17:17 INFO DAGScheduler: Got job 15 (collect at SparkRDDWriteClient.java:107) with 1 output partitions
26/02/21 15:17:17 INFO DAGScheduler: Final stage: ResultStage 32 (collect at SparkRDDWriteClient.java:107)
26/02/21 15:17:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/21 15:17:17 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:17 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107), which has no missing parents
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 314.4 KiB, free 432.5 MiB)
26/02/21 15:17:17 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 108.4 KiB, free 432.4 MiB)
26/02/21 15:17:17 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:33549 (size: 108.4 KiB, free: 433.2 MiB)
26/02/21 15:17:17 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:17 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
26/02/21 15:17:17 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 22) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:17 INFO Executor: Running task 0.0 in stage 32.0 (TID 22)
26/02/21 15:17:17 INFO BlockManager: Found block rdd_63_0 locally
26/02/21 15:17:17 INFO Executor: Finished task 0.0 in stage 32.0 (TID 22). 1710 bytes result sent to driver
26/02/21 15:17:17 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 22) in 10 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:17 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/21 15:17:17 INFO DAGScheduler: ResultStage 32 (collect at SparkRDDWriteClient.java:107) finished in 0.023 s
26/02/21 15:17:17 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/21 15:17:17 INFO DAGScheduler: Job 15 finished: collect at SparkRDDWriteClient.java:107, took 0.026132 s
26/02/21 15:17:17 INFO BaseHoodieWriteClient: Committing 20260221151714462 action commit
26/02/21 15:17:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:17 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151714462__commit__INFLIGHT__20260221151717244]}
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating remote first table view
26/02/21 15:17:18 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151714462__commit__INFLIGHT__20260221151717244]}
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating remote first table view
26/02/21 15:17:18 INFO BaseHoodieWriteClient: Committing 20260221151714462 action commit
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:18 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO HoodieTableMetadataUtil: Updating at 20260221151714462 from Commit/UPSERT. #partitions_updated=2, #files_added=1
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/21 15:17:18 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:17:18 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:18 INFO HoodieBackedTableMetadataWriter: New commit at 20260221151714462 being applied to MDT.
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:18 INFO CleanerUtils: Cleaned failed attempts if any
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260221151715905]}
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:18 INFO BaseHoodieWriteClient: Generate a new instant time: 20260221151714462 action: deltacommit
26/02/21 15:17:18 INFO HoodieActiveTimeline: Creating a new instant [==>20260221151714462__deltacommit__REQUESTED]
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260221151714462__deltacommit__REQUESTED__20260221151718129]}
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:18 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/21 15:17:18 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/21 15:17:18 INFO DAGScheduler: Registering RDD 74 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 8
26/02/21 15:17:18 INFO DAGScheduler: Got job 16 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/21 15:17:18 INFO DAGScheduler: Final stage: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105)
26/02/21 15:17:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
26/02/21 15:17:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 33)
26/02/21 15:17:18 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 10.1 KiB, free 432.4 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 432.3 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:33549 (size: 5.4 KiB, free: 433.2 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 23) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 33.0 (TID 23)
26/02/21 15:17:18 INFO MemoryStore: Block rdd_72_0 stored as values in memory (estimated size 342.0 B, free 432.3 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added rdd_72_0 in memory on spark-master:33549 (size: 342.0 B, free: 433.2 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:33549 in memory (size: 108.4 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 33.0 (TID 23). 1136 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 23) in 13 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ShuffleMapStage 33 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.016 s
26/02/21 15:17:18 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:18 INFO DAGScheduler: running: Set()
26/02/21 15:17:18 INFO DAGScheduler: waiting: Set(ResultStage 34)
26/02/21 15:17:18 INFO DAGScheduler: failed: Set()
26/02/21 15:17:18 INFO DAGScheduler: Submitting ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 5.5 KiB, free 432.8 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.7 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:33549 (size: 3.1 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 24) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 34.0 (TID 24)
26/02/21 15:17:18 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:33549 in memory (size: 108.0 KiB, free: 433.4 MiB)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 34.0 (TID 24). 1366 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 24) in 21 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.023 s
26/02/21 15:17:18 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
26/02/21 15:17:18 INFO DAGScheduler: Job 16 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.042234 s
26/02/21 15:17:18 INFO BaseSparkCommitActionExecutor: Source read and index timer 49
26/02/21 15:17:18 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/21 15:17:18 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/21 15:17:18 INFO DAGScheduler: Got job 17 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/21 15:17:18 INFO DAGScheduler: Final stage: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285)
26/02/21 15:17:18 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:18 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:18 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 262.2 KiB, free 432.9 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 432.8 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:33549 (size: 90.5 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 25) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 35.0 (TID 25)
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:18 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 35.0 (TID 25). 837 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 25) in 11 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285) finished in 0.019 s
26/02/21 15:17:18 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/21 15:17:18 INFO DAGScheduler: Job 17 finished: collectAsMap at UpsertPartitioner.java:285, took 0.021117 s
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/21 15:17:18 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260221151714462.deltacommit.inflight
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:33549 in memory (size: 3.1 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/21 15:17:18 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260221151714462
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:33549 in memory (size: 90.5 KiB, free: 433.4 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:33549 in memory (size: 5.4 KiB, free: 433.4 MiB)
26/02/21 15:17:18 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/21 15:17:18 INFO DAGScheduler: Registering RDD 78 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 9
26/02/21 15:17:18 INFO DAGScheduler: Got job 18 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/21 15:17:18 INFO DAGScheduler: Final stage: ResultStage 37 (collect at HoodieJavaRDD.java:177)
26/02/21 15:17:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)
26/02/21 15:17:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 36)
26/02/21 15:17:18 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 267.0 KiB, free 432.9 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 93.0 KiB, free 432.8 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:33549 (size: 93.0 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 26) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 36.0 (TID 26)
26/02/21 15:17:18 INFO BlockManager: Found block rdd_72_0 locally
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 36.0 (TID 26). 1050 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 26) in 8 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ShuffleMapStage 36 (mapToPair at HoodieJavaRDD.java:149) finished in 0.017 s
26/02/21 15:17:18 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:18 INFO DAGScheduler: running: Set()
26/02/21 15:17:18 INFO DAGScheduler: waiting: Set(ResultStage 37)
26/02/21 15:17:18 INFO DAGScheduler: failed: Set()
26/02/21 15:17:18 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 372.6 KiB, free 432.5 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.3 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:33549 (size: 132.2 KiB, free: 433.2 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 37.0 (TID 27)
26/02/21 15:17:18 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:18 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260221151714462 for file files-0000-0
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:18 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:18 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:18 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/21 15:17:18 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/21 15:17:18 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/21 15:17:18 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260221151714462/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND
26/02/21 15:17:18 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260221151714462/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND in 25 ms
26/02/21 15:17:18 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-37-27', fileLen=-1}
26/02/21 15:17:18 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:17:18 INFO CodecPool: Got brand-new compressor [.gz]
26/02/21 15:17:18 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-37-27, took 353 ms.
26/02/21 15:17:18 INFO MemoryStore: Block rdd_82_0 stored as values in memory (estimated size 343.0 B, free 432.3 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added rdd_82_0 in memory on spark-master:33549 (size: 343.0 B, free: 433.2 MiB)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 37.0 (TID 27). 1627 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 27) in 394 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ResultStage 37 (collect at HoodieJavaRDD.java:177) finished in 0.410 s
26/02/21 15:17:18 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
26/02/21 15:17:18 INFO DAGScheduler: Job 18 finished: collect at HoodieJavaRDD.java:177, took 0.430121 s
26/02/21 15:17:18 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/21 15:17:18 INFO BaseSparkCommitActionExecutor: Committing 20260221151714462, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/21 15:17:18 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/21 15:17:18 INFO DAGScheduler: Got job 19 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/21 15:17:18 INFO DAGScheduler: Final stage: ResultStage 38 (collect at HoodieSparkEngineContext.java:150)
26/02/21 15:17:18 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:18 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:18 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 101.2 KiB, free 432.2 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.2 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:33549 (size: 36.1 KiB, free: 433.1 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 38.0 (TID 28)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 38.0 (TID 28). 804 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 28) in 8 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ResultStage 38 (collect at HoodieSparkEngineContext.java:150) finished in 0.014 s
26/02/21 15:17:18 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/21 15:17:18 INFO DAGScheduler: Job 19 finished: collect at HoodieSparkEngineContext.java:150, took 0.015669 s
26/02/21 15:17:18 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/21 15:17:18 INFO DAGScheduler: Got job 20 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/21 15:17:18 INFO DAGScheduler: Final stage: ResultStage 39 (collect at HoodieSparkEngineContext.java:150)
26/02/21 15:17:18 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:18 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:18 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 101.1 KiB, free 432.1 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.1 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:33549 in memory (size: 93.0 KiB, free: 433.2 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:33549 (size: 36.1 KiB, free: 433.2 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:33549 in memory (size: 36.1 KiB, free: 433.2 MiB)
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 39.0 (TID 29)
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:33549 in memory (size: 132.2 KiB, free: 433.4 MiB)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 39.0 (TID 29). 857 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 29) in 15 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ResultStage 39 (collect at HoodieSparkEngineContext.java:150) finished in 0.027 s
26/02/21 15:17:18 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/21 15:17:18 INFO DAGScheduler: Job 20 finished: collect at HoodieSparkEngineContext.java:150, took 0.029007 s
26/02/21 15:17:18 INFO HoodieActiveTimeline: Marking instant complete [==>20260221151714462__deltacommit__INFLIGHT]
26/02/21 15:17:18 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260221151714462.deltacommit
26/02/21 15:17:18 INFO HoodieActiveTimeline: Completed [==>20260221151714462__deltacommit__INFLIGHT]
26/02/21 15:17:18 INFO BaseSparkCommitActionExecutor: Committed 20260221151714462
26/02/21 15:17:18 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/21 15:17:18 INFO DAGScheduler: Got job 21 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/21 15:17:18 INFO DAGScheduler: Final stage: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/21 15:17:18 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:18 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:18 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 101.3 KiB, free 433.0 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.9 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:33549 (size: 36.2 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 30) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 40.0 (TID 30)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 40.0 (TID 30). 904 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 30) in 18 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.022 s
26/02/21 15:17:18 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
26/02/21 15:17:18 INFO DAGScheduler: Job 21 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.023681 s
26/02/21 15:17:18 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260221151714462
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:18 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:18 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/21 15:17:18 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/21 15:17:18 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:18 INFO HoodieActiveTimeline: Marking instant complete [==>20260221151714462__commit__INFLIGHT]
26/02/21 15:17:18 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260221151714462.commit
26/02/21 15:17:18 INFO HoodieActiveTimeline: Completed [==>20260221151714462__commit__INFLIGHT]
26/02/21 15:17:18 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/21 15:17:18 INFO DAGScheduler: Got job 22 (collectAsMap at HoodieSparkEngineContext.java:164) with 2 output partitions
26/02/21 15:17:18 INFO DAGScheduler: Final stage: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/21 15:17:18 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:18 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:18 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 101.3 KiB, free 432.8 MiB)
26/02/21 15:17:18 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.8 MiB)
26/02/21 15:17:18 INFO BlockManager: Removing RDD 72
26/02/21 15:17:18 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:33549 (size: 36.2 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1))
26/02/21 15:17:18 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks resource profile 0
26/02/21 15:17:18 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4415 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 32) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 4411 bytes) taskResourceAssignments Map()
26/02/21 15:17:18 INFO BlockManager: Removing RDD 82
26/02/21 15:17:18 INFO Executor: Running task 1.0 in stage 41.0 (TID 32)
26/02/21 15:17:18 INFO Executor: Running task 0.0 in stage 41.0 (TID 31)
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:33549 in memory (size: 36.1 KiB, free: 433.3 MiB)
26/02/21 15:17:18 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:33549 in memory (size: 36.2 KiB, free: 433.4 MiB)
26/02/21 15:17:18 INFO Executor: Finished task 0.0 in stage 41.0 (TID 31). 894 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 31) in 25 ms on spark-master (executor driver) (1/2)
26/02/21 15:17:18 INFO Executor: Finished task 1.0 in stage 41.0 (TID 32). 890 bytes result sent to driver
26/02/21 15:17:18 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 32) in 63 ms on spark-master (executor driver) (2/2)
26/02/21 15:17:18 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/21 15:17:18 INFO DAGScheduler: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.073 s
26/02/21 15:17:18 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/21 15:17:18 INFO DAGScheduler: Job 22 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.075209 s
26/02/21 15:17:18 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/.temp/20260221151714462
26/02/21 15:17:18 INFO BaseHoodieWriteClient: Committed 20260221151714462
26/02/21 15:17:18 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
26/02/21 15:17:18 INFO BlockManager: Removing RDD 53
26/02/21 15:17:18 INFO MapPartitionsRDD: Removing RDD 63 from persistence list
26/02/21 15:17:19 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/21 15:17:19 INFO BlockManager: Removing RDD 63
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/21 15:17:19 INFO FileSystemViewManager: Creating remote first table view
26/02/21 15:17:19 INFO BaseHoodieWriteClient: Cleaner started
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/21 15:17:19 INFO FileSystemViewManager: Creating remote first table view
26/02/21 15:17:19 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260221151719000
26/02/21 15:17:19 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:35267, Timeout=300
26/02/21 15:17:19 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:35267/v1/hoodie/view/compactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260221151714462&timelinehash=3852677411135d036c8b2cb6ffc71fb966d5ac0763079888464eab754ba08920)
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:35267/v1/hoodie/view/logcompactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260221151714462&timelinehash=3852677411135d036c8b2cb6ffc71fb966d5ac0763079888464eab754ba08920)
26/02/21 15:17:19 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/21 15:17:19 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/21 15:17:19 INFO FileSystemViewManager: Creating remote first table view
26/02/21 15:17:19 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/21 15:17:19 INFO HoodieTimelineArchiver: No Instants to archive
26/02/21 15:17:19 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:35267, Timeout=300
26/02/21 15:17:19 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:35267/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260221151714462&timelinehash=3852677411135d036c8b2cb6ffc71fb966d5ac0763079888464eab754ba08920)
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__deltacommit__COMPLETED__20260221151718819]}
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Commit 20260221151714462 successful!
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/21 15:17:19 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/21 15:17:19 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/21 15:17:19 INFO BaseHoodieClient: Stopping Timeline service !!
26/02/21 15:17:19 INFO EmbeddedTimelineService: Closing Timeline server
26/02/21 15:17:19 INFO TimelineService: Closing Timeline Service
26/02/21 15:17:19 INFO Javalin: Stopping Javalin ...
26/02/21 15:17:19 INFO Javalin: Javalin has stopped
26/02/21 15:17:19 INFO TimelineService: Closed Timeline Service
26/02/21 15:17:19 INFO EmbeddedTimelineService: Closed Timeline server
26/02/21 15:17:19 INFO DataSourceUtils: Getting table path..
26/02/21 15:17:19 INFO TablePathUtils: Getting table path from path : s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/21 15:17:19 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:19 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/21 15:17:19 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260221151714462__commit__COMPLETED__20260221151718889]}
26/02/21 15:17:19 INFO BaseHoodieTableFileIndex: Refresh table silver_cleaned_trips, spent: 6 ms
26/02/21 15:17:19 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/21 15:17:19 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/21 15:17:19 INFO DAGScheduler: Got job 23 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/21 15:17:19 INFO DAGScheduler: Final stage: ResultStage 42 (collect at HoodieSparkEngineContext.java:116)
26/02/21 15:17:19 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:19 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:19 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 101.1 KiB, free 433.9 MiB)
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 433.9 MiB)
26/02/21 15:17:19 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:33549 (size: 36.3 KiB, free: 434.3 MiB)
26/02/21 15:17:19 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:19 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
26/02/21 15:17:19 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4368 bytes) taskResourceAssignments Map()
26/02/21 15:17:19 INFO Executor: Running task 0.0 in stage 42.0 (TID 33)
26/02/21 15:17:19 INFO Executor: Finished task 0.0 in stage 42.0 (TID 33). 1131 bytes result sent to driver
26/02/21 15:17:19 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 6 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:19 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/21 15:17:19 INFO DAGScheduler: ResultStage 42 (collect at HoodieSparkEngineContext.java:116) finished in 0.011 s
26/02/21 15:17:19 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/21 15:17:19 INFO DAGScheduler: Job 23 finished: collect at HoodieSparkEngineContext.java:116, took 0.011992 s
26/02/21 15:17:19 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/21 15:17:19 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/21 15:17:19 INFO FileSourceStrategy: Pushed Filters: 
26/02/21 15:17:19 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/21 15:17:19 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 210.5 KiB, free 433.7 MiB)
26/02/21 15:17:19 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:33549 in memory (size: 36.3 KiB, free: 434.3 MiB)
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 433.8 MiB)
26/02/21 15:17:19 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:33549 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/21 15:17:19 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:33549 (size: 35.2 KiB, free: 434.3 MiB)
26/02/21 15:17:19 INFO SparkContext: Created broadcast 35 from count at <unknown>:0
26/02/21 15:17:19 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/21 15:17:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/21 15:17:19 INFO DAGScheduler: Registering RDD 97 (count at <unknown>:0) as input to shuffle 10
26/02/21 15:17:19 INFO DAGScheduler: Got map stage job 24 (count at <unknown>:0) with 1 output partitions
26/02/21 15:17:19 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (count at <unknown>:0)
26/02/21 15:17:19 INFO DAGScheduler: Parents of final stage: List()
26/02/21 15:17:19 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:19 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0), which has no missing parents
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.9 MiB)
26/02/21 15:17:19 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:33549 (size: 7.4 KiB, free: 434.3 MiB)
26/02/21 15:17:19 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:19 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
26/02/21 15:17:19 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()
26/02/21 15:17:19 INFO Executor: Running task 0.0 in stage 43.0 (TID 34)
26/02/21 15:17:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:33549 in memory (size: 36.1 KiB, free: 434.4 MiB)
26/02/21 15:17:19 INFO FileScanRDD: Reading File path: s3a://warehouse/silver/cleaned_trips/10ac6d70-892a-4e25-83fa-aa923f7e8bc7-0_0-27-21_20260221151714462.parquet, range: 0-1019915, partition values: [empty row]
26/02/21 15:17:19 INFO BlockManager: Removing RDD 36
26/02/21 15:17:19 INFO BlockManager: Removing RDD 53
26/02/21 15:17:19 INFO BlockManager: Removing RDD 63
26/02/21 15:17:19 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:19 INFO S3AInputStream: Switching to Random IO seek policy
26/02/21 15:17:19 INFO Executor: Finished task 0.0 in stage 43.0 (TID 34). 2056 bytes result sent to driver
26/02/21 15:17:19 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 34) in 30 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:19 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
26/02/21 15:17:19 INFO DAGScheduler: ShuffleMapStage 43 (count at <unknown>:0) finished in 0.037 s
26/02/21 15:17:19 INFO DAGScheduler: looking for newly runnable stages
26/02/21 15:17:19 INFO DAGScheduler: running: Set()
26/02/21 15:17:19 INFO DAGScheduler: waiting: Set()
26/02/21 15:17:19 INFO DAGScheduler: failed: Set()
26/02/21 15:17:19 INFO SparkContext: Starting job: count at <unknown>:0
26/02/21 15:17:19 INFO DAGScheduler: Got job 25 (count at <unknown>:0) with 1 output partitions
26/02/21 15:17:19 INFO DAGScheduler: Final stage: ResultStage 45 (count at <unknown>:0)
26/02/21 15:17:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
26/02/21 15:17:19 INFO DAGScheduler: Missing parents: List()
26/02/21 15:17:19 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0), which has no missing parents
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/21 15:17:19 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/21 15:17:19 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:33549 (size: 5.5 KiB, free: 434.4 MiB)
26/02/21 15:17:19 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:33549 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/21 15:17:19 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1509
26/02/21 15:17:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/21 15:17:19 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
26/02/21 15:17:19 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/21 15:17:19 INFO Executor: Running task 0.0 in stage 45.0 (TID 35)
26/02/21 15:17:19 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/21 15:17:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/21 15:17:19 INFO Executor: Finished task 0.0 in stage 45.0 (TID 35). 2458 bytes result sent to driver
26/02/21 15:17:19 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 4 ms on spark-master (executor driver) (1/1)
26/02/21 15:17:19 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/21 15:17:19 INFO DAGScheduler: ResultStage 45 (count at <unknown>:0) finished in 0.012 s
26/02/21 15:17:19 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/21 15:17:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/21 15:17:19 INFO DAGScheduler: Job 25 finished: count at <unknown>:0, took 0.012604 s
Silver transform complete. Rows written: 9,855
Rows filtered out: 145
============================================================
  Silver Transform (Hudi Upsert) COMPLETE
============================================================
26/02/21 15:17:19 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/21 15:17:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/21 15:17:19 INFO MemoryStore: MemoryStore cleared
26/02/21 15:17:19 INFO BlockManager: BlockManager stopped
26/02/21 15:17:19 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/21 15:17:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/21 15:17:19 INFO SparkContext: Successfully stopped SparkContext
26/02/21 15:17:19 INFO ShutdownHookManager: Shutdown hook called
26/02/21 15:17:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-42e910d8-9e2a-4dfd-adb7-e5ad56d04430
26/02/21 15:17:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-56f596e7-8c1f-44e4-9f97-143cfff33118
26/02/21 15:17:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-56f596e7-8c1f-44e4-9f97-143cfff33118/pyspark-bf6e2477-dd1f-4fe3-9fac-5c0b0e0e0785
26/02/21 15:17:19 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
26/02/21 15:17:19 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
26/02/21 15:17:19 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
Silver transform complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir ."
 Container p22-minio  Running
[0m15:17:22  Running with dbt=1.11.5
[0m15:17:22  Installing dbt-labs/dbt_utils
[0m15:17:25  Installed from version 1.3.3
[0m15:17:25  Up to date!
[0m15:17:26  Running with dbt=1.11.5
[0m15:17:27  Registered adapter: duckdb=1.10.0
[0m15:17:27  Found 14 models, 74 data tests, 3 seeds, 1 source, 606 macros
[0m15:17:27  
[0m15:17:27  Concurrency: 4 threads (target='dev')
[0m15:17:27  
[0m15:17:28  1 of 91 START sql table model main.dim_dates ................................... [RUN]
[0m15:17:28  2 of 91 START sql view model main.stg_yellow_trips ............................. [RUN]
[0m15:17:28  3 of 91 START seed file main_main.payment_type_lookup .......................... [RUN]
[0m15:17:28  4 of 91 START seed file main_main.rate_code_lookup ............................. [RUN]
[0m15:17:29  4 of 91 OK loaded seed file main_main.rate_code_lookup ......................... [[32mCREATE 7[0m in 0.14s]
[0m15:17:29  3 of 91 OK loaded seed file main_main.payment_type_lookup ...................... [[32mCREATE 6[0m in 0.14s]
[0m15:17:29  5 of 91 START seed file main_main.taxi_zone_lookup ............................. [RUN]
[0m15:17:29  6 of 91 START test not_null_rate_code_lookup_rate_code_id ...................... [RUN]
[0m15:17:29  2 of 91 ERROR creating sql view model main.stg_yellow_trips .................... [[31mERROR[0m in 0.15s]
[0m15:17:29  7 of 91 START test unique_rate_code_lookup_rate_code_id ........................ [RUN]
[0m15:17:29  1 of 91 OK created sql table model main.dim_dates .............................. [[32mOK[0m in 0.19s]
[0m15:17:29  8 of 91 START test not_null_payment_type_lookup_payment_type_id ................ [RUN]
[0m15:17:29  6 of 91 PASS not_null_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.06s]
[0m15:17:29  9 of 91 START test unique_payment_type_lookup_payment_type_id .................. [RUN]
[0m15:17:29  7 of 91 PASS unique_rate_code_lookup_rate_code_id .............................. [[32mPASS[0m in 0.05s]
[0m15:17:29  10 of 91 SKIP test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[33mSKIP[0m]
[0m15:17:29  5 of 91 OK loaded seed file main_main.taxi_zone_lookup ......................... [[32mCREATE 265[0m in 0.08s]
[0m15:17:29  11 of 91 SKIP test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[33mSKIP[0m]
[0m15:17:29  12 of 91 SKIP test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ......... [[33mSKIP[0m]
[0m15:17:29  13 of 91 SKIP test assert_fare_not_exceeds_total ............................... [[33mSKIP[0m]
[0m15:17:29  14 of 91 SKIP test not_null_stg_yellow_trips_dropoff_datetime .................. [[33mSKIP[0m]
[0m15:17:29  15 of 91 SKIP test not_null_stg_yellow_trips_dropoff_location_id ............... [[33mSKIP[0m]
[0m15:17:29  16 of 91 SKIP test not_null_stg_yellow_trips_fare_amount ....................... [[33mSKIP[0m]
[0m15:17:29  17 of 91 SKIP test not_null_stg_yellow_trips_pickup_datetime ................... [[33mSKIP[0m]
[0m15:17:29  18 of 91 SKIP test not_null_stg_yellow_trips_pickup_location_id ................ [[33mSKIP[0m]
[0m15:17:29  19 of 91 SKIP test not_null_stg_yellow_trips_total_amount ...................... [[33mSKIP[0m]
[0m15:17:29  20 of 91 SKIP test not_null_stg_yellow_trips_trip_distance_miles ............... [[33mSKIP[0m]
[0m15:17:29  21 of 91 SKIP test not_null_stg_yellow_trips_trip_id ........................... [[33mSKIP[0m]
[0m15:17:29  8 of 91 PASS not_null_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.05s]
[0m15:17:29  22 of 91 SKIP test not_null_stg_yellow_trips_vendor_id ......................... [[33mSKIP[0m]
[0m15:17:29  23 of 91 SKIP test unique_stg_yellow_trips_trip_id ............................. [[33mSKIP[0m]
[0m15:17:29  24 of 91 START test not_null_dim_dates_date_key ................................ [RUN]
[0m15:17:29  25 of 91 START test not_null_dim_dates_day_of_week_name ........................ [RUN]
[0m15:17:29  26 of 91 START test not_null_dim_dates_is_holiday .............................. [RUN]
[0m15:17:29  9 of 91 PASS unique_payment_type_lookup_payment_type_id ........................ [[32mPASS[0m in 0.05s]
[0m15:17:29  27 of 91 START test not_null_dim_dates_is_weekend .............................. [RUN]
[0m15:17:29  25 of 91 PASS not_null_dim_dates_day_of_week_name .............................. [[32mPASS[0m in 0.04s]
[0m15:17:29  24 of 91 PASS not_null_dim_dates_date_key ...................................... [[32mPASS[0m in 0.05s]
[0m15:17:29  26 of 91 PASS not_null_dim_dates_is_holiday .................................... [[32mPASS[0m in 0.04s]
[0m15:17:29  28 of 91 START test unique_dim_dates_date_key .................................. [RUN]
[0m15:17:29  29 of 91 START sql view model main.stg_rate_codes .............................. [RUN]
[0m15:17:29  30 of 91 START test not_null_taxi_zone_lookup_Borough .......................... [RUN]
[0m15:17:29  27 of 91 PASS not_null_dim_dates_is_weekend .................................... [[32mPASS[0m in 0.04s]
[0m15:17:29  31 of 91 START test not_null_taxi_zone_lookup_LocationID ....................... [RUN]
[0m15:17:29  31 of 91 PASS not_null_taxi_zone_lookup_LocationID ............................. [[32mPASS[0m in 0.04s]
[0m15:17:29  30 of 91 PASS not_null_taxi_zone_lookup_Borough ................................ [[32mPASS[0m in 0.05s]
[0m15:17:29  32 of 91 START test unique_taxi_zone_lookup_LocationID ......................... [RUN]
[0m15:17:29  33 of 91 SKIP relation main.int_trip_metrics ................................... [[33mSKIP[0m]
[0m15:17:29  28 of 91 PASS unique_dim_dates_date_key ........................................ [[32mPASS[0m in 0.06s]
[0m15:17:29  34 of 91 START sql view model main.stg_payment_types ........................... [RUN]
[0m15:17:29  35 of 91 SKIP test assert_trip_duration_positive ............................... [[33mSKIP[0m]
[0m15:17:29  36 of 91 SKIP test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [[33mSKIP[0m]
[0m15:17:29  37 of 91 SKIP test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[33mSKIP[0m]
[0m15:17:29  38 of 91 SKIP test not_null_int_trip_metrics_is_weekend ........................ [[33mSKIP[0m]
[0m15:17:29  29 of 91 OK created sql view model main.stg_rate_codes ......................... [[32mOK[0m in 0.07s]
[0m15:17:29  39 of 91 SKIP test not_null_int_trip_metrics_pickup_date ....................... [[33mSKIP[0m]
[0m15:17:29  40 of 91 SKIP test not_null_int_trip_metrics_pickup_hour ....................... [[33mSKIP[0m]
[0m15:17:29  41 of 91 SKIP test not_null_int_trip_metrics_trip_duration_minutes ............. [[33mSKIP[0m]
[0m15:17:29  42 of 91 SKIP test not_null_int_trip_metrics_trip_id ........................... [[33mSKIP[0m]
[0m15:17:29  43 of 91 START test not_null_stg_rate_codes_rate_code_id ....................... [RUN]
[0m15:17:29  44 of 91 START test not_null_stg_rate_codes_rate_code_name ..................... [RUN]
[0m15:17:29  32 of 91 PASS unique_taxi_zone_lookup_LocationID ............................... [[32mPASS[0m in 0.05s]
[0m15:17:29  45 of 91 START test unique_stg_rate_codes_rate_code_id ......................... [RUN]
[0m15:17:29  34 of 91 OK created sql view model main.stg_payment_types ...................... [[32mOK[0m in 0.05s]
[0m15:17:29  46 of 91 SKIP relation main.int_daily_summary .................................. [[33mSKIP[0m]
[0m15:17:29  47 of 91 SKIP relation main.int_hourly_patterns ................................ [[33mSKIP[0m]
[0m15:17:29  48 of 91 START sql view model main.stg_taxi_zones .............................. [RUN]
[0m15:17:29  43 of 91 PASS not_null_stg_rate_codes_rate_code_id ............................. [[32mPASS[0m in 0.05s]
[0m15:17:29  44 of 91 PASS not_null_stg_rate_codes_rate_code_name ........................... [[32mPASS[0m in 0.05s]
[0m15:17:29  49 of 91 START test not_null_stg_payment_types_payment_type_id ................. [RUN]
[0m15:17:29  50 of 91 START test not_null_stg_payment_types_payment_type_name ............... [RUN]
[0m15:17:29  45 of 91 PASS unique_stg_rate_codes_rate_code_id ............................... [[32mPASS[0m in 0.05s]
[0m15:17:29  51 of 91 START test unique_stg_payment_types_payment_type_id ................... [RUN]
[0m15:17:29  49 of 91 PASS not_null_stg_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.04s]
[0m15:17:29  50 of 91 PASS not_null_stg_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.04s]
[0m15:17:29  52 of 91 SKIP test dbt_utils_accepted_range_int_daily_summary_total_trips__0 ... [[33mSKIP[0m]
[0m15:17:29  53 of 91 SKIP test not_null_int_daily_summary_pickup_date ...................... [[33mSKIP[0m]
[0m15:17:29  54 of 91 SKIP test not_null_int_daily_summary_total_revenue .................... [[33mSKIP[0m]
[0m15:17:29  55 of 91 SKIP test not_null_int_daily_summary_total_trips ...................... [[33mSKIP[0m]
[0m15:17:29  48 of 91 OK created sql view model main.stg_taxi_zones ......................... [[32mOK[0m in 0.06s]
[0m15:17:29  56 of 91 SKIP test unique_int_daily_summary_pickup_date ........................ [[33mSKIP[0m]
[0m15:17:29  57 of 91 SKIP test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [[33mSKIP[0m]
[0m15:17:29  58 of 91 SKIP test not_null_int_hourly_patterns_pickup_date .................... [[33mSKIP[0m]
[0m15:17:29  59 of 91 SKIP test not_null_int_hourly_patterns_pickup_hour .................... [[33mSKIP[0m]
[0m15:17:29  60 of 91 SKIP test not_null_int_hourly_patterns_total_trips .................... [[33mSKIP[0m]
[0m15:17:29  61 of 91 START test not_null_stg_taxi_zones_borough ............................ [RUN]
[0m15:17:29  62 of 91 START test not_null_stg_taxi_zones_location_id ........................ [RUN]
[0m15:17:29  63 of 91 START test not_null_stg_taxi_zones_zone_name .......................... [RUN]
[0m15:17:29  51 of 91 PASS unique_stg_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m15:17:29  64 of 91 SKIP test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[33mSKIP[0m]
[0m15:17:29  65 of 91 SKIP test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[33mSKIP[0m]
[0m15:17:29  66 of 91 START test unique_stg_taxi_zones_location_id .......................... [RUN]
[0m15:17:29  63 of 91 PASS not_null_stg_taxi_zones_zone_name ................................ [[32mPASS[0m in 0.04s]
[0m15:17:29  61 of 91 PASS not_null_stg_taxi_zones_borough .................................. [[32mPASS[0m in 0.05s]
[0m15:17:29  62 of 91 PASS not_null_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.05s]
[0m15:17:29  67 of 91 SKIP relation main.mart_daily_revenue ................................. [[33mSKIP[0m]
[0m15:17:29  66 of 91 PASS unique_stg_taxi_zones_location_id ................................ [[32mPASS[0m in 0.04s]
[0m15:17:29  68 of 91 SKIP relation main.mart_hourly_demand ................................. [[33mSKIP[0m]
[0m15:17:29  69 of 91 START sql table model main.dim_payment_types .......................... [RUN]
[0m15:17:29  70 of 91 SKIP test not_null_mart_daily_revenue_date_key ........................ [[33mSKIP[0m]
[0m15:17:29  71 of 91 SKIP test not_null_mart_daily_revenue_total_revenue ................... [[33mSKIP[0m]
[0m15:17:29  72 of 91 SKIP test unique_mart_daily_revenue_date_key .......................... [[33mSKIP[0m]
[0m15:17:29  73 of 91 START sql table model main.dim_locations .............................. [RUN]
[0m15:17:29  74 of 91 SKIP test not_null_mart_hourly_demand_is_weekend ...................... [[33mSKIP[0m]
[0m15:17:29  75 of 91 SKIP test not_null_mart_hourly_demand_pickup_hour ..................... [[33mSKIP[0m]
[0m15:17:29  69 of 91 OK created sql table model main.dim_payment_types ..................... [[32mOK[0m in 0.06s]
[0m15:17:29  73 of 91 OK created sql table model main.dim_locations ......................... [[32mOK[0m in 0.06s]
[0m15:17:29  76 of 91 START test not_null_dim_payment_types_payment_type_id ................. [RUN]
[0m15:17:29  77 of 91 START test not_null_dim_payment_types_payment_type_name ............... [RUN]
[0m15:17:29  78 of 91 START test unique_dim_payment_types_payment_type_id ................... [RUN]
[0m15:17:29  79 of 91 START test not_null_dim_locations_borough ............................. [RUN]
[0m15:17:29  77 of 91 PASS not_null_dim_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m15:17:29  80 of 91 START test not_null_dim_locations_location_id ......................... [RUN]
[0m15:17:29  78 of 91 PASS unique_dim_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.06s]
[0m15:17:29  76 of 91 PASS not_null_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m15:17:29  81 of 91 START test not_null_dim_locations_zone_name ........................... [RUN]
[0m15:17:29  79 of 91 PASS not_null_dim_locations_borough ................................... [[32mPASS[0m in 0.06s]
[0m15:17:29  82 of 91 START test unique_dim_locations_location_id ........................... [RUN]
[0m15:17:29  80 of 91 PASS not_null_dim_locations_location_id ............................... [[32mPASS[0m in 0.04s]
[0m15:17:29  81 of 91 PASS not_null_dim_locations_zone_name ................................. [[32mPASS[0m in 0.04s]
[0m15:17:29  82 of 91 PASS unique_dim_locations_location_id ................................. [[32mPASS[0m in 0.03s]
[0m15:17:29  83 of 91 SKIP relation main.fct_trips .......................................... [[33mSKIP[0m]
[0m15:17:29  84 of 91 SKIP test not_null_fct_trips_pickup_datetime .......................... [[33mSKIP[0m]
[0m15:17:29  85 of 91 SKIP test not_null_fct_trips_total_amount ............................. [[33mSKIP[0m]
[0m15:17:29  86 of 91 SKIP test not_null_fct_trips_trip_id .................................. [[33mSKIP[0m]
[0m15:17:29  87 of 91 SKIP test unique_fct_trips_trip_id .................................... [[33mSKIP[0m]
[0m15:17:29  88 of 91 SKIP relation main.mart_location_performance .......................... [[33mSKIP[0m]
[0m15:17:29  89 of 91 SKIP test not_null_mart_location_performance_pickup_location_id ....... [[33mSKIP[0m]
[0m15:17:29  90 of 91 SKIP test not_null_mart_location_performance_total_pickups ............ [[33mSKIP[0m]
[0m15:17:29  91 of 91 SKIP test unique_mart_location_performance_pickup_location_id ......... [[33mSKIP[0m]
[0m15:17:29  
[0m15:17:29  Finished running 1 incremental model, 3 seeds, 6 table models, 74 data tests, 7 view models in 0 hours 0 minutes and 1.73 seconds (1.73s).
[0m15:17:29  
[0m15:17:29  [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:17:29  
[0m15:17:29  [31mFailure in model stg_yellow_trips (models/staging/stg_yellow_trips.sql)[0m
[0m15:17:29    Runtime Error in model stg_yellow_trips (models/staging/stg_yellow_trips.sql)
  Binder Error: Referenced column "tpep_pickup_datetime" not found in FROM clause!
  Candidate bindings: "pickup_datetime", "trip_distance_miles", "pickup_location_id", "rate_code_id", "_hoodie_file_name"
  
  LINE 44:     where tpep_pickup_datetime is not null
                     ^
[0m15:17:29  
[0m15:17:29    compiled code at target/compiled/nyc_taxi_pipeline_22/models/staging/stg_yellow_trips.sql
[0m15:17:29  
[0m15:17:29  Done. PASS=38 WARN=0 ERROR=1 SKIP=52 NO-OP=0 TOTAL=91

make[1]: *** [Makefile:68: dbt-build] Error 1
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make: *** [Makefile:75: benchmark] Error 2
docker compose --profile generator --profile dbt down -v --remove-orphans
 Container p22-spark-worker  Stopping
 Container p22-spark-worker  Stopped
 Container p22-spark-worker  Removing
 Container p22-spark-worker  Removed
 Container p22-spark-master  Stopping
 Container p22-spark-master  Stopped
 Container p22-spark-master  Removing
 Container p22-spark-master  Removed
 Container p22-kafka  Stopping
 Container p22-mc-init  Stopping
 Container p22-mc-init  Stopped
 Container p22-mc-init  Removing
 Container p22-mc-init  Removed
 Container p22-minio  Stopping
 Container p22-minio  Stopped
 Container p22-minio  Removing
 Container p22-minio  Removed
 Container p22-kafka  Stopped
 Container p22-kafka  Removing
 Container p22-kafka  Removed
 Volume 22-hudi-cdc-storage_minio-data  Removing
 Network p22-pipeline-net  Removing
 Volume 22-hudi-cdc-storage_minio-data  Removed
 Network p22-pipeline-net  Removed
Pipeline 22 stopped.
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
Pipeline 22 cleaned.
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-minio  Creating
 Container p22-kafka  Creating
 Container p22-kafka  Created
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-minio  Starting
 Container p22-kafka  Starting
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-kafka  Started
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-kafka  Waiting
 Container p22-mc-init  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
============================================================
  Pipeline 22 Benchmark: Kafka + Spark + Apache Hudi
============================================================
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-minio  Creating
 Container p22-kafka  Creating
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-kafka  Created
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-minio  Starting
 Container p22-kafka  Starting
 Container p22-kafka  Started
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-kafka  Waiting
 Container p22-mc-init  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[2]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[2]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm \
	-e MODE=burst \
	-e MAX_EVENTS=10000 \
	data-generator
 Container p22-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.37s
  Rate:    27,250 events/sec
============================================================
  Metrics written to /tmp/generator_metrics.json
Limited data generation complete (10k events).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/bronze_ingest.py
26/02/22 12:31:48 INFO SparkContext: Running Spark version 3.3.3
26/02/22 12:31:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/22 12:31:48 INFO ResourceUtils: ==============================================================
26/02/22 12:31:48 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/22 12:31:48 INFO ResourceUtils: ==============================================================
26/02/22 12:31:48 INFO SparkContext: Submitted application: P22-Bronze-Ingest-Hudi
26/02/22 12:31:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/22 12:31:48 INFO ResourceProfile: Limiting resource is cpu
26/02/22 12:31:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/22 12:31:48 INFO SecurityManager: Changing view acls to: spark
26/02/22 12:31:48 INFO SecurityManager: Changing modify acls to: spark
26/02/22 12:31:48 INFO SecurityManager: Changing view acls groups to: 
26/02/22 12:31:48 INFO SecurityManager: Changing modify acls groups to: 
26/02/22 12:31:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/22 12:31:49 INFO Utils: Successfully started service 'sparkDriver' on port 39469.
26/02/22 12:31:49 INFO SparkEnv: Registering MapOutputTracker
26/02/22 12:31:49 INFO SparkEnv: Registering BlockManagerMaster
26/02/22 12:31:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/22 12:31:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/22 12:31:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/22 12:31:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-14bd9599-9f0e-4157-b45c-94cf81dc58ae
26/02/22 12:31:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/22 12:31:49 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/22 12:31:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/22 12:31:49 INFO Executor: Starting executor ID driver on host spark-master
26/02/22 12:31:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/22 12:31:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41593.
26/02/22 12:31:49 INFO NettyBlockTransferService: Server created on spark-master:41593
26/02/22 12:31:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/22 12:31:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 41593, None)
26/02/22 12:31:49 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:41593 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 41593, None)
26/02/22 12:31:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 41593, None)
26/02/22 12:31:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 41593, None)
26/02/22 12:31:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/22 12:31:49 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/22 12:31:51 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/22 12:31:51 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:31:51 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/22 12:31:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:51 WARN HoodieStreamingSink: Ignore TableNotFoundException as it is first microbatch.
26/02/22 12:31:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/22 12:31:52 INFO ResolveWriteToStream: Checkpoint root s3a://warehouse/checkpoints/bronze resolved to s3a://warehouse/checkpoints/bronze.
26/02/22 12:31:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/22 12:31:52 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
26/02/22 12:31:52 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/metadata using temp file s3a://warehouse/checkpoints/bronze/.metadata.6061a777-86d7-4701-b5b7-5edbb9cbe9e1.tmp
26/02/22 12:31:52 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/.metadata.6061a777-86d7-4701-b5b7-5edbb9cbe9e1.tmp to s3a://warehouse/checkpoints/bronze/metadata
26/02/22 12:31:52 INFO MicroBatchExecution: Starting [id = 71332e9e-d3e0-4b36-9dc5-dd1841a9db0c, runId = 6cf9fe99-f3ba-4c77-b653-dc1ecaa81628]. Use s3a://warehouse/checkpoints/bronze to store the query checkpoint.
26/02/22 12:31:52 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@699661e5] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7acbac97]
26/02/22 12:31:52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:31:52 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:31:52 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:31:52 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:31:52 INFO AppInfoParser: Kafka startTimeMs: 1771763512563
26/02/22 12:31:52 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Subscribed to topic(s): taxi.raw_trips
26/02/22 12:31:52 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:31:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] (Re-)joining group
26/02/22 12:31:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:31:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:31:52 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:31:53 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] (Re-)joining group
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:31:53 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] (Re-)joining group
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:31:53 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] (Re-)joining group
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Request joining group due to: need to re-join with the given member-id: consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1-619966dc-0417-41c0-9cec-4ac5111d0b37
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] (Re-)joining group
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1-619966dc-0417-41c0-9cec-4ac5111d0b37', protocol='range'}
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1-619966dc-0417-41c0-9cec-4ac5111d0b37=Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])}
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1-619966dc-0417-41c0-9cec-4ac5111d0b37', protocol='range'}
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Notifying assignor about the new Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])
26/02/22 12:31:53 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Adding newly assigned partitions: taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-0
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-1
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-2
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-3
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-4
26/02/22 12:31:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-5
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/sources/0/0 using temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.79e7f495-b5cc-4a9d-940c-6db08004a877.tmp
26/02/22 12:31:53 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.79e7f495-b5cc-4a9d-940c-6db08004a877.tmp to s3a://warehouse/checkpoints/bronze/sources/0/0
26/02/22 12:31:53 INFO KafkaMicroBatchStream: Initial offsets: {"taxi.raw_trips":{"2":0,"5":0,"4":0,"1":0,"3":0,"0":0}}
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:53 INFO MicroBatchExecution: Starting new streaming query.
26/02/22 12:31:53 INFO MicroBatchExecution: Stream started from {}
26/02/22 12:31:53 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/offsets/0 using temp file s3a://warehouse/checkpoints/bronze/offsets/.0.ed62f81e-4cb0-468e-b40c-8ea5eb97ce9e.tmp
26/02/22 12:31:53 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/offsets/.0.ed62f81e-4cb0-468e-b40c-8ea5eb97ce9e.tmp to s3a://warehouse/checkpoints/bronze/offsets/0
26/02/22 12:31:53 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1771763513696,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
26/02/22 12:31:53 INFO IncrementalExecution: Current batch timestamp = 1771763513696
26/02/22 12:31:53 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:31:53 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:31:54 INFO IncrementalExecution: Current batch timestamp = 1771763513696
26/02/22 12:31:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:31:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:31:54 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/22 12:31:54 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/22 12:31:54 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips as hoodie table s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:31:54 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark
26/02/22 12:31:54 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:31:54 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:31:54 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:31:54 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:31:54 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:54 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:54 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123154225 action: commit
26/02/22 12:31:54 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123154225__commit__REQUESTED]
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123154225__commit__REQUESTED__20260222123154610]}
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:54 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/22 12:31:54 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123154225__commit__REQUESTED__20260222123154610]}
26/02/22 12:31:54 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips/.hoodie/metadata as hoodie table s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:54 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:54 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:54 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:54 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:54 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:54 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
26/02/22 12:31:54 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:54 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.5 KiB, free 434.3 MiB)
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/22 12:31:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:41593 (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:31:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/22 12:31:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4401 bytes) taskResourceAssignments Map()
26/02/22 12:31:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/22 12:31:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 926 bytes result sent to driver
26/02/22 12:31:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 95 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/22 12:31:55 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 0.328 s
26/02/22 12:31:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/22 12:31:55 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 0.349840 s
26/02/22 12:31:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:31:55 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/22 12:31:55 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/22 12:31:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:55 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:55 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
26/02/22 12:31:55 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:55 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:55 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KiB, free 434.3 MiB)
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.3 MiB)
26/02/22 12:31:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:41593 (size: 1866.0 B, free: 434.4 MiB)
26/02/22 12:31:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/22 12:31:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:31:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/22 12:31:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 803 bytes result sent to driver
26/02/22 12:31:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/22 12:31:55 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 0.018 s
26/02/22 12:31:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/22 12:31:55 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 0.019946 s
26/02/22 12:31:55 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/22 12:31:55 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/22 12:31:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:55 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:55 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
26/02/22 12:31:55 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:55 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:55 INFO DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 393.8 KiB, free 433.9 MiB)
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 129.8 KiB, free 433.7 MiB)
26/02/22 12:31:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:41593 (size: 129.8 KiB, free: 434.2 MiB)
26/02/22 12:31:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/22 12:31:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/22 12:31:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
26/02/22 12:31:55 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:31:55 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:31:55 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/22 12:31:55 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/22 12:31:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 803 bytes result sent to driver
26/02/22 12:31:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 65 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/22 12:31:55 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 0.091 s
26/02/22 12:31:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
26/02/22 12:31:55 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 0.093590 s
26/02/22 12:31:55 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:55 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:41593 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:31:55 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:31:55 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:31:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:41593 in memory (size: 1866.0 B, free: 434.3 MiB)
26/02/22 12:31:55 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:31:55 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:31:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:55 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:55 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:55 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:31:55 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:31:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:55 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:55 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:55 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/22 12:31:55 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:55 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:55 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260222123155452]}
26/02/22 12:31:55 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:55 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:55 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:55 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:31:55 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:31:55 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/22 12:31:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:55 INFO DAGScheduler: Registering RDD 14 (start at <unknown>:0) as input to shuffle 0
26/02/22 12:31:55 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:55 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
26/02/22 12:31:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/22 12:31:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
26/02/22 12:31:55 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.5 KiB, free 433.9 MiB)
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.9 MiB)
26/02/22 12:31:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:41593 (size: 4.7 KiB, free: 434.3 MiB)
26/02/22 12:31:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/22 12:31:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/22 12:31:55 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
26/02/22 12:31:55 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 964 bytes result sent to driver
26/02/22 12:31:55 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:55 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/22 12:31:55 INFO DAGScheduler: ShuffleMapStage 3 (start at <unknown>:0) finished in 0.049 s
26/02/22 12:31:55 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:31:55 INFO DAGScheduler: running: Set()
26/02/22 12:31:55 INFO DAGScheduler: waiting: Set(ResultStage 4)
26/02/22 12:31:55 INFO DAGScheduler: failed: Set()
26/02/22 12:31:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.3 KiB, free 433.9 MiB)
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.9 MiB)
26/02/22 12:31:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:41593 (size: 3.9 KiB, free: 434.3 MiB)
26/02/22 12:31:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:55 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/22 12:31:55 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:55 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
26/02/22 12:31:55 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
26/02/22 12:31:55 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1246 bytes result sent to driver
26/02/22 12:31:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 46 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:55 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/22 12:31:55 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.051 s
26/02/22 12:31:55 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/22 12:31:55 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.124602 s
26/02/22 12:31:55 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:31:55 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/22 12:31:55 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:55 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:55 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
26/02/22 12:31:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
26/02/22 12:31:55 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:55 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 267.1 KiB, free 433.6 MiB)
26/02/22 12:31:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 433.5 MiB)
26/02/22 12:31:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:41593 (size: 91.4 KiB, free: 434.2 MiB)
26/02/22 12:31:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:55 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/22 12:31:55 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:55 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/22 12:31:55 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:55 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:31:55 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE
26/02/22 12:31:55 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE in 29 ms
26/02/22 12:31:55 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/22 12:31:55 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:31:55 INFO MetricsSystemImpl: HBase metrics system started
26/02/22 12:31:55 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/22 12:31:55 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:41593 in memory (size: 129.8 KiB, free: 434.3 MiB)
26/02/22 12:31:55 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:41593 in memory (size: 3.9 KiB, free: 434.3 MiB)
26/02/22 12:31:55 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:31:55 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:31:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:41593 in memory (size: 4.7 KiB, free: 434.3 MiB)
26/02/22 12:31:55 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/22 12:31:55 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/22 12:31:56 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 290 ms.
26/02/22 12:31:56 INFO MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 271.0 B, free 434.0 MiB)
26/02/22 12:31:56 INFO BlockManagerInfo: Added rdd_19_0 in memory on spark-master:41593 (size: 271.0 B, free: 434.3 MiB)
26/02/22 12:31:56 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 1481 bytes result sent to driver
26/02/22 12:31:56 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 323 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:56 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/22 12:31:56 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.335 s
26/02/22 12:31:56 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/22 12:31:56 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.339702 s
26/02/22 12:31:56 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:31:56 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/22 12:31:56 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:56 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:56 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
26/02/22 12:31:56 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:56 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:56 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:56 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 101.2 KiB, free 434.0 MiB)
26/02/22 12:31:56 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.9 MiB)
26/02/22 12:31:56 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:41593 (size: 36.1 KiB, free: 434.3 MiB)
26/02/22 12:31:56 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:56 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/22 12:31:56 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/22 12:31:56 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 854 bytes result sent to driver
26/02/22 12:31:56 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 12 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:56 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/22 12:31:56 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.021 s
26/02/22 12:31:56 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/22 12:31:56 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.022358 s
26/02/22 12:31:56 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:31:56 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/22 12:31:56 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:31:56 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/22 12:31:56 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:56 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:56 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
26/02/22 12:31:56 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:56 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:56 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:56 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 101.3 KiB, free 433.8 MiB)
26/02/22 12:31:56 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.8 MiB)
26/02/22 12:31:56 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:41593 (size: 36.2 KiB, free: 434.2 MiB)
26/02/22 12:31:56 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/22 12:31:56 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/22 12:31:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 900 bytes result sent to driver
26/02/22 12:31:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 26 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/22 12:31:56 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.034 s
26/02/22 12:31:56 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/22 12:31:56 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.036111 s
26/02/22 12:31:56 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: MDT s3a://warehouse/bronze/raw_trips partition FILES has been enabled
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:56 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 1002 in ms
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/22 12:31:56 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:31:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:56 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:31:56 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123154225__commit__REQUESTED__20260222123154610]}
26/02/22 12:31:56 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/22 12:31:56 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123154225__commit__REQUESTED__20260222123154610]}
26/02/22 12:31:56 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:31:56 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:56 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:56 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:56 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:56 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:56 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:56 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:31:56 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:31:56 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:56 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:56 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:56 INFO DAGScheduler: Registering RDD 26 (start at <unknown>:0) as input to shuffle 1
26/02/22 12:31:56 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 6 output partitions
26/02/22 12:31:56 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
26/02/22 12:31:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/22 12:31:56 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
26/02/22 12:31:56 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:56 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 90.9 KiB, free 433.7 MiB)
26/02/22 12:31:56 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 433.7 MiB)
26/02/22 12:31:56 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:41593 (size: 31.0 KiB, free: 434.2 MiB)
26/02/22 12:31:56 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:31:56 INFO TaskSchedulerImpl: Adding task set 9.0 with 6 tasks resource profile 0
26/02/22 12:31:56 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 9) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 10) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 11) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 12) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 13) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:56 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
26/02/22 12:31:56 INFO Executor: Running task 1.0 in stage 9.0 (TID 9)
26/02/22 12:31:56 INFO Executor: Running task 2.0 in stage 9.0 (TID 10)
26/02/22 12:31:56 INFO Executor: Running task 4.0 in stage 9.0 (TID 12)
26/02/22 12:31:56 INFO Executor: Running task 5.0 in stage 9.0 (TID 13)
26/02/22 12:31:56 INFO Executor: Running task 3.0 in stage 9.0 (TID 11)
26/02/22 12:31:56 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:41593 in memory (size: 36.1 KiB, free: 434.2 MiB)
26/02/22 12:31:56 INFO BlockManager: Removing RDD 19
26/02/22 12:31:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:41593 in memory (size: 91.4 KiB, free: 434.3 MiB)
26/02/22 12:31:56 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:41593 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:31:56 INFO CodeGenerator: Code generated in 115.922471 ms
26/02/22 12:31:56 INFO CodeGenerator: Code generated in 9.963746 ms
26/02/22 12:31:56 INFO CodeGenerator: Code generated in 11.137822 ms
26/02/22 12:31:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:31:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:31:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:31:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:31:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:31:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:31:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:31:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:31:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:31:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:31:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:31:56 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:31:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:31:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:31:56 INFO AppInfoParser: Kafka startTimeMs: 1771763516852
26/02/22 12:31:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:31:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Assigned to partition(s): taxi.raw_trips-4
26/02/22 12:31:56 INFO AppInfoParser: Kafka startTimeMs: 1771763516852
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Assigned to partition(s): taxi.raw_trips-5
26/02/22 12:31:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:31:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:31:56 INFO AppInfoParser: Kafka startTimeMs: 1771763516853
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Assigned to partition(s): taxi.raw_trips-3
26/02/22 12:31:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:31:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:31:56 INFO AppInfoParser: Kafka startTimeMs: 1771763516854
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Assigned to partition(s): taxi.raw_trips-1
26/02/22 12:31:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:31:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:31:56 INFO AppInfoParser: Kafka startTimeMs: 1771763516856
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Assigned to partition(s): taxi.raw_trips-2
26/02/22 12:31:56 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:31:56 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:31:56 INFO AppInfoParser: Kafka startTimeMs: 1771763516856
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Assigned to partition(s): taxi.raw_trips-0
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-3
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-0
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-2
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-4
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-1
26/02/22 12:31:56 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-5
26/02/22 12:31:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:31:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:31:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:31:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:31:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:31:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-1
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-5
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-3
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-4
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-2
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-0
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-2
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-1
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-4
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-3
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-5
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-3
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-1
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 2000 for partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to offset 2500 for partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO MemoryStore: Block rdd_6_3 stored as values in memory (estimated size 49.2 KiB, free 434.2 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added rdd_6_3 in memory on spark-master:41593 (size: 49.2 KiB, free: 434.3 MiB)
26/02/22 12:31:57 INFO Executor: Finished task 3.0 in stage 9.0 (TID 11). 1753 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 11) in 1164 ms on spark-master (executor driver) (1/6)
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO MemoryStore: Block rdd_6_2 stored as values in memory (estimated size 78.9 KiB, free 434.2 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added rdd_6_2 in memory on spark-master:41593 (size: 78.9 KiB, free: 434.2 MiB)
26/02/22 12:31:57 INFO MemoryStore: Block rdd_6_1 stored as values in memory (estimated size 94.2 KiB, free 434.1 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added rdd_6_1 in memory on spark-master:41593 (size: 94.2 KiB, free: 434.2 MiB)
26/02/22 12:31:57 INFO Executor: Finished task 2.0 in stage 9.0 (TID 10). 1753 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 10) in 1186 ms on spark-master (executor driver) (2/6)
26/02/22 12:31:57 INFO Executor: Finished task 1.0 in stage 9.0 (TID 9). 1753 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 9) in 1189 ms on spark-master (executor driver) (3/6)
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO MemoryStore: Block rdd_6_0 stored as values in memory (estimated size 126.8 KiB, free 433.9 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added rdd_6_0 in memory on spark-master:41593 (size: 126.8 KiB, free: 434.0 MiB)
26/02/22 12:31:57 INFO MemoryStore: Block rdd_6_5 stored as values in memory (estimated size 132.6 KiB, free 433.8 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added rdd_6_5 in memory on spark-master:41593 (size: 132.6 KiB, free: 433.9 MiB)
26/02/22 12:31:57 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1753 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 5.0 in stage 9.0 (TID 13). 1753 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 1220 ms on spark-master (executor driver) (4/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 13) in 1219 ms on spark-master (executor driver) (5/6)
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:31:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:31:57 INFO MemoryStore: Block rdd_6_4 stored as values in memory (estimated size 191.4 KiB, free 433.6 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added rdd_6_4 in memory on spark-master:41593 (size: 191.4 KiB, free: 433.7 MiB)
26/02/22 12:31:57 INFO Executor: Finished task 4.0 in stage 9.0 (TID 12). 1753 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 12) in 1244 ms on spark-master (executor driver) (6/6)
26/02/22 12:31:57 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
26/02/22 12:31:57 INFO DAGScheduler: ShuffleMapStage 9 (start at <unknown>:0) finished in 1.278 s
26/02/22 12:31:57 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:31:57 INFO DAGScheduler: running: Set()
26/02/22 12:31:57 INFO DAGScheduler: waiting: Set(ResultStage 10)
26/02/22 12:31:57 INFO DAGScheduler: failed: Set()
26/02/22 12:31:57 INFO DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:57 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.5 KiB, free 433.6 MiB)
26/02/22 12:31:57 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.6 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:41593 (size: 3.1 KiB, free: 433.7 MiB)
26/02/22 12:31:57 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:57 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:31:57 INFO TaskSchedulerImpl: Adding task set 10.0 with 6 tasks resource profile 0
26/02/22 12:31:57 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14) (spark-master, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 15) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 16) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 17) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 18) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 19) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO Executor: Running task 2.0 in stage 10.0 (TID 16)
26/02/22 12:31:57 INFO Executor: Running task 5.0 in stage 10.0 (TID 19)
26/02/22 12:31:57 INFO Executor: Running task 3.0 in stage 10.0 (TID 17)
26/02/22 12:31:57 INFO Executor: Running task 1.0 in stage 10.0 (TID 14)
26/02/22 12:31:57 INFO Executor: Running task 0.0 in stage 10.0 (TID 15)
26/02/22 12:31:57 INFO Executor: Running task 4.0 in stage 10.0 (TID 18)
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Getting 6 (528.0 B) non-empty blocks including 6 (528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:57 INFO Executor: Finished task 3.0 in stage 10.0 (TID 17). 1235 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 5.0 in stage 10.0 (TID 19). 1235 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 1.0 in stage 10.0 (TID 14). 1284 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 0.0 in stage 10.0 (TID 15). 1235 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 4.0 in stage 10.0 (TID 18). 1235 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 2.0 in stage 10.0 (TID 16). 1235 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 19) in 16 ms on spark-master (executor driver) (1/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 17) in 18 ms on spark-master (executor driver) (2/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 15) in 19 ms on spark-master (executor driver) (3/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 18) in 17 ms on spark-master (executor driver) (4/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 16) in 18 ms on spark-master (executor driver) (5/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 21 ms on spark-master (executor driver) (6/6)
26/02/22 12:31:57 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/22 12:31:57 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.027 s
26/02/22 12:31:57 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/22 12:31:57 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 1.311969 s
26/02/22 12:31:57 INFO BaseSparkCommitActionExecutor: Source read and index timer 1333
26/02/22 12:31:57 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:31:57 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:57 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:57 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
26/02/22 12:31:57 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:57 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:57 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:57 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 269.4 KiB, free 433.4 MiB)
26/02/22 12:31:57 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 93.4 KiB, free 433.3 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:41593 (size: 93.4 KiB, free: 433.6 MiB)
26/02/22 12:31:57 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:57 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/22 12:31:57 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO Executor: Running task 0.0 in stage 11.0 (TID 20)
26/02/22 12:31:57 INFO Executor: Finished task 0.0 in stage 11.0 (TID 20). 790 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 20) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/22 12:31:57 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.024 s
26/02/22 12:31:57 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/22 12:31:57 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.027080 s
26/02/22 12:31:57 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:57 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:57 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/22 12:31:57 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 10000, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/22 12:31:57 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/22 12:31:57 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/22 12:31:57 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260222123154225.inflight
26/02/22 12:31:57 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:31:57 INFO BaseCommitActionExecutor: Auto commit disabled for 20260222123154225
26/02/22 12:31:57 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:57 INFO DAGScheduler: Registering RDD 30 (start at <unknown>:0) as input to shuffle 2
26/02/22 12:31:57 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:57 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
26/02/22 12:31:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
26/02/22 12:31:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
26/02/22 12:31:57 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:57 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:41593 in memory (size: 93.4 KiB, free: 433.7 MiB)
26/02/22 12:31:57 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 299.7 KiB, free 433.1 MiB)
26/02/22 12:31:57 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.0 KiB, free 433.2 MiB)
26/02/22 12:31:57 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:41593 (size: 106.0 KiB, free: 433.6 MiB)
26/02/22 12:31:57 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:57 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:41593 in memory (size: 3.1 KiB, free: 433.6 MiB)
26/02/22 12:31:57 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:31:57 INFO TaskSchedulerImpl: Adding task set 12.0 with 6 tasks resource profile 0
26/02/22 12:31:57 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 21) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 22) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 23) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 24) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 25) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 26) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:31:57 INFO Executor: Running task 5.0 in stage 12.0 (TID 26)
26/02/22 12:31:57 INFO Executor: Running task 2.0 in stage 12.0 (TID 23)
26/02/22 12:31:57 INFO Executor: Running task 4.0 in stage 12.0 (TID 25)
26/02/22 12:31:57 INFO Executor: Running task 0.0 in stage 12.0 (TID 21)
26/02/22 12:31:57 INFO Executor: Running task 3.0 in stage 12.0 (TID 24)
26/02/22 12:31:57 INFO Executor: Running task 1.0 in stage 12.0 (TID 22)
26/02/22 12:31:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:41593 in memory (size: 31.0 KiB, free: 433.6 MiB)
26/02/22 12:31:57 INFO BlockManager: Found block rdd_6_0 locally
26/02/22 12:31:57 INFO BlockManager: Found block rdd_6_2 locally
26/02/22 12:31:57 INFO BlockManager: Found block rdd_6_4 locally
26/02/22 12:31:57 INFO BlockManager: Found block rdd_6_1 locally
26/02/22 12:31:57 INFO BlockManager: Found block rdd_6_3 locally
26/02/22 12:31:57 INFO BlockManager: Found block rdd_6_5 locally
26/02/22 12:31:57 INFO Executor: Finished task 3.0 in stage 12.0 (TID 24). 1619 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 24) in 61 ms on spark-master (executor driver) (1/6)
26/02/22 12:31:57 INFO Executor: Finished task 2.0 in stage 12.0 (TID 23). 1619 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 1.0 in stage 12.0 (TID 22). 1619 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 23) in 68 ms on spark-master (executor driver) (2/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 22) in 68 ms on spark-master (executor driver) (3/6)
26/02/22 12:31:57 INFO Executor: Finished task 5.0 in stage 12.0 (TID 26). 1619 bytes result sent to driver
26/02/22 12:31:57 INFO Executor: Finished task 0.0 in stage 12.0 (TID 21). 1619 bytes result sent to driver
26/02/22 12:31:57 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 26) in 72 ms on spark-master (executor driver) (4/6)
26/02/22 12:31:57 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 21) in 75 ms on spark-master (executor driver) (5/6)
26/02/22 12:31:58 INFO Executor: Finished task 4.0 in stage 12.0 (TID 25). 1619 bytes result sent to driver
26/02/22 12:31:58 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 25) in 79 ms on spark-master (executor driver) (6/6)
26/02/22 12:31:58 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/22 12:31:58 INFO DAGScheduler: ShuffleMapStage 12 (start at <unknown>:0) finished in 0.104 s
26/02/22 12:31:58 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:31:58 INFO DAGScheduler: running: Set()
26/02/22 12:31:58 INFO DAGScheduler: waiting: Set(ResultStage 13)
26/02/22 12:31:58 INFO DAGScheduler: failed: Set()
26/02/22 12:31:58 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:58 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 302.5 KiB, free 433.1 MiB)
26/02/22 12:31:58 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 107.4 KiB, free 432.9 MiB)
26/02/22 12:31:58 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:41593 (size: 107.4 KiB, free: 433.5 MiB)
26/02/22 12:31:58 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:58 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/22 12:31:58 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:58 INFO Executor: Running task 0.0 in stage 13.0 (TID 27)
26/02/22 12:31:58 INFO ShuffleBlockFetcherIterator: Getting 6 (837.5 KiB) non-empty blocks including 6 (837.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:58 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:31:58 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222123154225/a19b2540-f90a-4622-8a74-213a73de31ee-0_0-13-27_20260222123154225.parquet.marker.CREATE
26/02/22 12:31:58 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222123154225/a19b2540-f90a-4622-8a74-213a73de31ee-0_0-13-27_20260222123154225.parquet.marker.CREATE in 31 ms
26/02/22 12:31:58 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:31:58 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:41593 in memory (size: 106.0 KiB, free: 433.6 MiB)
26/02/22 12:31:58 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId a19b2540-f90a-4622-8a74-213a73de31ee-0
26/02/22 12:31:58 INFO HoodieCreateHandle: Closing the file a19b2540-f90a-4622-8a74-213a73de31ee-0 as we are done with all the records 10000
26/02/22 12:31:58 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID a19b2540-f90a-4622-8a74-213a73de31ee-0, took 769 ms.
26/02/22 12:31:58 INFO MemoryStore: Block rdd_34_0 stored as values in memory (estimated size 295.0 B, free 433.3 MiB)
26/02/22 12:31:58 INFO BlockManagerInfo: Added rdd_34_0 in memory on spark-master:41593 (size: 295.0 B, free: 433.6 MiB)
26/02/22 12:31:58 INFO Executor: Finished task 0.0 in stage 13.0 (TID 27). 1716 bytes result sent to driver
26/02/22 12:31:58 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 27) in 798 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:58 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/22 12:31:58 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.813 s
26/02/22 12:31:58 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
26/02/22 12:31:58 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.921095 s
26/02/22 12:31:58 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/22 12:31:58 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:58 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:58 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
26/02/22 12:31:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/22 12:31:58 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:58 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:58 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 303.2 KiB, free 433.0 MiB)
26/02/22 12:31:58 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:41593 in memory (size: 107.4 KiB, free: 433.7 MiB)
26/02/22 12:31:58 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 107.8 KiB, free 433.2 MiB)
26/02/22 12:31:58 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:41593 (size: 107.8 KiB, free: 433.6 MiB)
26/02/22 12:31:58 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:58 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/22 12:31:58 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:58 INFO Executor: Running task 0.0 in stage 15.0 (TID 28)
26/02/22 12:31:58 INFO BlockManager: Found block rdd_34_0 locally
26/02/22 12:31:58 INFO Executor: Finished task 0.0 in stage 15.0 (TID 28). 1720 bytes result sent to driver
26/02/22 12:31:58 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 28) in 27 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:58 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/22 12:31:58 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.044 s
26/02/22 12:31:58 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/22 12:31:58 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.046165 s
26/02/22 12:31:58 INFO BaseHoodieWriteClient: Committing 20260222123154225 action commit
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123154225__commit__INFLIGHT__20260222123157863]}
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:58 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:58 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:58 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:58 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:58 INFO CommitUtils: Creating  metadata for INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123154225__commit__INFLIGHT__20260222123157863]}
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:58 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:58 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:58 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:58 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:58 INFO BaseHoodieWriteClient: Committing 20260222123154225 action commit
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:58 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:58 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:58 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:58 INFO HoodieTableMetadataUtil: Updating at 20260222123154225 from Commit/INSERT. #partitions_updated=2, #files_added=1
26/02/22 12:31:58 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:58 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:58 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:31:58 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:31:58 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:31:58 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:58 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:58 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:59 INFO HoodieBackedTableMetadataWriter: New commit at 20260222123154225 being applied to MDT.
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:59 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123156097]}
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:59 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123154225 action: deltacommit
26/02/22 12:31:59 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123154225__deltacommit__REQUESTED]
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123154225__deltacommit__REQUESTED__20260222123159032]}
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:59 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:31:59 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:31:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:59 INFO DAGScheduler: Registering RDD 45 (start at <unknown>:0) as input to shuffle 3
26/02/22 12:31:59 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:59 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
26/02/22 12:31:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/22 12:31:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)
26/02/22 12:31:59 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 10.1 KiB, free 433.3 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 433.3 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:41593 (size: 5.4 KiB, free: 433.6 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:41593 in memory (size: 107.8 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 16.0 (TID 29)
26/02/22 12:31:59 INFO MemoryStore: Block rdd_43_0 stored as values in memory (estimated size 342.0 B, free 433.7 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added rdd_43_0 in memory on spark-master:41593 (size: 342.0 B, free: 433.7 MiB)
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 16.0 (TID 29). 1093 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 29) in 22 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ShuffleMapStage 16 (start at <unknown>:0) finished in 0.033 s
26/02/22 12:31:59 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:31:59 INFO DAGScheduler: running: Set()
26/02/22 12:31:59 INFO DAGScheduler: waiting: Set(ResultStage 17)
26/02/22 12:31:59 INFO DAGScheduler: failed: Set()
26/02/22 12:31:59 INFO DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KiB, free 433.7 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.7 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:41593 (size: 3.1 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 30) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 17.0 (TID 30)
26/02/22 12:31:59 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 17.0 (TID 30). 1366 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 30) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.012 s
26/02/22 12:31:59 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/22 12:31:59 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.050369 s
26/02/22 12:31:59 INFO BaseSparkCommitActionExecutor: Source read and index timer 57
26/02/22 12:31:59 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:31:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:59 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:59 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
26/02/22 12:31:59 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:59 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:59 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 262.2 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 433.4 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:41593 (size: 90.5 KiB, free: 433.6 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:41593 in memory (size: 3.1 KiB, free: 433.6 MiB)
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 18.0 (TID 31)
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:41593 in memory (size: 5.4 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:59 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:31:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:59 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 18.0 (TID 31). 837 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 31) in 21 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.036 s
26/02/22 12:31:59 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/22 12:31:59 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.038177 s
26/02/22 12:31:59 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:59 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/22 12:31:59 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260222123154225.deltacommit.inflight
26/02/22 12:31:59 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:31:59 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260222123154225
26/02/22 12:31:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:59 INFO DAGScheduler: Registering RDD 49 (start at <unknown>:0) as input to shuffle 4
26/02/22 12:31:59 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:59 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
26/02/22 12:31:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/22 12:31:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:41593 in memory (size: 90.5 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 266.9 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 93.0 KiB, free 433.4 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:41593 (size: 93.0 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 32) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 19.0 (TID 32)
26/02/22 12:31:59 INFO BlockManager: Found block rdd_43_0 locally
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 19.0 (TID 32). 1050 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 32) in 13 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ShuffleMapStage 19 (start at <unknown>:0) finished in 0.030 s
26/02/22 12:31:59 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:31:59 INFO DAGScheduler: running: Set()
26/02/22 12:31:59 INFO DAGScheduler: waiting: Set(ResultStage 20)
26/02/22 12:31:59 INFO DAGScheduler: failed: Set()
26/02/22 12:31:59 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 372.6 KiB, free 433.0 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.9 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:41593 (size: 132.2 KiB, free: 433.5 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 33) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 20.0 (TID 33)
26/02/22 12:31:59 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:31:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:31:59 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260222123154225 for file files-0000-0
26/02/22 12:31:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:59 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:31:59 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/22 12:31:59 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:31:59 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/22 12:31:59 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:31:59 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:31:59 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222123154225/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND
26/02/22 12:31:59 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222123154225/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND in 28 ms
26/02/22 12:31:59 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-20-33', fileLen=-1}
26/02/22 12:31:59 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:31:59 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:31:59 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-20-33, took 389 ms.
26/02/22 12:31:59 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 342.0 B, free 432.9 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:41593 (size: 342.0 B, free: 433.5 MiB)
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 20.0 (TID 33). 1626 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 33) in 432 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.449 s
26/02/22 12:31:59 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/22 12:31:59 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.485274 s
26/02/22 12:31:59 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/22 12:31:59 INFO BaseSparkCommitActionExecutor: Committing 20260222123154225, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/22 12:31:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:59 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:59 INFO DAGScheduler: Final stage: ResultStage 21 (start at <unknown>:0)
26/02/22 12:31:59 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:59 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:59 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 101.2 KiB, free 432.8 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.8 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:41593 (size: 36.1 KiB, free: 433.5 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:41593 in memory (size: 132.2 KiB, free: 433.6 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 21.0 (TID 34)
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:41593 in memory (size: 93.0 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 21.0 (TID 34). 804 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 34) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ResultStage 21 (start at <unknown>:0) finished in 0.024 s
26/02/22 12:31:59 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/22 12:31:59 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.033720 s
26/02/22 12:31:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:59 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:59 INFO DAGScheduler: Final stage: ResultStage 22 (start at <unknown>:0)
26/02/22 12:31:59 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:59 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:59 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 101.1 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:41593 in memory (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:41593 (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 35) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 22.0 (TID 35)
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 22.0 (TID 35). 857 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 35) in 16 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ResultStage 22 (start at <unknown>:0) finished in 0.030 s
26/02/22 12:31:59 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/22 12:31:59 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.031810 s
26/02/22 12:31:59 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123154225__deltacommit__INFLIGHT]
26/02/22 12:31:59 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260222123154225.deltacommit
26/02/22 12:31:59 INFO HoodieActiveTimeline: Completed [==>20260222123154225__deltacommit__INFLIGHT]
26/02/22 12:31:59 INFO BaseSparkCommitActionExecutor: Committed 20260222123154225
26/02/22 12:31:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:59 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:59 INFO DAGScheduler: Final stage: ResultStage 23 (start at <unknown>:0)
26/02/22 12:31:59 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:59 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:59 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 101.3 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:41593 (size: 36.2 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:41593 in memory (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 36) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 23.0 (TID 36)
26/02/22 12:31:59 INFO Executor: Finished task 0.0 in stage 23.0 (TID 36). 900 bytes result sent to driver
26/02/22 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 36) in 32 ms on spark-master (executor driver) (1/1)
26/02/22 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/22 12:31:59 INFO DAGScheduler: ResultStage 23 (start at <unknown>:0) finished in 0.044 s
26/02/22 12:31:59 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/22 12:31:59 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 0.045346 s
26/02/22 12:31:59 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222123154225
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:31:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__deltacommit__COMPLETED__20260222123159836]}
26/02/22 12:31:59 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:31:59 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:31:59 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:31:59 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__deltacommit__COMPLETED__20260222123159836]}
26/02/22 12:31:59 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123154225__commit__INFLIGHT]
26/02/22 12:31:59 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260222123154225.commit
26/02/22 12:31:59 INFO HoodieActiveTimeline: Completed [==>20260222123154225__commit__INFLIGHT]
26/02/22 12:31:59 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:31:59 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
26/02/22 12:31:59 INFO DAGScheduler: Final stage: ResultStage 24 (start at <unknown>:0)
26/02/22 12:31:59 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:31:59 INFO DAGScheduler: Missing parents: List()
26/02/22 12:31:59 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0), which has no missing parents
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 101.3 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:31:59 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:41593 (size: 36.2 KiB, free: 433.7 MiB)
26/02/22 12:31:59 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/22 12:31:59 INFO BlockManager: Removing RDD 43
26/02/22 12:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:31:59 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
26/02/22 12:31:59 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 37) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:31:59 INFO Executor: Running task 0.0 in stage 24.0 (TID 37)
26/02/22 12:31:59 INFO BlockManager: Removing RDD 53
26/02/22 12:31:59 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:41593 in memory (size: 36.2 KiB, free: 433.7 MiB)
26/02/22 12:32:00 INFO Executor: Finished task 0.0 in stage 24.0 (TID 37). 964 bytes result sent to driver
26/02/22 12:32:00 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 37) in 34 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:00 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/22 12:32:00 INFO DAGScheduler: ResultStage 24 (start at <unknown>:0) finished in 0.046 s
26/02/22 12:32:00 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/22 12:32:00 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.047134 s
26/02/22 12:32:00 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222123154225
26/02/22 12:32:00 INFO BaseHoodieWriteClient: Committed 20260222123154225
26/02/22 12:32:00 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
26/02/22 12:32:00 INFO MapPartitionsRDD: Removing RDD 34 from persistence list
26/02/22 12:32:00 INFO BlockManager: Removing RDD 6
26/02/22 12:32:00 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/22 12:32:00 INFO BlockManager: Removing RDD 34
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__deltacommit__COMPLETED__20260222123159836]}
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:00 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:00 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__deltacommit__COMPLETED__20260222123159836]}
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:00 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:00 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260222123200018
26/02/22 12:32:00 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:32:00 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__deltacommit__COMPLETED__20260222123159836]}
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:00 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:00 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__deltacommit__COMPLETED__20260222123159836]}
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/22 12:32:00 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:32:00 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__deltacommit__COMPLETED__20260222123159836]}
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Commit 20260222123154225 successful!
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:32:00 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:32:00 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/22 12:32:00 INFO HoodieStreamingSink: Micro batch id=0 succeeded for commit=20260222123154225
26/02/22 12:32:00 INFO HoodieStreamingSink: Current value of latestCommittedBatchId: -1. Setting latestCommittedBatchId to batchId 0.
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieStreamingSink: Micro batch id=0 succeeded
26/02/22 12:32:00 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/commits/0 using temp file s3a://warehouse/checkpoints/bronze/commits/.0.fdb69f23-589e-450f-9ab2-64d06df019e8.tmp
26/02/22 12:32:00 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/commits/.0.fdb69f23-589e-450f-9ab2-64d06df019e8.tmp to s3a://warehouse/checkpoints/bronze/commits/0
26/02/22 12:32:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "71332e9e-d3e0-4b36-9dc5-dd1841a9db0c",
  "runId" : "6cf9fe99-f3ba-4c77-b653-dc1ecaa81628",
  "name" : null,
  "timestamp" : "2026-02-22T12:31:53.646Z",
  "batchId" : 0,
  "numInputRows" : 10000,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 1529.753709652746,
  "durationMs" : {
    "addBatch" : 6125,
    "getBatch" : 13,
    "latestOffset" : 27,
    "queryPlanning" : 232,
    "triggerExecution" : 6536,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[taxi.raw_trips]]",
    "startOffset" : null,
    "endOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "latestOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "numInputRows" : 10000,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 1529.753709652746,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "HoodieStreamingSink[s3a://warehouse/bronze/raw_trips]",
    "numOutputRows" : -1
  }
}
26/02/22 12:32:00 INFO MicroBatchExecution: Finished processing all available data for the trigger, terminating this Trigger.AvailableNow query
26/02/22 12:32:00 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Revoke previously assigned partitions taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/22 12:32:00 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] The pause flag in partitions [taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5] will be removed due to revocation.
26/02/22 12:32:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Member consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1-619966dc-0417-41c0-9cec-4ac5111d0b37 sending LeaveGroup request to coordinator kafka:9092 (id: 2147483646 rack: null) due to the consumer is being closed
26/02/22 12:32:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:32:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:32:00 INFO Metrics: Metrics scheduler closed
26/02/22 12:32:00 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:32:00 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:32:00 INFO Metrics: Metrics reporters closed
26/02/22 12:32:00 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-driver-0-1 unregistered
26/02/22 12:32:00 INFO DataSourceUtils: Getting table path..
26/02/22 12:32:00 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:00 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:00 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:00 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:00 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 7 ms
26/02/22 12:32:00 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:32:00 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:32:00 INFO DAGScheduler: Got job 18 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:32:00 INFO DAGScheduler: Final stage: ResultStage 25 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:32:00 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:00 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:00 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 101.1 KiB, free 434.2 MiB)
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.1 MiB)
26/02/22 12:32:00 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:41593 (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:32:00 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:00 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:41593 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:32:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:00 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
26/02/22 12:32:00 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 38) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/22 12:32:00 INFO Executor: Running task 0.0 in stage 25.0 (TID 38)
26/02/22 12:32:00 INFO Executor: Finished task 0.0 in stage 25.0 (TID 38). 1123 bytes result sent to driver
26/02/22 12:32:00 INFO BlockManager: Removing RDD 34
26/02/22 12:32:00 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 38) in 19 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:00 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
26/02/22 12:32:00 INFO BlockManager: Removing RDD 6
26/02/22 12:32:00 INFO DAGScheduler: ResultStage 25 (collect at HoodieSparkEngineContext.java:116) finished in 0.033 s
26/02/22 12:32:00 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
26/02/22 12:32:00 INFO DAGScheduler: Job 18 finished: collect at HoodieSparkEngineContext.java:116, took 0.034911 s
26/02/22 12:32:00 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:00 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:00 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:32:00 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:32:00 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:32:00 INFO CodeGenerator: Code generated in 9.936389 ms
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/22 12:32:00 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:41593 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.2 MiB)
26/02/22 12:32:00 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:41593 (size: 35.2 KiB, free: 434.4 MiB)
26/02/22 12:32:00 INFO SparkContext: Created broadcast 24 from count at <unknown>:0
26/02/22 12:32:00 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:32:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:32:00 INFO DAGScheduler: Registering RDD 68 (count at <unknown>:0) as input to shuffle 5
26/02/22 12:32:00 INFO DAGScheduler: Got map stage job 19 (count at <unknown>:0) with 1 output partitions
26/02/22 12:32:00 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (count at <unknown>:0)
26/02/22 12:32:00 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:00 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:00 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0), which has no missing parents
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.1 MiB)
26/02/22 12:32:00 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:41593 (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:32:00 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:00 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/22 12:32:00 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 39) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:32:00 INFO Executor: Running task 0.0 in stage 26.0 (TID 39)
26/02/22 12:32:00 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/a19b2540-f90a-4622-8a74-213a73de31ee-0_0-13-27_20260222123154225.parquet, range: 0-681068, partition values: [empty row]
26/02/22 12:32:00 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:00 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:00 INFO Executor: Finished task 0.0 in stage 26.0 (TID 39). 2056 bytes result sent to driver
26/02/22 12:32:00 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 39) in 102 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:00 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/22 12:32:00 INFO DAGScheduler: ShuffleMapStage 26 (count at <unknown>:0) finished in 0.124 s
26/02/22 12:32:00 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:00 INFO DAGScheduler: running: Set()
26/02/22 12:32:00 INFO DAGScheduler: waiting: Set()
26/02/22 12:32:00 INFO DAGScheduler: failed: Set()
26/02/22 12:32:00 INFO CodeGenerator: Code generated in 3.54742 ms
26/02/22 12:32:00 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:32:00 INFO DAGScheduler: Got job 20 (count at <unknown>:0) with 1 output partitions
26/02/22 12:32:00 INFO DAGScheduler: Final stage: ResultStage 28 (count at <unknown>:0)
26/02/22 12:32:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
26/02/22 12:32:00 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:00 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0), which has no missing parents
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/22 12:32:00 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/22 12:32:00 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:41593 (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:32:00 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:41593 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:32:00 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:00 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
26/02/22 12:32:00 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 40) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:32:00 INFO Executor: Running task 0.0 in stage 28.0 (TID 40)
26/02/22 12:32:00 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:00 INFO Executor: Finished task 0.0 in stage 28.0 (TID 40). 2458 bytes result sent to driver
26/02/22 12:32:00 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 40) in 24 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:00 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
26/02/22 12:32:00 INFO DAGScheduler: ResultStage 28 (count at <unknown>:0) finished in 0.033 s
26/02/22 12:32:00 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
26/02/22 12:32:00 INFO DAGScheduler: Job 20 finished: count at <unknown>:0, took 0.035544 s
Bronze ingest complete. Total rows in Hudi table: 10,000
============================================================
  Bronze Ingest (Hudi COW) COMPLETE
============================================================
26/02/22 12:32:00 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/22 12:32:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/22 12:32:00 INFO MemoryStore: MemoryStore cleared
26/02/22 12:32:00 INFO BlockManager: BlockManager stopped
26/02/22 12:32:00 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/22 12:32:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/22 12:32:00 INFO SparkContext: Successfully stopped SparkContext
26/02/22 12:32:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:32:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:32:00 INFO Metrics: Metrics scheduler closed
26/02/22 12:32:00 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:32:00 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:32:00 INFO Metrics: Metrics reporters closed
26/02/22 12:32:00 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-6 unregistered
26/02/22 12:32:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:32:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO Metrics: Metrics scheduler closed
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:32:01 INFO Metrics: Metrics reporters closed
26/02/22 12:32:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-5 unregistered
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO Metrics: Metrics scheduler closed
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:32:01 INFO Metrics: Metrics reporters closed
26/02/22 12:32:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-4 unregistered
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO Metrics: Metrics scheduler closed
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:32:01 INFO Metrics: Metrics reporters closed
26/02/22 12:32:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-2 unregistered
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO Metrics: Metrics scheduler closed
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:32:01 INFO Metrics: Metrics reporters closed
26/02/22 12:32:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-7 unregistered
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3, groupId=spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:32:01 INFO Metrics: Metrics scheduler closed
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:32:01 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:32:01 INFO Metrics: Metrics reporters closed
26/02/22 12:32:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1c988d44-444d-42e8-bdbe-7518eb1a8cc5--1125050931-executor-3 unregistered
26/02/22 12:32:01 INFO ShutdownHookManager: Shutdown hook called
26/02/22 12:32:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab1aac3e-27e8-4a25-8a62-9c5284d01b5b
26/02/22 12:32:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-24737403-eb9a-461f-8ad6-37f3814b5cd1
26/02/22 12:32:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab1aac3e-27e8-4a25-8a62-9c5284d01b5b/pyspark-8a2d881a-92c9-497e-ab90-dff0719584ad
Bronze ingest complete.
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/silver_transform.py
26/02/22 12:32:02 INFO SparkContext: Running Spark version 3.3.3
26/02/22 12:32:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/22 12:32:02 INFO ResourceUtils: ==============================================================
26/02/22 12:32:02 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/22 12:32:02 INFO ResourceUtils: ==============================================================
26/02/22 12:32:02 INFO SparkContext: Submitted application: P22-Silver-Transform-Hudi
26/02/22 12:32:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/22 12:32:02 INFO ResourceProfile: Limiting resource is cpu
26/02/22 12:32:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/22 12:32:02 INFO SecurityManager: Changing view acls to: spark
26/02/22 12:32:02 INFO SecurityManager: Changing modify acls to: spark
26/02/22 12:32:02 INFO SecurityManager: Changing view acls groups to: 
26/02/22 12:32:02 INFO SecurityManager: Changing modify acls groups to: 
26/02/22 12:32:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/22 12:32:02 INFO Utils: Successfully started service 'sparkDriver' on port 44191.
26/02/22 12:32:02 INFO SparkEnv: Registering MapOutputTracker
26/02/22 12:32:02 INFO SparkEnv: Registering BlockManagerMaster
26/02/22 12:32:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/22 12:32:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/22 12:32:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/22 12:32:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1976b248-51ab-41ac-bfaf-6b463ceaeb97
26/02/22 12:32:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/22 12:32:02 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/22 12:32:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/22 12:32:03 INFO Executor: Starting executor ID driver on host spark-master
26/02/22 12:32:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/22 12:32:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35201.
26/02/22 12:32:03 INFO NettyBlockTransferService: Server created on spark-master:35201
26/02/22 12:32:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/22 12:32:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 35201, None)
26/02/22 12:32:03 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:35201 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 35201, None)
26/02/22 12:32:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 35201, None)
26/02/22 12:32:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 35201, None)
26/02/22 12:32:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/22 12:32:03 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/22 12:32:03 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/22 12:32:03 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:32:03 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/22 12:32:04 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/22 12:32:04 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/22 12:32:04 INFO DataSourceUtils: Getting table path..
26/02/22 12:32:04 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/22 12:32:04 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/22 12:32:04 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:04 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:04 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:32:04 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:32:04 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:04 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:04 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:32:04 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123154225__commit__COMPLETED__20260222123159934]}
26/02/22 12:32:04 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 14 ms
26/02/22 12:32:05 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:32:05 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:32:05 INFO DAGScheduler: Got job 0 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:32:05 INFO DAGScheduler: Final stage: ResultStage 0 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:32:05 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:05 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:32:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
26/02/22 12:32:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/22 12:32:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:35201 (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:32:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/22 12:32:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/22 12:32:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/22 12:32:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1123 bytes result sent to driver
26/02/22 12:32:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 89 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/22 12:32:05 INFO DAGScheduler: ResultStage 0 (collect at HoodieSparkEngineContext.java:116) finished in 0.282 s
26/02/22 12:32:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/22 12:32:05 INFO DAGScheduler: Job 0 finished: collect at HoodieSparkEngineContext.java:116, took 0.303606 s
26/02/22 12:32:05 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:05 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:06 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:32:06 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:32:06 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:32:06 INFO CodeGenerator: Code generated in 72.985302 ms
26/02/22 12:32:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/22 12:32:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.0 MiB)
26/02/22 12:32:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:35201 (size: 35.2 KiB, free: 434.3 MiB)
26/02/22 12:32:06 INFO SparkContext: Created broadcast 1 from count at <unknown>:0
26/02/22 12:32:06 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:32:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:32:06 INFO DAGScheduler: Registering RDD 5 (count at <unknown>:0) as input to shuffle 0
26/02/22 12:32:06 INFO DAGScheduler: Got map stage job 1 (count at <unknown>:0) with 1 output partitions
26/02/22 12:32:06 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at <unknown>:0)
26/02/22 12:32:06 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:06 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:06 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0), which has no missing parents
26/02/22 12:32:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
26/02/22 12:32:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.0 MiB)
26/02/22 12:32:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:35201 (size: 7.4 KiB, free: 434.3 MiB)
26/02/22 12:32:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/22 12:32:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:32:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/22 12:32:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:35201 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:32:06 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/a19b2540-f90a-4622-8a74-213a73de31ee-0_0-13-27_20260222123154225.parquet, range: 0-681068, partition values: [empty row]
26/02/22 12:32:06 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:06 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2099 bytes result sent to driver
26/02/22 12:32:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 144 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/22 12:32:06 INFO DAGScheduler: ShuffleMapStage 1 (count at <unknown>:0) finished in 0.178 s
26/02/22 12:32:06 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:06 INFO DAGScheduler: running: Set()
26/02/22 12:32:06 INFO DAGScheduler: waiting: Set()
26/02/22 12:32:06 INFO DAGScheduler: failed: Set()
26/02/22 12:32:06 INFO CodeGenerator: Code generated in 6.812492 ms
26/02/22 12:32:06 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:32:06 INFO DAGScheduler: Got job 2 (count at <unknown>:0) with 1 output partitions
26/02/22 12:32:06 INFO DAGScheduler: Final stage: ResultStage 3 (count at <unknown>:0)
26/02/22 12:32:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
26/02/22 12:32:06 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:06 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0), which has no missing parents
26/02/22 12:32:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/22 12:32:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/22 12:32:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:35201 (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:32:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:06 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/22 12:32:06 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:32:06 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
26/02/22 12:32:06 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
26/02/22 12:32:06 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2458 bytes result sent to driver
26/02/22 12:32:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 36 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/22 12:32:06 INFO DAGScheduler: ResultStage 3 (count at <unknown>:0) finished in 0.045 s
26/02/22 12:32:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
26/02/22 12:32:06 INFO DAGScheduler: Job 2 finished: count at <unknown>:0, took 0.051336 s
Bronze rows read: 10,000
26/02/22 12:32:06 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/22 12:32:06 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips as hoodie table s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:32:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:35201 in memory (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:32:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:35201 in memory (size: 35.2 KiB, free: 434.4 MiB)
26/02/22 12:32:07 INFO EmbeddedTimelineService: Overriding hostIp to (spark-master) found in spark-conf. It was null
26/02/22 12:32:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:35201 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:32:07 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:07 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:07 INFO log: Logging initialized @5580ms to org.apache.hudi.org.apache.jetty.util.log.Slf4jLog
26/02/22 12:32:07 INFO Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

26/02/22 12:32:07 INFO Javalin: Starting Javalin ...
26/02/22 12:32:07 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1217 days old. Consider checking for a newer version.).
26/02/22 12:32:07 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.20+8
26/02/22 12:32:07 INFO Server: Started @5797ms
26/02/22 12:32:07 INFO Javalin: Listening on http://localhost:37085/
26/02/22 12:32:07 INFO Javalin: Javalin started in 111ms \o/
26/02/22 12:32:07 INFO TimelineService: Starting Timeline server on port :37085
26/02/22 12:32:07 INFO EmbeddedTimelineService: Started embedded timeline server at spark-master:37085
26/02/22 12:32:07 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
26/02/22 12:32:07 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:32:07 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:32:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(tpep_pickup_datetime),IsNotNull(tpep_dropoff_datetime),IsNotNull(trip_distance),IsNotNull(fare_amount),GreaterThanOrEqual(trip_distance,0.0),GreaterThanOrEqual(fare_amount,0.0)
26/02/22 12:32:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(tpep_pickup_datetime#6),isnotnull(tpep_dropoff_datetime#7),isnotnull(trip_distance#9),isnotnull(fare_amount#15),isnotnull(cast(tpep_pickup_datetime#6 as timestamp)),isnotnull(cast(tpep_dropoff_datetime#7 as timestamp)),(trip_distance#9 >= 0.0),(fare_amount#15 >= 0.0),(cast(tpep_pickup_datetime#6 as timestamp) >= 2024-01-01 00:00:00)
26/02/22 12:32:07 INFO FileSourceStrategy: Output Data Schema: struct<VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: double, trip_distance: double ... 17 more fields>
26/02/22 12:32:07 INFO CodeGenerator: Code generated in 37.324138 ms
26/02/22 12:32:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.1 KiB, free 434.2 MiB)
26/02/22 12:32:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.2 MiB)
26/02/22 12:32:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:35201 (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:32:07 INFO SparkContext: Created broadcast 4 from toRdd at HoodieSparkUtils.scala:111
26/02/22 12:32:07 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:32:07 INFO HoodieFileIndex: Total file slices: 1; candidate file slices after data skipping: 1; skipping percentage 0.0
26/02/22 12:32:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:32:07 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:32:07 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:32:07 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123207175 action: commit
26/02/22 12:32:07 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123207175__commit__REQUESTED]
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123207175__commit__REQUESTED__20260222123207709]}
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:07 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/22 12:32:07 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123207175__commit__REQUESTED__20260222123207709]}
26/02/22 12:32:07 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips/.hoodie/metadata as hoodie table s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:07 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:07 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:07 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:07 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:32:07 INFO DAGScheduler: Got job 3 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:32:07 INFO DAGScheduler: Final stage: ResultStage 4 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:32:07 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:07 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:07 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:32:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 101.5 KiB, free 434.1 MiB)
26/02/22 12:32:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 434.0 MiB)
26/02/22 12:32:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:35201 (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:32:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:07 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/22 12:32:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4405 bytes) taskResourceAssignments Map()
26/02/22 12:32:07 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
26/02/22 12:32:07 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 926 bytes result sent to driver
26/02/22 12:32:07 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 14 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:07 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/22 12:32:07 INFO DAGScheduler: ResultStage 4 (collect at HoodieSparkEngineContext.java:116) finished in 0.023 s
26/02/22 12:32:07 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/22 12:32:07 INFO DAGScheduler: Job 3 finished: collect at HoodieSparkEngineContext.java:116, took 0.026153 s
26/02/22 12:32:07 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:32:07 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/22 12:32:07 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/22 12:32:07 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
26/02/22 12:32:07 INFO DAGScheduler: Got job 4 (count at HoodieJavaRDD.java:115) with 1 output partitions
26/02/22 12:32:07 INFO DAGScheduler: Final stage: ResultStage 5 (count at HoodieJavaRDD.java:115)
26/02/22 12:32:07 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:07 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:07 INFO DAGScheduler: Submitting ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
26/02/22 12:32:07 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.0 KiB, free 434.0 MiB)
26/02/22 12:32:07 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.0 MiB)
26/02/22 12:32:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:35201 (size: 1866.0 B, free: 434.3 MiB)
26/02/22 12:32:07 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:07 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
26/02/22 12:32:07 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:32:07 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
26/02/22 12:32:07 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 760 bytes result sent to driver
26/02/22 12:32:07 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 7 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:07 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
26/02/22 12:32:07 INFO DAGScheduler: ResultStage 5 (count at HoodieJavaRDD.java:115) finished in 0.014 s
26/02/22 12:32:07 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
26/02/22 12:32:07 INFO DAGScheduler: Job 4 finished: count at HoodieJavaRDD.java:115, took 0.017044 s
26/02/22 12:32:07 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/22 12:32:08 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/22 12:32:08 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
26/02/22 12:32:08 INFO DAGScheduler: Got job 5 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
26/02/22 12:32:08 INFO DAGScheduler: Final stage: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155)
26/02/22 12:32:08 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:08 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:08 INFO DAGScheduler: Submitting ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 393.9 KiB, free 433.6 MiB)
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 130.0 KiB, free 433.5 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:35201 (size: 130.0 KiB, free: 434.2 MiB)
26/02/22 12:32:08 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:08 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/22 12:32:08 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/22 12:32:08 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/22 12:32:08 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:32:08 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:32:08 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/22 12:32:08 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/22 12:32:08 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 803 bytes result sent to driver
26/02/22 12:32:08 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 62 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:08 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/22 12:32:08 INFO DAGScheduler: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155) finished in 0.089 s
26/02/22 12:32:08 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/22 12:32:08 INFO DAGScheduler: Job 5 finished: foreach at HoodieSparkEngineContext.java:155, took 0.092186 s
26/02/22 12:32:08 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:08 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:08 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:32:08 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:32:08 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:32:08 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:08 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:08 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:32:08 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:08 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:08 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/22 12:32:08 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260222123208223]}
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:08 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:08 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:32:08 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:32:08 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/22 12:32:08 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
26/02/22 12:32:08 INFO DAGScheduler: Registering RDD 23 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 1
26/02/22 12:32:08 INFO DAGScheduler: Got job 6 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
26/02/22 12:32:08 INFO DAGScheduler: Final stage: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
26/02/22 12:32:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
26/02/22 12:32:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)
26/02/22 12:32:08 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.5 KiB, free 433.5 MiB)
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.5 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:35201 (size: 4.7 KiB, free: 434.2 MiB)
26/02/22 12:32:08 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:08 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/22 12:32:08 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/22 12:32:08 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/22 12:32:08 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 964 bytes result sent to driver
26/02/22 12:32:08 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 17 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:08 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/22 12:32:08 INFO DAGScheduler: ShuffleMapStage 7 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.027 s
26/02/22 12:32:08 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:08 INFO DAGScheduler: running: Set()
26/02/22 12:32:08 INFO DAGScheduler: waiting: Set(ResultStage 8)
26/02/22 12:32:08 INFO DAGScheduler: failed: Set()
26/02/22 12:32:08 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.3 KiB, free 433.5 MiB)
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.5 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:35201 (size: 3.9 KiB, free: 434.2 MiB)
26/02/22 12:32:08 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:08 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/22 12:32:08 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:08 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/22 12:32:08 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:08 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 1246 bytes result sent to driver
26/02/22 12:32:08 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 25 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:08 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/22 12:32:08 INFO DAGScheduler: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.030 s
26/02/22 12:32:08 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/22 12:32:08 INFO DAGScheduler: Job 6 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.065372 s
26/02/22 12:32:08 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:32:08 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/22 12:32:08 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:32:08 INFO DAGScheduler: Got job 7 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:32:08 INFO DAGScheduler: Final stage: ResultStage 10 (collect at HoodieJavaRDD.java:177)
26/02/22 12:32:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/22 12:32:08 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:08 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 267.2 KiB, free 433.2 MiB)
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 91.3 KiB, free 433.1 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:35201 (size: 91.3 KiB, free: 434.1 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:35201 in memory (size: 3.9 KiB, free: 434.1 MiB)
26/02/22 12:32:08 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:08 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
26/02/22 12:32:08 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:08 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:35201 in memory (size: 4.7 KiB, free: 434.1 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:35201 in memory (size: 1866.0 B, free: 434.1 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:35201 in memory (size: 36.3 KiB, free: 434.1 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:35201 in memory (size: 130.0 KiB, free: 434.3 MiB)
26/02/22 12:32:08 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:08 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:32:08 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE
26/02/22 12:32:08 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE in 30 ms
26/02/22 12:32:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/22 12:32:08 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:32:08 INFO MetricsSystemImpl: HBase metrics system started
26/02/22 12:32:08 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/22 12:32:08 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:32:08 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:32:08 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/22 12:32:08 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/22 12:32:08 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 252 ms.
26/02/22 12:32:08 INFO MemoryStore: Block rdd_28_0 stored as values in memory (estimated size 272.0 B, free 433.8 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Added rdd_28_0 in memory on spark-master:35201 (size: 272.0 B, free: 434.3 MiB)
26/02/22 12:32:08 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1439 bytes result sent to driver
26/02/22 12:32:08 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 294 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:08 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/22 12:32:08 INFO DAGScheduler: ResultStage 10 (collect at HoodieJavaRDD.java:177) finished in 0.315 s
26/02/22 12:32:08 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/22 12:32:08 INFO DAGScheduler: Job 7 finished: collect at HoodieJavaRDD.java:177, took 0.317978 s
26/02/22 12:32:08 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:32:08 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/22 12:32:08 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:32:08 INFO DAGScheduler: Got job 8 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:32:08 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:32:08 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:08 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:08 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 101.2 KiB, free 433.7 MiB)
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.7 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:35201 (size: 36.1 KiB, free: 434.2 MiB)
26/02/22 12:32:08 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:08 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/22 12:32:08 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:32:08 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)
26/02/22 12:32:08 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 855 bytes result sent to driver
26/02/22 12:32:08 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 14 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:08 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/22 12:32:08 INFO DAGScheduler: ResultStage 11 (collect at HoodieSparkEngineContext.java:150) finished in 0.022 s
26/02/22 12:32:08 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/22 12:32:08 INFO DAGScheduler: Job 8 finished: collect at HoodieSparkEngineContext.java:150, took 0.024467 s
26/02/22 12:32:08 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:32:08 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/22 12:32:08 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:32:08 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/22 12:32:08 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:32:08 INFO DAGScheduler: Got job 9 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/22 12:32:08 INFO DAGScheduler: Final stage: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:32:08 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:08 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:08 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 101.3 KiB, free 433.6 MiB)
26/02/22 12:32:08 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:35201 (size: 36.2 KiB, free: 434.2 MiB)
26/02/22 12:32:08 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:08 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
26/02/22 12:32:08 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:32:08 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
26/02/22 12:32:08 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 904 bytes result sent to driver
26/02/22 12:32:08 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 25 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:08 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/22 12:32:08 INFO DAGScheduler: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.031 s
26/02/22 12:32:08 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
26/02/22 12:32:08 INFO DAGScheduler: Job 9 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.033628 s
26/02/22 12:32:08 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:08 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:35201 in memory (size: 91.3 KiB, free: 434.3 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:35201 in memory (size: 36.1 KiB, free: 434.3 MiB)
26/02/22 12:32:08 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:35201 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:32:08 INFO BlockManager: Removing RDD 28
26/02/22 12:32:08 INFO HoodieTableConfig: MDT s3a://warehouse/silver/cleaned_trips partition FILES has been enabled
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:08 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:08 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 960 in ms
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:08 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:08 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:08 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:08 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:08 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:08 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:08 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:32:08 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:09 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:09 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/22 12:32:09 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:32:09 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:09 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:09 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:32:09 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:09 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123207175__commit__REQUESTED__20260222123207709]}
26/02/22 12:32:09 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:09 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:09 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/22 12:32:09 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:09 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123207175__commit__REQUESTED__20260222123207709]}
26/02/22 12:32:09 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:32:09 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:09 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:09 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:09 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:09 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:32:09 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:32:09 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:32:09 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:32:09 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:32:09 INFO DAGScheduler: Registering RDD 34 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 3
26/02/22 12:32:09 INFO DAGScheduler: Registering RDD 40 (distinct at HoodieJavaRDD.java:157) as input to shuffle 2
26/02/22 12:32:09 INFO DAGScheduler: Got job 10 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:32:09 INFO DAGScheduler: Final stage: ResultStage 15 (collect at HoodieJavaRDD.java:177)
26/02/22 12:32:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/22 12:32:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 14)
26/02/22 12:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 104.0 KiB, free 434.1 MiB)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KiB, free 434.0 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:35201 (size: 31.5 KiB, free: 434.3 MiB)
26/02/22 12:32:09 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:09 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/22 12:32:09 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:32:09 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)
26/02/22 12:32:09 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/a19b2540-f90a-4622-8a74-213a73de31ee-0_0-13-27_20260222123154225.parquet, range: 0-681068, partition values: [empty row]
26/02/22 12:32:09 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:09 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(tpep_pickup_datetime, null), noteq(tpep_dropoff_datetime, null)), noteq(trip_distance, null)), noteq(fare_amount, null)), gteq(trip_distance, 0.0)), gteq(fare_amount, 0.0))
26/02/22 12:32:09 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:09 INFO CodecPool: Got brand-new decompressor [.gz]
26/02/22 12:32:09 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 1824 bytes result sent to driver
26/02/22 12:32:09 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 423 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:09 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/22 12:32:09 INFO DAGScheduler: ShuffleMapStage 13 (mapToPair at HoodieJavaRDD.java:149) finished in 0.432 s
26/02/22 12:32:09 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:09 INFO DAGScheduler: running: Set()
26/02/22 12:32:09 INFO DAGScheduler: waiting: Set(ResultStage 15, ShuffleMapStage 14)
26/02/22 12:32:09 INFO DAGScheduler: failed: Set()
26/02/22 12:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 30.1 KiB, free 434.0 MiB)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 434.0 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:35201 (size: 13.5 KiB, free: 434.3 MiB)
26/02/22 12:32:09 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:09 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
26/02/22 12:32:09 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 12) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/22 12:32:09 INFO Executor: Running task 0.0 in stage 14.0 (TID 12)
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:09 INFO MemoryStore: Block rdd_36_0 stored as values in memory (estimated size 994.0 KiB, free 433.0 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added rdd_36_0 in memory on spark-master:35201 (size: 994.0 KiB, free: 433.4 MiB)
26/02/22 12:32:09 INFO Executor: Finished task 0.0 in stage 14.0 (TID 12). 1394 bytes result sent to driver
26/02/22 12:32:09 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 12) in 105 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:09 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
26/02/22 12:32:09 INFO DAGScheduler: ShuffleMapStage 14 (distinct at HoodieJavaRDD.java:157) finished in 0.114 s
26/02/22 12:32:09 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:09 INFO DAGScheduler: running: Set()
26/02/22 12:32:09 INFO DAGScheduler: waiting: Set(ResultStage 15)
26/02/22 12:32:09 INFO DAGScheduler: failed: Set()
26/02/22 12:32:09 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 6.3 KiB, free 433.0 MiB)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.0 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:35201 (size: 3.4 KiB, free: 433.3 MiB)
26/02/22 12:32:09 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:09 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/22 12:32:09 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:09 INFO Executor: Running task 0.0 in stage 15.0 (TID 13)
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:09 INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 1237 bytes result sent to driver
26/02/22 12:32:09 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:09 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/22 12:32:09 INFO DAGScheduler: ResultStage 15 (collect at HoodieJavaRDD.java:177) finished in 0.012 s
26/02/22 12:32:09 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/22 12:32:09 INFO DAGScheduler: Job 10 finished: collect at HoodieJavaRDD.java:177, took 0.563394 s
26/02/22 12:32:09 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:35201 in memory (size: 31.5 KiB, free: 433.4 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:35201 in memory (size: 13.5 KiB, free: 433.4 MiB)
26/02/22 12:32:09 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:32:09 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:35201 in memory (size: 3.4 KiB, free: 433.4 MiB)
26/02/22 12:32:09 INFO DAGScheduler: Got job 11 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:32:09 INFO DAGScheduler: Final stage: ResultStage 16 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:32:09 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:09 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:09 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 268.6 KiB, free 432.9 MiB)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 432.8 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:35201 (size: 93.3 KiB, free: 433.3 MiB)
26/02/22 12:32:09 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:09 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/22 12:32:09 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:32:09 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)
26/02/22 12:32:09 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 761 bytes result sent to driver
26/02/22 12:32:09 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 11 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:09 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/22 12:32:09 INFO DAGScheduler: ResultStage 16 (collect at HoodieSparkEngineContext.java:150) finished in 0.032 s
26/02/22 12:32:09 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
26/02/22 12:32:09 INFO DAGScheduler: Job 11 finished: collect at HoodieSparkEngineContext.java:150, took 0.035042 s
26/02/22 12:32:09 INFO MapPartitionsRDD: Removing RDD 36 from persistence list
26/02/22 12:32:09 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:09 INFO BlockManager: Removing RDD 36
26/02/22 12:32:09 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:09 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/22 12:32:09 INFO DAGScheduler: Registering RDD 47 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 5
26/02/22 12:32:09 INFO DAGScheduler: Registering RDD 37 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 4
26/02/22 12:32:09 INFO DAGScheduler: Registering RDD 55 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 6
26/02/22 12:32:09 INFO DAGScheduler: Got job 12 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/22 12:32:09 INFO DAGScheduler: Final stage: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105)
26/02/22 12:32:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
26/02/22 12:32:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
26/02/22 12:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 270.6 KiB, free 433.5 MiB)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 94.4 KiB, free 433.4 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:35201 (size: 94.4 KiB, free: 434.2 MiB)
26/02/22 12:32:09 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:09 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/22 12:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:32:09 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4319 bytes) taskResourceAssignments Map()
26/02/22 12:32:09 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 29.0 KiB, free 433.4 MiB)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 433.4 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:35201 (size: 13.1 KiB, free: 434.2 MiB)
26/02/22 12:32:09 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:09 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/22 12:32:09 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/22 12:32:09 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:09 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 792 bytes result sent to driver
26/02/22 12:32:09 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 13 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:09 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/22 12:32:09 INFO DAGScheduler: ShuffleMapStage 18 (mapToPair at HoodieJavaRDD.java:149) finished in 0.024 s
26/02/22 12:32:09 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:09 INFO DAGScheduler: running: Set(ShuffleMapStage 19)
26/02/22 12:32:09 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/22 12:32:09 INFO DAGScheduler: failed: Set()
26/02/22 12:32:09 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 1394 bytes result sent to driver
26/02/22 12:32:09 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 64 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:09 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/22 12:32:09 INFO DAGScheduler: ShuffleMapStage 19 (mapToPair at HoodieJavaRDD.java:149) finished in 0.069 s
26/02/22 12:32:09 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:09 INFO DAGScheduler: running: Set()
26/02/22 12:32:09 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/22 12:32:09 INFO DAGScheduler: failed: Set()
26/02/22 12:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.8 KiB, free 433.4 MiB)
26/02/22 12:32:09 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.4 MiB)
26/02/22 12:32:09 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:35201 (size: 5.1 KiB, free: 434.2 MiB)
26/02/22 12:32:09 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:09 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/22 12:32:09 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/22 12:32:09 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:10 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:35201 in memory (size: 93.3 KiB, free: 434.3 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:35201 in memory (size: 94.4 KiB, free: 434.3 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:35201 in memory (size: 13.1 KiB, free: 434.4 MiB)
26/02/22 12:32:10 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 994.0 KiB, free 433.2 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:35201 (size: 994.0 KiB, free: 433.4 MiB)
26/02/22 12:32:10 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 1437 bytes result sent to driver
26/02/22 12:32:10 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 124 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:10 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/22 12:32:10 INFO DAGScheduler: ShuffleMapStage 20 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.130 s
26/02/22 12:32:10 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:10 INFO DAGScheduler: running: Set()
26/02/22 12:32:10 INFO DAGScheduler: waiting: Set(ResultStage 21)
26/02/22 12:32:10 INFO DAGScheduler: failed: Set()
26/02/22 12:32:10 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.5 KiB, free 433.2 MiB)
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.2 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:35201 (size: 3.1 KiB, free: 433.4 MiB)
26/02/22 12:32:10 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:10 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/22 12:32:10 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:10 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
26/02/22 12:32:10 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:10 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1284 bytes result sent to driver
26/02/22 12:32:10 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:10 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/22 12:32:10 INFO DAGScheduler: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.011 s
26/02/22 12:32:10 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/22 12:32:10 INFO DAGScheduler: Job 12 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.228746 s
26/02/22 12:32:10 INFO BaseSparkCommitActionExecutor: Source read and index timer 246
26/02/22 12:32:10 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:32:10 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/22 12:32:10 INFO DAGScheduler: Got job 13 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/22 12:32:10 INFO DAGScheduler: Final stage: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285)
26/02/22 12:32:10 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:10 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:10 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 269.4 KiB, free 432.9 MiB)
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 93.7 KiB, free 432.8 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:35201 (size: 93.7 KiB, free: 433.3 MiB)
26/02/22 12:32:10 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:10 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/22 12:32:10 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:32:10 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
26/02/22 12:32:10 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 790 bytes result sent to driver
26/02/22 12:32:10 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:10 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/22 12:32:10 INFO DAGScheduler: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285) finished in 0.018 s
26/02/22 12:32:10 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/22 12:32:10 INFO DAGScheduler: Job 13 finished: collectAsMap at UpsertPartitioner.java:285, took 0.021811 s
26/02/22 12:32:10 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:10 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:10 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/22 12:32:10 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 9855, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/22 12:32:10 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/22 12:32:10 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/22 12:32:10 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260222123207175.inflight
26/02/22 12:32:10 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:32:10 INFO BaseCommitActionExecutor: Auto commit disabled for 20260222123207175
26/02/22 12:32:10 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1073
26/02/22 12:32:10 INFO DAGScheduler: Registering RDD 59 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 7
26/02/22 12:32:10 INFO DAGScheduler: Got job 14 (count at HoodieSparkSqlWriter.scala:1073) with 1 output partitions
26/02/22 12:32:10 INFO DAGScheduler: Final stage: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073)
26/02/22 12:32:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
26/02/22 12:32:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 26)
26/02/22 12:32:10 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 274.5 KiB, free 432.5 MiB)
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 95.3 KiB, free 432.4 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:35201 (size: 95.3 KiB, free: 433.2 MiB)
26/02/22 12:32:10 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:10 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/22 12:32:10 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/22 12:32:10 INFO Executor: Running task 0.0 in stage 26.0 (TID 20)
26/02/22 12:32:10 INFO BlockManager: Found block rdd_53_0 locally
26/02/22 12:32:10 INFO Executor: Finished task 0.0 in stage 26.0 (TID 20). 1050 bytes result sent to driver
26/02/22 12:32:10 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 50 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:10 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/22 12:32:10 INFO DAGScheduler: ShuffleMapStage 26 (mapToPair at HoodieJavaRDD.java:149) finished in 0.059 s
26/02/22 12:32:10 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:10 INFO DAGScheduler: running: Set()
26/02/22 12:32:10 INFO DAGScheduler: waiting: Set(ResultStage 27)
26/02/22 12:32:10 INFO DAGScheduler: failed: Set()
26/02/22 12:32:10 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073), which has no missing parents
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 313.8 KiB, free 432.1 MiB)
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 108.0 KiB, free 432.0 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:35201 in memory (size: 93.7 KiB, free: 433.3 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:35201 (size: 108.0 KiB, free: 433.2 MiB)
26/02/22 12:32:10 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:10 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
26/02/22 12:32:10 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:10 INFO Executor: Running task 0.0 in stage 27.0 (TID 21)
26/02/22 12:32:10 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:35201 in memory (size: 5.1 KiB, free: 433.2 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:35201 in memory (size: 3.1 KiB, free: 433.2 MiB)
26/02/22 12:32:10 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:10 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:32:10 INFO MarkerHandler: Request: create marker: 05b30081-6220-490c-9dbd-f2e0630349a1-0_0-27-21_20260222123207175.parquet.marker.CREATE
26/02/22 12:32:10 INFO TimelineServerBasedWriteMarkers: [timeline-server-based] Created marker file /05b30081-6220-490c-9dbd-f2e0630349a1-0_0-27-21_20260222123207175.parquet.marker.CREATE in 242 ms
26/02/22 12:32:10 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:32:10 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId 05b30081-6220-490c-9dbd-f2e0630349a1-0
26/02/22 12:32:10 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:35201 in memory (size: 95.3 KiB, free: 433.3 MiB)
26/02/22 12:32:10 INFO HoodieCreateHandle: Closing the file 05b30081-6220-490c-9dbd-f2e0630349a1-0 as we are done with all the records 9855
26/02/22 12:32:10 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID 05b30081-6220-490c-9dbd-f2e0630349a1-0, took 645 ms.
26/02/22 12:32:10 INFO MemoryStore: Block rdd_63_0 stored as values in memory (estimated size 295.0 B, free 432.8 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Added rdd_63_0 in memory on spark-master:35201 (size: 295.0 B, free: 433.3 MiB)
26/02/22 12:32:10 INFO Executor: Finished task 0.0 in stage 27.0 (TID 21). 1749 bytes result sent to driver
26/02/22 12:32:10 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 689 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:10 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/22 12:32:10 INFO DAGScheduler: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073) finished in 0.712 s
26/02/22 12:32:10 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/22 12:32:10 INFO DAGScheduler: Job 14 finished: count at HoodieSparkSqlWriter.scala:1073, took 0.776305 s
26/02/22 12:32:10 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/22 12:32:10 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:107
26/02/22 12:32:10 INFO DAGScheduler: Got job 15 (collect at SparkRDDWriteClient.java:107) with 1 output partitions
26/02/22 12:32:10 INFO DAGScheduler: Final stage: ResultStage 32 (collect at SparkRDDWriteClient.java:107)
26/02/22 12:32:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/22 12:32:10 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:10 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107), which has no missing parents
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 314.4 KiB, free 432.5 MiB)
26/02/22 12:32:10 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 108.4 KiB, free 432.4 MiB)
26/02/22 12:32:10 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:35201 (size: 108.4 KiB, free: 433.2 MiB)
26/02/22 12:32:10 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:10 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
26/02/22 12:32:10 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 22) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:10 INFO Executor: Running task 0.0 in stage 32.0 (TID 22)
26/02/22 12:32:10 INFO BlockManager: Found block rdd_63_0 locally
26/02/22 12:32:10 INFO Executor: Finished task 0.0 in stage 32.0 (TID 22). 1710 bytes result sent to driver
26/02/22 12:32:10 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 22) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:10 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/22 12:32:10 INFO DAGScheduler: ResultStage 32 (collect at SparkRDDWriteClient.java:107) finished in 0.023 s
26/02/22 12:32:10 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/22 12:32:10 INFO DAGScheduler: Job 15 finished: collect at SparkRDDWriteClient.java:107, took 0.026176 s
26/02/22 12:32:10 INFO BaseHoodieWriteClient: Committing 20260222123207175 action commit
26/02/22 12:32:10 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:10 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:10 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123207175__commit__INFLIGHT__20260222123210133]}
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:32:11 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:32:11 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123207175__commit__INFLIGHT__20260222123210133]}
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:32:11 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:32:11 INFO BaseHoodieWriteClient: Committing 20260222123207175 action commit
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:11 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO HoodieTableMetadataUtil: Updating at 20260222123207175 from Commit/UPSERT. #partitions_updated=2, #files_added=1
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:32:11 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:32:11 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:11 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:11 INFO HoodieBackedTableMetadataWriter: New commit at 20260222123207175 being applied to MDT.
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:11 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123208798]}
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:11 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:11 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123207175 action: deltacommit
26/02/22 12:32:11 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123207175__deltacommit__REQUESTED]
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:11 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123207175__deltacommit__REQUESTED__20260222123211229]}
26/02/22 12:32:11 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:11 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:11 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:11 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:32:11 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/22 12:32:11 INFO DAGScheduler: Registering RDD 74 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 8
26/02/22 12:32:11 INFO DAGScheduler: Got job 16 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/22 12:32:11 INFO DAGScheduler: Final stage: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105)
26/02/22 12:32:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
26/02/22 12:32:11 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 33)
26/02/22 12:32:11 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 10.1 KiB, free 432.4 MiB)
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 432.3 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:35201 (size: 5.4 KiB, free: 433.2 MiB)
26/02/22 12:32:11 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:11 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
26/02/22 12:32:11 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 23) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:32:11 INFO Executor: Running task 0.0 in stage 33.0 (TID 23)
26/02/22 12:32:11 INFO MemoryStore: Block rdd_72_0 stored as values in memory (estimated size 342.0 B, free 432.3 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added rdd_72_0 in memory on spark-master:35201 (size: 342.0 B, free: 433.2 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:35201 in memory (size: 108.4 KiB, free: 433.3 MiB)
26/02/22 12:32:11 INFO Executor: Finished task 0.0 in stage 33.0 (TID 23). 1136 bytes result sent to driver
26/02/22 12:32:11 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 23) in 24 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:11 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/22 12:32:11 INFO DAGScheduler: ShuffleMapStage 33 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.029 s
26/02/22 12:32:11 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:11 INFO DAGScheduler: running: Set()
26/02/22 12:32:11 INFO DAGScheduler: waiting: Set(ResultStage 34)
26/02/22 12:32:11 INFO DAGScheduler: failed: Set()
26/02/22 12:32:11 INFO DAGScheduler: Submitting ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 5.5 KiB, free 432.8 MiB)
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.7 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:35201 (size: 3.1 KiB, free: 433.3 MiB)
26/02/22 12:32:11 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:11 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
26/02/22 12:32:11 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 24) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:11 INFO Executor: Running task 0.0 in stage 34.0 (TID 24)
26/02/22 12:32:11 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:35201 in memory (size: 108.0 KiB, free: 433.4 MiB)
26/02/22 12:32:11 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:11 INFO Executor: Finished task 0.0 in stage 34.0 (TID 24). 1366 bytes result sent to driver
26/02/22 12:32:11 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 24) in 34 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:11 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
26/02/22 12:32:11 INFO DAGScheduler: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.038 s
26/02/22 12:32:11 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
26/02/22 12:32:11 INFO DAGScheduler: Job 16 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.071293 s
26/02/22 12:32:11 INFO BaseSparkCommitActionExecutor: Source read and index timer 81
26/02/22 12:32:11 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:32:11 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/22 12:32:11 INFO DAGScheduler: Got job 17 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/22 12:32:11 INFO DAGScheduler: Final stage: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285)
26/02/22 12:32:11 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:11 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:11 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 262.2 KiB, free 432.9 MiB)
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 432.8 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:35201 (size: 90.5 KiB, free: 433.3 MiB)
26/02/22 12:32:11 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:11 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
26/02/22 12:32:11 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 25) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/22 12:32:11 INFO Executor: Running task 0.0 in stage 35.0 (TID 25)
26/02/22 12:32:11 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:11 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:11 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:32:11 INFO Executor: Finished task 0.0 in stage 35.0 (TID 25). 837 bytes result sent to driver
26/02/22 12:32:11 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 25) in 12 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:11 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/22 12:32:11 INFO DAGScheduler: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285) finished in 0.022 s
26/02/22 12:32:11 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/22 12:32:11 INFO DAGScheduler: Job 17 finished: collectAsMap at UpsertPartitioner.java:285, took 0.023308 s
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/22 12:32:11 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260222123207175.deltacommit.inflight
26/02/22 12:32:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:35201 in memory (size: 90.5 KiB, free: 433.4 MiB)
26/02/22 12:32:11 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:32:11 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260222123207175
26/02/22 12:32:11 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:35201 in memory (size: 5.4 KiB, free: 433.4 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:35201 in memory (size: 3.1 KiB, free: 433.4 MiB)
26/02/22 12:32:11 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:32:11 INFO DAGScheduler: Registering RDD 78 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 9
26/02/22 12:32:11 INFO DAGScheduler: Got job 18 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:32:11 INFO DAGScheduler: Final stage: ResultStage 37 (collect at HoodieJavaRDD.java:177)
26/02/22 12:32:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)
26/02/22 12:32:11 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 36)
26/02/22 12:32:11 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 267.0 KiB, free 432.9 MiB)
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 93.0 KiB, free 432.8 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:35201 (size: 93.0 KiB, free: 433.3 MiB)
26/02/22 12:32:11 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:11 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
26/02/22 12:32:11 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 26) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:32:11 INFO Executor: Running task 0.0 in stage 36.0 (TID 26)
26/02/22 12:32:11 INFO BlockManager: Found block rdd_72_0 locally
26/02/22 12:32:11 INFO Executor: Finished task 0.0 in stage 36.0 (TID 26). 1050 bytes result sent to driver
26/02/22 12:32:11 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 26) in 11 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:11 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/22 12:32:11 INFO DAGScheduler: ShuffleMapStage 36 (mapToPair at HoodieJavaRDD.java:149) finished in 0.020 s
26/02/22 12:32:11 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:11 INFO DAGScheduler: running: Set()
26/02/22 12:32:11 INFO DAGScheduler: waiting: Set(ResultStage 37)
26/02/22 12:32:11 INFO DAGScheduler: failed: Set()
26/02/22 12:32:11 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 372.6 KiB, free 432.5 MiB)
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.3 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:35201 (size: 132.2 KiB, free: 433.2 MiB)
26/02/22 12:32:11 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:11 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
26/02/22 12:32:11 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:32:11 INFO Executor: Running task 0.0 in stage 37.0 (TID 27)
26/02/22 12:32:11 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:11 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260222123207175 for file files-0000-0
26/02/22 12:32:11 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:11 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:11 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:11 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:11 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/22 12:32:11 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:32:11 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:32:11 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222123207175/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND
26/02/22 12:32:11 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222123207175/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND in 29 ms
26/02/22 12:32:11 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-37-27', fileLen=-1}
26/02/22 12:32:11 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:32:11 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:32:11 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-37-27, took 391 ms.
26/02/22 12:32:11 INFO MemoryStore: Block rdd_82_0 stored as values in memory (estimated size 343.0 B, free 432.3 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added rdd_82_0 in memory on spark-master:35201 (size: 343.0 B, free: 433.2 MiB)
26/02/22 12:32:11 INFO Executor: Finished task 0.0 in stage 37.0 (TID 27). 1627 bytes result sent to driver
26/02/22 12:32:11 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 27) in 438 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:11 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
26/02/22 12:32:11 INFO DAGScheduler: ResultStage 37 (collect at HoodieJavaRDD.java:177) finished in 0.455 s
26/02/22 12:32:11 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
26/02/22 12:32:11 INFO DAGScheduler: Job 18 finished: collect at HoodieJavaRDD.java:177, took 0.478584 s
26/02/22 12:32:11 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/22 12:32:11 INFO BaseSparkCommitActionExecutor: Committing 20260222123207175, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/22 12:32:11 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:32:11 INFO DAGScheduler: Got job 19 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:32:11 INFO DAGScheduler: Final stage: ResultStage 38 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:32:11 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:11 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:11 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 101.2 KiB, free 432.2 MiB)
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.2 MiB)
26/02/22 12:32:11 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:35201 (size: 36.1 KiB, free: 433.1 MiB)
26/02/22 12:32:11 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:11 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
26/02/22 12:32:11 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:32:11 INFO Executor: Running task 0.0 in stage 38.0 (TID 28)
26/02/22 12:32:11 INFO Executor: Finished task 0.0 in stage 38.0 (TID 28). 804 bytes result sent to driver
26/02/22 12:32:11 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 28) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:11 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/22 12:32:11 INFO DAGScheduler: ResultStage 38 (collect at HoodieSparkEngineContext.java:150) finished in 0.015 s
26/02/22 12:32:11 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/22 12:32:11 INFO DAGScheduler: Job 19 finished: collect at HoodieSparkEngineContext.java:150, took 0.016887 s
26/02/22 12:32:11 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:32:11 INFO DAGScheduler: Got job 20 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:32:11 INFO DAGScheduler: Final stage: ResultStage 39 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:32:11 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:11 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:11 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:32:11 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 101.1 KiB, free 432.1 MiB)
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.2 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:35201 in memory (size: 93.0 KiB, free: 433.2 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:35201 (size: 36.1 KiB, free: 433.2 MiB)
26/02/22 12:32:12 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:12 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:35201 in memory (size: 36.1 KiB, free: 433.2 MiB)
26/02/22 12:32:12 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:32:12 INFO Executor: Running task 0.0 in stage 39.0 (TID 29)
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:35201 in memory (size: 132.2 KiB, free: 433.4 MiB)
26/02/22 12:32:12 INFO Executor: Finished task 0.0 in stage 39.0 (TID 29). 857 bytes result sent to driver
26/02/22 12:32:12 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 29) in 16 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:12 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/22 12:32:12 INFO DAGScheduler: ResultStage 39 (collect at HoodieSparkEngineContext.java:150) finished in 0.027 s
26/02/22 12:32:12 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/22 12:32:12 INFO DAGScheduler: Job 20 finished: collect at HoodieSparkEngineContext.java:150, took 0.029297 s
26/02/22 12:32:12 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123207175__deltacommit__INFLIGHT]
26/02/22 12:32:12 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260222123207175.deltacommit
26/02/22 12:32:12 INFO HoodieActiveTimeline: Completed [==>20260222123207175__deltacommit__INFLIGHT]
26/02/22 12:32:12 INFO BaseSparkCommitActionExecutor: Committed 20260222123207175
26/02/22 12:32:12 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:32:12 INFO DAGScheduler: Got job 21 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/22 12:32:12 INFO DAGScheduler: Final stage: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:32:12 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:12 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:12 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 101.3 KiB, free 433.0 MiB)
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.9 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:35201 (size: 36.2 KiB, free: 433.3 MiB)
26/02/22 12:32:12 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:12 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
26/02/22 12:32:12 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 30) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:32:12 INFO Executor: Running task 0.0 in stage 40.0 (TID 30)
26/02/22 12:32:12 INFO Executor: Finished task 0.0 in stage 40.0 (TID 30). 904 bytes result sent to driver
26/02/22 12:32:12 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 30) in 21 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:12 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
26/02/22 12:32:12 INFO DAGScheduler: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.028 s
26/02/22 12:32:12 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
26/02/22 12:32:12 INFO DAGScheduler: Job 21 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.029105 s
26/02/22 12:32:12 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222123207175
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:32:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123207175__commit__INFLIGHT]
26/02/22 12:32:12 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260222123207175.commit
26/02/22 12:32:12 INFO HoodieActiveTimeline: Completed [==>20260222123207175__commit__INFLIGHT]
26/02/22 12:32:12 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:32:12 INFO DAGScheduler: Got job 22 (collectAsMap at HoodieSparkEngineContext.java:164) with 2 output partitions
26/02/22 12:32:12 INFO DAGScheduler: Final stage: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:32:12 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:12 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:12 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 101.3 KiB, free 432.8 MiB)
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.8 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:35201 in memory (size: 36.2 KiB, free: 433.4 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:35201 (size: 36.2 KiB, free: 433.3 MiB)
26/02/22 12:32:12 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:12 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1))
26/02/22 12:32:12 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks resource profile 0
26/02/22 12:32:12 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4415 bytes) taskResourceAssignments Map()
26/02/22 12:32:12 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 32) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 4411 bytes) taskResourceAssignments Map()
26/02/22 12:32:12 INFO BlockManager: Removing RDD 82
26/02/22 12:32:12 INFO Executor: Running task 0.0 in stage 41.0 (TID 31)
26/02/22 12:32:12 INFO Executor: Running task 1.0 in stage 41.0 (TID 32)
26/02/22 12:32:12 INFO BlockManager: Removing RDD 72
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:35201 in memory (size: 36.1 KiB, free: 433.4 MiB)
26/02/22 12:32:12 INFO Executor: Finished task 1.0 in stage 41.0 (TID 32). 890 bytes result sent to driver
26/02/22 12:32:12 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 32) in 29 ms on spark-master (executor driver) (1/2)
26/02/22 12:32:12 INFO Executor: Finished task 0.0 in stage 41.0 (TID 31). 894 bytes result sent to driver
26/02/22 12:32:12 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 31) in 66 ms on spark-master (executor driver) (2/2)
26/02/22 12:32:12 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/22 12:32:12 INFO DAGScheduler: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.077 s
26/02/22 12:32:12 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/22 12:32:12 INFO DAGScheduler: Job 22 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.078755 s
26/02/22 12:32:12 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/.temp/20260222123207175
26/02/22 12:32:12 INFO BaseHoodieWriteClient: Committed 20260222123207175
26/02/22 12:32:12 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
26/02/22 12:32:12 INFO BlockManager: Removing RDD 53
26/02/22 12:32:12 INFO MapPartitionsRDD: Removing RDD 63 from persistence list
26/02/22 12:32:12 INFO BlockManager: Removing RDD 63
26/02/22 12:32:12 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:32:12 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:32:12 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:32:12 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:32:12 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260222123212245
26/02/22 12:32:12 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:37085, Timeout=300
26/02/22 12:32:12 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:37085/v1/hoodie/view/compactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222123207175&timelinehash=bbfbb319a0b5dc723e529d2af1e9751d3a046c97509b3f9b547996dba356cc35)
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:37085/v1/hoodie/view/logcompactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222123207175&timelinehash=bbfbb319a0b5dc723e529d2af1e9751d3a046c97509b3f9b547996dba356cc35)
26/02/22 12:32:12 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:32:12 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:32:12 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:32:12 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/22 12:32:12 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:32:12 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:37085, Timeout=300
26/02/22 12:32:12 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:37085/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222123207175&timelinehash=bbfbb319a0b5dc723e529d2af1e9751d3a046c97509b3f9b547996dba356cc35)
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__deltacommit__COMPLETED__20260222123212036]}
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Commit 20260222123207175 successful!
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:32:12 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:32:12 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/22 12:32:12 INFO BaseHoodieClient: Stopping Timeline service !!
26/02/22 12:32:12 INFO EmbeddedTimelineService: Closing Timeline server
26/02/22 12:32:12 INFO TimelineService: Closing Timeline Service
26/02/22 12:32:12 INFO Javalin: Stopping Javalin ...
26/02/22 12:32:12 INFO Javalin: Javalin has stopped
26/02/22 12:32:12 INFO TimelineService: Closed Timeline Service
26/02/22 12:32:12 INFO EmbeddedTimelineService: Closed Timeline server
26/02/22 12:32:12 INFO DataSourceUtils: Getting table path..
26/02/22 12:32:12 INFO TablePathUtils: Getting table path from path : s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:32:12 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:32:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123207175__commit__COMPLETED__20260222123212125]}
26/02/22 12:32:12 INFO BaseHoodieTableFileIndex: Refresh table silver_cleaned_trips, spent: 7 ms
26/02/22 12:32:12 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:32:12 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:32:12 INFO DAGScheduler: Got job 23 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:32:12 INFO DAGScheduler: Final stage: ResultStage 42 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:32:12 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:12 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:12 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 101.1 KiB, free 433.9 MiB)
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 433.9 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:35201 (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:32:12 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:12 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
26/02/22 12:32:12 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4368 bytes) taskResourceAssignments Map()
26/02/22 12:32:12 INFO Executor: Running task 0.0 in stage 42.0 (TID 33)
26/02/22 12:32:12 INFO Executor: Finished task 0.0 in stage 42.0 (TID 33). 1131 bytes result sent to driver
26/02/22 12:32:12 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 6 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:12 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/22 12:32:12 INFO DAGScheduler: ResultStage 42 (collect at HoodieSparkEngineContext.java:116) finished in 0.011 s
26/02/22 12:32:12 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/22 12:32:12 INFO DAGScheduler: Job 23 finished: collect at HoodieSparkEngineContext.java:116, took 0.012117 s
26/02/22 12:32:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:32:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:32:12 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:32:12 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:32:12 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 210.5 KiB, free 433.7 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:35201 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 433.9 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:35201 (size: 35.2 KiB, free: 434.3 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:35201 in memory (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:32:12 INFO SparkContext: Created broadcast 35 from count at <unknown>:0
26/02/22 12:32:12 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:32:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:32:12 INFO DAGScheduler: Registering RDD 97 (count at <unknown>:0) as input to shuffle 10
26/02/22 12:32:12 INFO DAGScheduler: Got map stage job 24 (count at <unknown>:0) with 1 output partitions
26/02/22 12:32:12 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (count at <unknown>:0)
26/02/22 12:32:12 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:32:12 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:12 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0), which has no missing parents
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.9 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:35201 (size: 7.4 KiB, free: 434.3 MiB)
26/02/22 12:32:12 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:12 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
26/02/22 12:32:12 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()
26/02/22 12:32:12 INFO Executor: Running task 0.0 in stage 43.0 (TID 34)
26/02/22 12:32:12 INFO BlockManager: Removing RDD 36
26/02/22 12:32:12 INFO BlockManager: Removing RDD 53
26/02/22 12:32:12 INFO FileScanRDD: Reading File path: s3a://warehouse/silver/cleaned_trips/05b30081-6220-490c-9dbd-f2e0630349a1-0_0-27-21_20260222123207175.parquet, range: 0-1019928, partition values: [empty row]
26/02/22 12:32:12 INFO BlockManager: Removing RDD 63
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:35201 in memory (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:32:12 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:12 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:32:12 INFO Executor: Finished task 0.0 in stage 43.0 (TID 34). 2056 bytes result sent to driver
26/02/22 12:32:12 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 34) in 33 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:12 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
26/02/22 12:32:12 INFO DAGScheduler: ShuffleMapStage 43 (count at <unknown>:0) finished in 0.038 s
26/02/22 12:32:12 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:32:12 INFO DAGScheduler: running: Set()
26/02/22 12:32:12 INFO DAGScheduler: waiting: Set()
26/02/22 12:32:12 INFO DAGScheduler: failed: Set()
26/02/22 12:32:12 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:32:12 INFO DAGScheduler: Got job 25 (count at <unknown>:0) with 1 output partitions
26/02/22 12:32:12 INFO DAGScheduler: Final stage: ResultStage 45 (count at <unknown>:0)
26/02/22 12:32:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
26/02/22 12:32:12 INFO DAGScheduler: Missing parents: List()
26/02/22 12:32:12 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0), which has no missing parents
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/22 12:32:12 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:35201 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:32:12 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:35201 (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:32:12 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1509
26/02/22 12:32:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:32:12 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
26/02/22 12:32:12 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:32:12 INFO Executor: Running task 0.0 in stage 45.0 (TID 35)
26/02/22 12:32:12 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:32:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:32:12 INFO Executor: Finished task 0.0 in stage 45.0 (TID 35). 2458 bytes result sent to driver
26/02/22 12:32:12 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 5 ms on spark-master (executor driver) (1/1)
26/02/22 12:32:12 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/22 12:32:12 INFO DAGScheduler: ResultStage 45 (count at <unknown>:0) finished in 0.012 s
26/02/22 12:32:12 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:32:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/22 12:32:12 INFO DAGScheduler: Job 25 finished: count at <unknown>:0, took 0.014084 s
Silver transform complete. Rows written: 9,855
Rows filtered out: 145
============================================================
  Silver Transform (Hudi Upsert) COMPLETE
============================================================
26/02/22 12:32:12 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/22 12:32:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/22 12:32:12 INFO MemoryStore: MemoryStore cleared
26/02/22 12:32:12 INFO BlockManager: BlockManager stopped
26/02/22 12:32:12 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/22 12:32:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/22 12:32:12 INFO SparkContext: Successfully stopped SparkContext
26/02/22 12:32:12 INFO ShutdownHookManager: Shutdown hook called
26/02/22 12:32:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e41a6e8-6eee-4ff6-b14a-6e666743874e
26/02/22 12:32:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e41a6e8-6eee-4ff6-b14a-6e666743874e/pyspark-a8b4f790-a053-4745-b442-f7129a22bce3
26/02/22 12:32:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-55dc3554-602c-4824-aa20-f130ec7ed642
26/02/22 12:32:12 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
26/02/22 12:32:12 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
26/02/22 12:32:12 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
Silver transform complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir ."
 Container p22-minio  Running
[0m12:32:15  Running with dbt=1.11.5
[0m12:32:15  Installing dbt-labs/dbt_utils
[0m12:32:21  Installed from version 1.3.3
[0m12:32:21  Up to date!
[0m12:32:23  Running with dbt=1.11.5
[0m12:32:23  Registered adapter: duckdb=1.10.0
[0m12:32:24  Found 14 models, 74 data tests, 3 seeds, 1 source, 606 macros
[0m12:32:24  
[0m12:32:24  Concurrency: 4 threads (target='dev')
[0m12:32:24  
[0m12:32:25  1 of 91 START sql table model main.dim_dates ................................... [RUN]
[0m12:32:25  2 of 91 START sql view model main.stg_yellow_trips ............................. [RUN]
[0m12:32:25  3 of 91 START seed file main_main.payment_type_lookup .......................... [RUN]
[0m12:32:25  4 of 91 START seed file main_main.rate_code_lookup ............................. [RUN]
[0m12:32:25  3 of 91 OK loaded seed file main_main.payment_type_lookup ...................... [[32mCREATE 6[0m in 0.15s]
[0m12:32:25  4 of 91 OK loaded seed file main_main.rate_code_lookup ......................... [[32mCREATE 7[0m in 0.14s]
[0m12:32:25  5 of 91 START seed file main_main.taxi_zone_lookup ............................. [RUN]
[0m12:32:25  6 of 91 START test not_null_payment_type_lookup_payment_type_id ................ [RUN]
[0m12:32:25  2 of 91 OK created sql view model main.stg_yellow_trips ........................ [[32mOK[0m in 0.17s]
[0m12:32:25  7 of 91 START test unique_payment_type_lookup_payment_type_id .................. [RUN]
[0m12:32:25  1 of 91 OK created sql table model main.dim_dates .............................. [[32mOK[0m in 0.18s]
[0m12:32:25  8 of 91 START test not_null_rate_code_lookup_rate_code_id ...................... [RUN]
[0m12:32:25  6 of 91 PASS not_null_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.06s]
[0m12:32:25  9 of 91 START test unique_rate_code_lookup_rate_code_id ........................ [RUN]
[0m12:32:25  7 of 91 PASS unique_payment_type_lookup_payment_type_id ........................ [[32mPASS[0m in 0.06s]
[0m12:32:25  10 of 91 START test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [RUN]
[0m12:32:25  5 of 91 OK loaded seed file main_main.taxi_zone_lookup ......................... [[32mCREATE 265[0m in 0.08s]
[0m12:32:25  11 of 91 START test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [RUN]
[0m12:32:25  8 of 91 PASS not_null_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.05s]
[0m12:32:25  12 of 91 START test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ........ [RUN]
[0m12:32:25  9 of 91 PASS unique_rate_code_lookup_rate_code_id .............................. [[32mPASS[0m in 0.05s]
[0m12:32:25  13 of 91 START test assert_fare_not_exceeds_total .............................. [RUN]
[0m12:32:25  11 of 91 PASS accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[32mPASS[0m in 0.04s]
[0m12:32:25  10 of 91 PASS accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[32mPASS[0m in 0.05s]
[0m12:32:25  14 of 91 START test not_null_stg_yellow_trips_dropoff_datetime ................. [RUN]
[0m12:32:25  15 of 91 START test not_null_stg_yellow_trips_dropoff_location_id .............. [RUN]
[0m12:32:25  12 of 91 PASS accepted_values_stg_yellow_trips_vendor_id__1__2__6 .............. [[32mPASS[0m in 0.05s]
[0m12:32:25  16 of 91 START test not_null_stg_yellow_trips_fare_amount ...................... [RUN]
[0m12:32:25  14 of 91 PASS not_null_stg_yellow_trips_dropoff_datetime ....................... [[32mPASS[0m in 0.04s]
[0m12:32:25  15 of 91 PASS not_null_stg_yellow_trips_dropoff_location_id .................... [[32mPASS[0m in 0.04s]
[0m12:32:25  13 of 91 PASS assert_fare_not_exceeds_total .................................... [[32mPASS[0m in 0.05s]
[0m12:32:25  17 of 91 START test not_null_stg_yellow_trips_pickup_datetime .................. [RUN]
[0m12:32:25  18 of 91 START test not_null_stg_yellow_trips_pickup_location_id ............... [RUN]
[0m12:32:25  19 of 91 START test not_null_stg_yellow_trips_total_amount ..................... [RUN]
[0m12:32:25  16 of 91 PASS not_null_stg_yellow_trips_fare_amount ............................ [[32mPASS[0m in 0.05s]
[0m12:32:25  20 of 91 START test not_null_stg_yellow_trips_trip_distance_miles .............. [RUN]
[0m12:32:25  17 of 91 PASS not_null_stg_yellow_trips_pickup_datetime ........................ [[32mPASS[0m in 0.05s]
[0m12:32:25  19 of 91 PASS not_null_stg_yellow_trips_total_amount ........................... [[32mPASS[0m in 0.05s]
[0m12:32:25  18 of 91 PASS not_null_stg_yellow_trips_pickup_location_id ..................... [[32mPASS[0m in 0.05s]
[0m12:32:25  21 of 91 START test not_null_stg_yellow_trips_trip_id .......................... [RUN]
[0m12:32:25  22 of 91 START test not_null_stg_yellow_trips_vendor_id ........................ [RUN]
[0m12:32:25  23 of 91 START test unique_stg_yellow_trips_trip_id ............................ [RUN]
[0m12:32:25  20 of 91 PASS not_null_stg_yellow_trips_trip_distance_miles .................... [[32mPASS[0m in 0.09s]
[0m12:32:25  24 of 91 START test not_null_dim_dates_date_key ................................ [RUN]
[0m12:32:25  21 of 91 PASS not_null_stg_yellow_trips_trip_id ................................ [[32mPASS[0m in 0.09s]
[0m12:32:25  22 of 91 PASS not_null_stg_yellow_trips_vendor_id .............................. [[32mPASS[0m in 0.09s]
[0m12:32:25  25 of 91 START test not_null_dim_dates_day_of_week_name ........................ [RUN]
[0m12:32:25  23 of 91 PASS unique_stg_yellow_trips_trip_id .................................. [[32mPASS[0m in 0.09s]
[0m12:32:25  26 of 91 START test not_null_dim_dates_is_holiday .............................. [RUN]
[0m12:32:25  27 of 91 START test not_null_dim_dates_is_weekend .............................. [RUN]
[0m12:32:25  24 of 91 PASS not_null_dim_dates_date_key ...................................... [[32mPASS[0m in 0.06s]
[0m12:32:25  28 of 91 START test unique_dim_dates_date_key .................................. [RUN]
[0m12:32:25  25 of 91 PASS not_null_dim_dates_day_of_week_name .............................. [[32mPASS[0m in 0.05s]
[0m12:32:25  26 of 91 PASS not_null_dim_dates_is_holiday .................................... [[32mPASS[0m in 0.05s]
[0m12:32:25  29 of 91 START sql view model main.stg_payment_types ........................... [RUN]
[0m12:32:25  27 of 91 PASS not_null_dim_dates_is_weekend .................................... [[32mPASS[0m in 0.05s]
[0m12:32:25  30 of 91 START test not_null_taxi_zone_lookup_Borough .......................... [RUN]
[0m12:32:25  31 of 91 START test not_null_taxi_zone_lookup_LocationID ....................... [RUN]
[0m12:32:25  28 of 91 PASS unique_dim_dates_date_key ........................................ [[32mPASS[0m in 0.05s]
[0m12:32:25  32 of 91 START test unique_taxi_zone_lookup_LocationID ......................... [RUN]
[0m12:32:25  31 of 91 PASS not_null_taxi_zone_lookup_LocationID ............................. [[32mPASS[0m in 0.04s]
[0m12:32:25  33 of 91 START sql view model main.stg_rate_codes .............................. [RUN]
[0m12:32:25  30 of 91 PASS not_null_taxi_zone_lookup_Borough ................................ [[32mPASS[0m in 0.05s]
[0m12:32:25  34 of 91 START sql view model main.int_trip_metrics ............................ [RUN]
[0m12:32:25  29 of 91 OK created sql view model main.stg_payment_types ...................... [[32mOK[0m in 0.07s]
[0m12:32:25  35 of 91 START test not_null_stg_payment_types_payment_type_id ................. [RUN]
[0m12:32:25  32 of 91 PASS unique_taxi_zone_lookup_LocationID ............................... [[32mPASS[0m in 0.05s]
[0m12:32:25  36 of 91 START test not_null_stg_payment_types_payment_type_name ............... [RUN]
[0m12:32:25  33 of 91 OK created sql view model main.stg_rate_codes ......................... [[32mOK[0m in 0.06s]
[0m12:32:26  37 of 91 START test unique_stg_payment_types_payment_type_id ................... [RUN]
[0m12:32:26  35 of 91 PASS not_null_stg_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m12:32:26  34 of 91 OK created sql view model main.int_trip_metrics ....................... [[32mOK[0m in 0.07s]
[0m12:32:26  36 of 91 PASS not_null_stg_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m12:32:26  38 of 91 START sql view model main.stg_taxi_zones .............................. [RUN]
[0m12:32:26  39 of 91 START test not_null_stg_rate_codes_rate_code_id ....................... [RUN]
[0m12:32:26  40 of 91 START test not_null_stg_rate_codes_rate_code_name ..................... [RUN]
[0m12:32:26  37 of 91 PASS unique_stg_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m12:32:26  41 of 91 START test unique_stg_rate_codes_rate_code_id ......................... [RUN]
[0m12:32:26  40 of 91 PASS not_null_stg_rate_codes_rate_code_name ........................... [[32mPASS[0m in 0.05s]
[0m12:32:26  38 of 91 OK created sql view model main.stg_taxi_zones ......................... [[32mOK[0m in 0.05s]
[0m12:32:26  39 of 91 PASS not_null_stg_rate_codes_rate_code_id ............................. [[32mPASS[0m in 0.05s]
[0m12:32:26  42 of 91 START test assert_trip_duration_positive .............................. [RUN]
[0m12:32:26  43 of 91 START test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [RUN]
[0m12:32:26  44 of 91 START test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [RUN]
[0m12:32:26  41 of 91 PASS unique_stg_rate_codes_rate_code_id ............................... [[32mPASS[0m in 0.05s]
[0m12:32:26  45 of 91 START test not_null_int_trip_metrics_is_weekend ....................... [RUN]
[0m12:32:26  42 of 91 PASS assert_trip_duration_positive .................................... [[32mPASS[0m in 0.04s]
[0m12:32:26  46 of 91 START test not_null_int_trip_metrics_pickup_date ...................... [RUN]
[0m12:32:26  43 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0 ..... [[32mPASS[0m in 0.05s]
[0m12:32:26  47 of 91 START test not_null_int_trip_metrics_pickup_hour ...................... [RUN]
[0m12:32:26  45 of 91 PASS not_null_int_trip_metrics_is_weekend ............................. [[32mPASS[0m in 0.04s]
[0m12:32:26  44 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[32mPASS[0m in 0.08s]
[0m12:32:26  48 of 91 START test not_null_int_trip_metrics_trip_duration_minutes ............ [RUN]
[0m12:32:26  49 of 91 START test not_null_int_trip_metrics_trip_id .......................... [RUN]
[0m12:32:26  46 of 91 PASS not_null_int_trip_metrics_pickup_date ............................ [[32mPASS[0m in 0.05s]
[0m12:32:26  50 of 91 START sql table model main.dim_payment_types .......................... [RUN]
[0m12:32:26  47 of 91 PASS not_null_int_trip_metrics_pickup_hour ............................ [[32mPASS[0m in 0.05s]
[0m12:32:26  51 of 91 START test not_null_stg_taxi_zones_borough ............................ [RUN]
[0m12:32:26  49 of 91 PASS not_null_int_trip_metrics_trip_id ................................ [[32mPASS[0m in 0.04s]
[0m12:32:26  52 of 91 START test not_null_stg_taxi_zones_location_id ........................ [RUN]
[0m12:32:26  48 of 91 PASS not_null_int_trip_metrics_trip_duration_minutes .................. [[32mPASS[0m in 0.06s]
[0m12:32:26  53 of 91 START test not_null_stg_taxi_zones_zone_name .......................... [RUN]
[0m12:32:26  51 of 91 PASS not_null_stg_taxi_zones_borough .................................. [[32mPASS[0m in 0.05s]
[0m12:32:26  54 of 91 START test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:32:26  52 of 91 PASS not_null_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.04s]
[0m12:32:26  50 of 91 OK created sql table model main.dim_payment_types ..................... [[32mOK[0m in 0.08s]
[0m12:32:26  55 of 91 START test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:32:26  56 of 91 START test unique_stg_taxi_zones_location_id .......................... [RUN]
[0m12:32:26  53 of 91 PASS not_null_stg_taxi_zones_zone_name ................................ [[32mPASS[0m in 0.05s]
[0m12:32:26  57 of 91 START sql view model main.int_daily_summary ........................... [RUN]
[0m12:32:26  54 of 91 PASS relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.05s]
[0m12:32:26  58 of 91 START sql view model main.int_hourly_patterns ......................... [RUN]
[0m12:32:26  55 of 91 PASS relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.04s]
[0m12:32:26  59 of 91 START test not_null_dim_payment_types_payment_type_id ................. [RUN]
[0m12:32:26  56 of 91 PASS unique_stg_taxi_zones_location_id ................................ [[32mPASS[0m in 0.07s]
[0m12:32:26  60 of 91 START test not_null_dim_payment_types_payment_type_name ............... [RUN]
[0m12:32:26  57 of 91 OK created sql view model main.int_daily_summary ...................... [[32mOK[0m in 0.05s]
[0m12:32:26  61 of 91 START test unique_dim_payment_types_payment_type_id ................... [RUN]
[0m12:32:26  59 of 91 PASS not_null_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.05s]
[0m12:32:26  58 of 91 OK created sql view model main.int_hourly_patterns .................... [[32mOK[0m in 0.06s]
[0m12:32:26  62 of 91 START sql table model main.dim_locations .............................. [RUN]
[0m12:32:26  63 of 91 START test dbt_utils_accepted_range_int_daily_summary_total_trips__0 .. [RUN]
[0m12:32:26  60 of 91 PASS not_null_dim_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.06s]
[0m12:32:26  64 of 91 START test not_null_int_daily_summary_pickup_date ..................... [RUN]
[0m12:32:26  61 of 91 PASS unique_dim_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m12:32:26  65 of 91 START test not_null_int_daily_summary_total_revenue ................... [RUN]
[0m12:32:26  64 of 91 PASS not_null_int_daily_summary_pickup_date ........................... [[32mPASS[0m in 0.05s]
[0m12:32:26  66 of 91 START test not_null_int_daily_summary_total_trips ..................... [RUN]
[0m12:32:26  63 of 91 PASS dbt_utils_accepted_range_int_daily_summary_total_trips__0 ........ [[32mPASS[0m in 0.08s]
[0m12:32:26  62 of 91 OK created sql table model main.dim_locations ......................... [[32mOK[0m in 0.08s]
[0m12:32:26  67 of 91 START test unique_int_daily_summary_pickup_date ....................... [RUN]
[0m12:32:26  68 of 91 START test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [RUN]
[0m12:32:26  65 of 91 PASS not_null_int_daily_summary_total_revenue ......................... [[32mPASS[0m in 0.06s]
[0m12:32:26  69 of 91 START test not_null_int_hourly_patterns_pickup_date ................... [RUN]
[0m12:32:26  68 of 91 PASS dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0 .. [[32mPASS[0m in 0.03s]
[0m12:32:26  70 of 91 START test not_null_int_hourly_patterns_pickup_hour ................... [RUN]
[0m12:32:26  69 of 91 PASS not_null_int_hourly_patterns_pickup_date ......................... [[32mPASS[0m in 0.03s]
[0m12:32:26  71 of 91 START test not_null_int_hourly_patterns_total_trips ................... [RUN]
[0m12:32:26  66 of 91 PASS not_null_int_daily_summary_total_trips ........................... [[32mPASS[0m in 0.07s]
[0m12:32:26  72 of 91 START test not_null_dim_locations_borough ............................. [RUN]
[0m12:32:26  67 of 91 PASS unique_int_daily_summary_pickup_date ............................. [[32mPASS[0m in 0.07s]
[0m12:32:26  73 of 91 START test not_null_dim_locations_location_id ......................... [RUN]
[0m12:32:26  70 of 91 PASS not_null_int_hourly_patterns_pickup_hour ......................... [[32mPASS[0m in 0.05s]
[0m12:32:26  74 of 91 START test not_null_dim_locations_zone_name ........................... [RUN]
[0m12:32:26  73 of 91 PASS not_null_dim_locations_location_id ............................... [[32mPASS[0m in 0.04s]
[0m12:32:26  75 of 91 START test unique_dim_locations_location_id ........................... [RUN]
[0m12:32:26  72 of 91 PASS not_null_dim_locations_borough ................................... [[32mPASS[0m in 0.06s]
[0m12:32:26  71 of 91 PASS not_null_int_hourly_patterns_total_trips ......................... [[32mPASS[0m in 0.07s]
[0m12:32:26  76 of 91 START sql table model main.mart_daily_revenue ......................... [RUN]
[0m12:32:26  77 of 91 START sql table model main.mart_hourly_demand ......................... [RUN]
[0m12:32:26  74 of 91 PASS not_null_dim_locations_zone_name ................................. [[32mPASS[0m in 0.05s]
[0m12:32:26  75 of 91 PASS unique_dim_locations_location_id ................................. [[32mPASS[0m in 0.05s]
[0m12:32:26  76 of 91 ERROR creating sql table model main.mart_daily_revenue ................ [[31mERROR[0m in 0.05s]
[0m12:32:26  78 of 91 START sql incremental model main.fct_trips ............................ [RUN]
[0m12:32:26  79 of 91 SKIP test not_null_mart_daily_revenue_date_key ........................ [[33mSKIP[0m]
[0m12:32:26  80 of 91 SKIP test not_null_mart_daily_revenue_total_revenue ................... [[33mSKIP[0m]
[0m12:32:26  81 of 91 SKIP test unique_mart_daily_revenue_date_key .......................... [[33mSKIP[0m]
[0m12:32:26  78 of 91 ERROR creating sql incremental model main.fct_trips ................... [[31mERROR[0m in 0.05s]
[0m12:32:26  82 of 91 SKIP test not_null_fct_trips_pickup_datetime .......................... [[33mSKIP[0m]
[0m12:32:26  83 of 91 SKIP test not_null_fct_trips_total_amount ............................. [[33mSKIP[0m]
[0m12:32:26  84 of 91 SKIP test not_null_fct_trips_trip_id .................................. [[33mSKIP[0m]
[0m12:32:26  85 of 91 SKIP test unique_fct_trips_trip_id .................................... [[33mSKIP[0m]
[0m12:32:26  86 of 91 SKIP relation main.mart_location_performance .......................... [[33mSKIP[0m]
[0m12:32:26  87 of 91 SKIP test not_null_mart_location_performance_pickup_location_id ....... [[33mSKIP[0m]
[0m12:32:26  88 of 91 SKIP test not_null_mart_location_performance_total_pickups ............ [[33mSKIP[0m]
[0m12:32:26  89 of 91 SKIP test unique_mart_location_performance_pickup_location_id ......... [[33mSKIP[0m]
[0m12:32:26  77 of 91 OK created sql table model main.mart_hourly_demand .................... [[32mOK[0m in 0.12s]
[0m12:32:26  90 of 91 START test not_null_mart_hourly_demand_is_weekend ..................... [RUN]
[0m12:32:26  91 of 91 START test not_null_mart_hourly_demand_pickup_hour .................... [RUN]
[0m12:32:26  90 of 91 PASS not_null_mart_hourly_demand_is_weekend ........................... [[32mPASS[0m in 0.09s]
[0m12:32:26  91 of 91 PASS not_null_mart_hourly_demand_pickup_hour .......................... [[32mPASS[0m in 0.09s]
[0m12:32:26  
[0m12:32:26  Finished running 1 incremental model, 3 seeds, 6 table models, 74 data tests, 7 view models in 0 hours 0 minutes and 2.44 seconds (2.44s).
[0m12:32:26  
[0m12:32:26  [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m12:32:26  
[0m12:32:26  [31mFailure in model mart_daily_revenue (models/marts/analytics/mart_daily_revenue.sql)[0m
[0m12:32:26    Compilation Error in model mart_daily_revenue (models/marts/analytics/mart_daily_revenue.sql)
  This model has an enforced contract that failed.
  Please ensure the name, data_type, and number of columns in your contract match the columns in your model's definition.
  
  | column_name                 | definition_type | contract_type | mismatch_reason    |
  | --------------------------- | --------------- | ------------- | ------------------ |
  | cumulative_revenue          | DOUBLE          | DECIMAL(38,2) | data type mismatch |
  | revenue_change_vs_prior_day | DOUBLE          | DECIMAL(38,2) | data type mismatch |
  | total_fare_revenue          | DOUBLE          | DECIMAL(38,2) | data type mismatch |
  | total_revenue               | DOUBLE          | DECIMAL(38,2) | data type mismatch |
  | total_tip_revenue           | DOUBLE          | DECIMAL(38,2) | data type mismatch |
  
  
  > in macro assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro default__get_assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro get_assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro duckdb__create_table_as (macros/adapters.sql)
  > called by macro create_table_as (macros/relations/table/create.sql)
  > called by macro statement (macros/etc/statement.sql)
  > called by macro materialization_table_duckdb (macros/materializations/table.sql)
  > called by model mart_daily_revenue (models/marts/analytics/mart_daily_revenue.sql)
[0m12:32:26  
[0m12:32:26    compiled code at target/compiled/nyc_taxi_pipeline_22/models/marts/analytics/mart_daily_revenue.sql
[0m12:32:26  
[0m12:32:26  [31mFailure in model fct_trips (models/marts/core/fct_trips.sql)[0m
[0m12:32:26    Compilation Error in model fct_trips (models/marts/core/fct_trips.sql)
  This model has an enforced contract that failed.
  Please ensure the name, data_type, and number of columns in your contract match the columns in your model's definition.
  
  | column_name           | definition_type          | contract_type | mismatch_reason    |
  | --------------------- | ------------------------ | ------------- | ------------------ |
  | airport_fee           | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  | congestion_surcharge  | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  | dropoff_datetime      | TIMESTAMP WITH TIME ZONE | TIMESTAMP     | data type mismatch |
  | fare_amount           | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  | improvement_surcharge | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  | mta_tax               | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  | pickup_datetime       | TIMESTAMP WITH TIME ZONE | TIMESTAMP     | data type mismatch |
  | tip_amount            | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  | tolls_amount          | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  | total_amount          | DOUBLE                   | DECIMAL(10,2) | data type mismatch |
  
  
  > in macro assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro default__get_assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro get_assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro duckdb__create_table_as (macros/adapters.sql)
  > called by macro create_table_as (macros/relations/table/create.sql)
  > called by macro materialization_incremental_duckdb (macros/materializations/incremental.sql)
  > called by model fct_trips (models/marts/core/fct_trips.sql)
[0m12:32:26  
[0m12:32:26    compiled code at target/compiled/nyc_taxi_pipeline_22/models/marts/core/fct_trips.sql
[0m12:32:26  
[0m12:32:26  Done. PASS=78 WARN=0 ERROR=2 SKIP=11 NO-OP=0 TOTAL=91

make[1]: *** [Makefile:68: dbt-build] Error 1
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make: *** [Makefile:75: benchmark] Error 2
docker compose --profile generator --profile dbt down -v --remove-orphans
 Container p22-spark-worker  Stopping
 Container p22-spark-worker  Stopped
 Container p22-spark-worker  Removing
 Container p22-spark-worker  Removed
 Container p22-spark-master  Stopping
 Container p22-spark-master  Stopped
 Container p22-spark-master  Removing
 Container p22-spark-master  Removed
 Container p22-mc-init  Stopping
 Container p22-kafka  Stopping
 Container p22-mc-init  Stopped
 Container p22-mc-init  Removing
 Container p22-mc-init  Removed
 Container p22-minio  Stopping
 Container p22-minio  Stopped
 Container p22-minio  Removing
 Container p22-minio  Removed
 Container p22-kafka  Stopped
 Container p22-kafka  Removing
 Container p22-kafka  Removed
 Network p22-pipeline-net  Removing
 Volume 22-hudi-cdc-storage_minio-data  Removing
 Volume 22-hudi-cdc-storage_minio-data  Removed
 Network p22-pipeline-net  Removed
Pipeline 22 stopped.
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
Pipeline 22 cleaned.
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-minio  Creating
 Container p22-kafka  Creating
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-kafka  Created
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-minio  Starting
 Container p22-kafka  Starting
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-kafka  Started
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-mc-init  Waiting
 Container p22-kafka  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
============================================================
  Pipeline 22 Benchmark: Kafka + Spark + Apache Hudi
============================================================
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-minio  Creating
 Container p22-kafka  Creating
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-kafka  Created
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-minio  Starting
 Container p22-kafka  Starting
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-kafka  Started
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-kafka  Waiting
 Container p22-mc-init  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[2]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[2]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm \
	-e MODE=burst \
	-e MAX_EVENTS=10000 \
	data-generator
 Container p22-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.38s
  Rate:    26,410 events/sec
============================================================
  Metrics written to /tmp/generator_metrics.json
Limited data generation complete (10k events).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/bronze_ingest.py
26/02/22 12:37:27 INFO SparkContext: Running Spark version 3.3.3
26/02/22 12:37:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/22 12:37:27 INFO ResourceUtils: ==============================================================
26/02/22 12:37:27 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/22 12:37:27 INFO ResourceUtils: ==============================================================
26/02/22 12:37:27 INFO SparkContext: Submitted application: P22-Bronze-Ingest-Hudi
26/02/22 12:37:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/22 12:37:27 INFO ResourceProfile: Limiting resource is cpu
26/02/22 12:37:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/22 12:37:27 INFO SecurityManager: Changing view acls to: spark
26/02/22 12:37:27 INFO SecurityManager: Changing modify acls to: spark
26/02/22 12:37:27 INFO SecurityManager: Changing view acls groups to: 
26/02/22 12:37:27 INFO SecurityManager: Changing modify acls groups to: 
26/02/22 12:37:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/22 12:37:27 INFO Utils: Successfully started service 'sparkDriver' on port 41033.
26/02/22 12:37:27 INFO SparkEnv: Registering MapOutputTracker
26/02/22 12:37:27 INFO SparkEnv: Registering BlockManagerMaster
26/02/22 12:37:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/22 12:37:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/22 12:37:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/22 12:37:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-06da6d2d-0617-49cf-b4c2-5bce180a6df3
26/02/22 12:37:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/22 12:37:27 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/22 12:37:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/22 12:37:27 INFO Executor: Starting executor ID driver on host spark-master
26/02/22 12:37:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/22 12:37:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38955.
26/02/22 12:37:27 INFO NettyBlockTransferService: Server created on spark-master:38955
26/02/22 12:37:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/22 12:37:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 38955, None)
26/02/22 12:37:27 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:38955 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 38955, None)
26/02/22 12:37:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 38955, None)
26/02/22 12:37:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 38955, None)
26/02/22 12:37:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/22 12:37:28 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/22 12:37:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/22 12:37:29 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:37:29 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/22 12:37:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:30 WARN HoodieStreamingSink: Ignore TableNotFoundException as it is first microbatch.
26/02/22 12:37:30 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/22 12:37:30 INFO ResolveWriteToStream: Checkpoint root s3a://warehouse/checkpoints/bronze resolved to s3a://warehouse/checkpoints/bronze.
26/02/22 12:37:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/22 12:37:30 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
26/02/22 12:37:30 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/metadata using temp file s3a://warehouse/checkpoints/bronze/.metadata.c3521a65-544a-4974-a4ac-8ad3213abb3f.tmp
26/02/22 12:37:30 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/.metadata.c3521a65-544a-4974-a4ac-8ad3213abb3f.tmp to s3a://warehouse/checkpoints/bronze/metadata
26/02/22 12:37:30 INFO MicroBatchExecution: Starting [id = c44f2a56-fbf1-400e-879d-db826dcc4c57, runId = f18f3cb5-4351-458b-bdf5-b88b0f13203e]. Use s3a://warehouse/checkpoints/bronze to store the query checkpoint.
26/02/22 12:37:30 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@52bcc4df] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1a8dbb3b]
26/02/22 12:37:30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:37:30 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:37:30 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:37:30 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:37:30 INFO AppInfoParser: Kafka startTimeMs: 1771763850557
26/02/22 12:37:30 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Subscribed to topic(s): taxi.raw_trips
26/02/22 12:37:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] (Re-)joining group
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:37:30 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:37:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] (Re-)joining group
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:37:31 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] (Re-)joining group
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:37:31 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] (Re-)joining group
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Request joining group due to: need to re-join with the given member-id: consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1-ec596a27-76a1-4445-9f8f-4db81c4928e9
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] (Re-)joining group
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1-ec596a27-76a1-4445-9f8f-4db81c4928e9', protocol='range'}
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1-ec596a27-76a1-4445-9f8f-4db81c4928e9=Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])}
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1-ec596a27-76a1-4445-9f8f-4db81c4928e9', protocol='range'}
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Notifying assignor about the new Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])
26/02/22 12:37:31 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Adding newly assigned partitions: taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-0
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-1
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-2
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-3
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-4
26/02/22 12:37:31 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-5
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/sources/0/0 using temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.4be287ec-8ee4-48cb-ac09-c9704f8a78dd.tmp
26/02/22 12:37:31 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.4be287ec-8ee4-48cb-ac09-c9704f8a78dd.tmp to s3a://warehouse/checkpoints/bronze/sources/0/0
26/02/22 12:37:31 INFO KafkaMicroBatchStream: Initial offsets: {"taxi.raw_trips":{"2":0,"5":0,"4":0,"1":0,"3":0,"0":0}}
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:31 INFO MicroBatchExecution: Starting new streaming query.
26/02/22 12:37:31 INFO MicroBatchExecution: Stream started from {}
26/02/22 12:37:31 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/offsets/0 using temp file s3a://warehouse/checkpoints/bronze/offsets/.0.c79b3d11-ec16-4ed9-a29e-7a8da2b50264.tmp
26/02/22 12:37:31 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/offsets/.0.c79b3d11-ec16-4ed9-a29e-7a8da2b50264.tmp to s3a://warehouse/checkpoints/bronze/offsets/0
26/02/22 12:37:31 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1771763851688,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
26/02/22 12:37:31 INFO IncrementalExecution: Current batch timestamp = 1771763851688
26/02/22 12:37:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:37:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:37:32 INFO IncrementalExecution: Current batch timestamp = 1771763851688
26/02/22 12:37:32 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:37:32 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:37:32 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/22 12:37:32 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/22 12:37:32 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips as hoodie table s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:32 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark
26/02/22 12:37:32 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:32 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:32 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:37:32 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:32 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:32 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:32 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123732236 action: commit
26/02/22 12:37:32 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123732236__commit__REQUESTED]
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123732236__commit__REQUESTED__20260222123732591]}
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:32 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/22 12:37:32 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123732236__commit__REQUESTED__20260222123732591]}
26/02/22 12:37:32 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips/.hoodie/metadata as hoodie table s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:32 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:32 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:32 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:32 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:32 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
26/02/22 12:37:32 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:32 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.5 KiB, free 434.3 MiB)
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:38955 (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:37:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/22 12:37:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4401 bytes) taskResourceAssignments Map()
26/02/22 12:37:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/22 12:37:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 926 bytes result sent to driver
26/02/22 12:37:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 88 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/22 12:37:33 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 0.322 s
26/02/22 12:37:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/22 12:37:33 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 0.345312 s
26/02/22 12:37:33 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:33 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/22 12:37:33 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/22 12:37:33 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:33 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:33 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
26/02/22 12:37:33 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:33 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:33 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KiB, free 434.3 MiB)
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.3 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:38955 (size: 1866.0 B, free: 434.4 MiB)
26/02/22 12:37:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/22 12:37:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:37:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/22 12:37:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 760 bytes result sent to driver
26/02/22 12:37:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/22 12:37:33 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 0.016 s
26/02/22 12:37:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/22 12:37:33 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 0.018175 s
26/02/22 12:37:33 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/22 12:37:33 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/22 12:37:33 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:33 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:33 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
26/02/22 12:37:33 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:33 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:33 INFO DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 393.8 KiB, free 433.9 MiB)
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 129.8 KiB, free 433.7 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:38955 (size: 129.8 KiB, free: 434.2 MiB)
26/02/22 12:37:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/22 12:37:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/22 12:37:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
26/02/22 12:37:33 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:37:33 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:37:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:38955 in memory (size: 1866.0 B, free: 434.2 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:38955 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:37:33 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/22 12:37:33 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/22 12:37:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 846 bytes result sent to driver
26/02/22 12:37:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 64 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/22 12:37:33 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 0.088 s
26/02/22 12:37:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
26/02/22 12:37:33 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 0.090161 s
26/02/22 12:37:33 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:33 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:33 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:37:33 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:37:33 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:33 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:33 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:33 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:33 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:33 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:33 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:33 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:33 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:33 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:33 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:33 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:33 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:33 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/22 12:37:33 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:33 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:33 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260222123733448]}
26/02/22 12:37:33 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:33 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:33 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:33 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:37:33 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:37:33 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/22 12:37:33 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:33 INFO DAGScheduler: Registering RDD 14 (start at <unknown>:0) as input to shuffle 0
26/02/22 12:37:33 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:33 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
26/02/22 12:37:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/22 12:37:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
26/02/22 12:37:33 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.5 KiB, free 433.9 MiB)
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.9 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:38955 (size: 4.7 KiB, free: 434.3 MiB)
26/02/22 12:37:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/22 12:37:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/22 12:37:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
26/02/22 12:37:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 964 bytes result sent to driver
26/02/22 12:37:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/22 12:37:33 INFO DAGScheduler: ShuffleMapStage 3 (start at <unknown>:0) finished in 0.046 s
26/02/22 12:37:33 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:33 INFO DAGScheduler: running: Set()
26/02/22 12:37:33 INFO DAGScheduler: waiting: Set(ResultStage 4)
26/02/22 12:37:33 INFO DAGScheduler: failed: Set()
26/02/22 12:37:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.3 KiB, free 433.9 MiB)
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.9 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:38955 (size: 3.9 KiB, free: 434.3 MiB)
26/02/22 12:37:33 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/22 12:37:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:33 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
26/02/22 12:37:33 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
26/02/22 12:37:33 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1246 bytes result sent to driver
26/02/22 12:37:33 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 48 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:33 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/22 12:37:33 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.054 s
26/02/22 12:37:33 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/22 12:37:33 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.120511 s
26/02/22 12:37:33 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:37:33 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/22 12:37:33 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:33 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:33 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
26/02/22 12:37:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
26/02/22 12:37:33 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:33 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 267.1 KiB, free 433.6 MiB)
26/02/22 12:37:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 433.5 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:38955 (size: 91.4 KiB, free: 434.2 MiB)
26/02/22 12:37:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/22 12:37:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/22 12:37:33 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:33 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:37:33 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE
26/02/22 12:37:33 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE in 28 ms
26/02/22 12:37:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:38955 in memory (size: 129.8 KiB, free: 434.3 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:38955 in memory (size: 4.7 KiB, free: 434.3 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:38955 in memory (size: 3.9 KiB, free: 434.3 MiB)
26/02/22 12:37:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/22 12:37:33 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:37:33 INFO MetricsSystemImpl: HBase metrics system started
26/02/22 12:37:33 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/22 12:37:33 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:33 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:33 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/22 12:37:33 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/22 12:37:33 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 253 ms.
26/02/22 12:37:33 INFO MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 271.0 B, free 434.0 MiB)
26/02/22 12:37:33 INFO BlockManagerInfo: Added rdd_19_0 in memory on spark-master:38955 (size: 271.0 B, free: 434.3 MiB)
26/02/22 12:37:33 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 1481 bytes result sent to driver
26/02/22 12:37:33 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 286 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:33 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/22 12:37:33 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.297 s
26/02/22 12:37:33 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/22 12:37:33 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.303431 s
26/02/22 12:37:33 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:37:33 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/22 12:37:34 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:34 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:34 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
26/02/22 12:37:34 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:34 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:34 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:34 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 101.2 KiB, free 434.0 MiB)
26/02/22 12:37:34 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.9 MiB)
26/02/22 12:37:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:38955 (size: 36.1 KiB, free: 434.3 MiB)
26/02/22 12:37:34 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:34 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/22 12:37:34 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/22 12:37:34 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 854 bytes result sent to driver
26/02/22 12:37:34 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 12 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:34 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/22 12:37:34 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.021 s
26/02/22 12:37:34 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/22 12:37:34 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.023635 s
26/02/22 12:37:34 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:37:34 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/22 12:37:34 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:37:34 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/22 12:37:34 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:34 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:34 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
26/02/22 12:37:34 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:34 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:34 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:34 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 101.3 KiB, free 433.8 MiB)
26/02/22 12:37:34 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.8 MiB)
26/02/22 12:37:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:38955 (size: 36.2 KiB, free: 434.2 MiB)
26/02/22 12:37:34 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:34 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/22 12:37:34 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/22 12:37:34 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 900 bytes result sent to driver
26/02/22 12:37:34 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 31 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:34 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/22 12:37:34 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.039 s
26/02/22 12:37:34 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/22 12:37:34 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.041715 s
26/02/22 12:37:34 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: MDT s3a://warehouse/bronze/raw_trips partition FILES has been enabled
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:34 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 993 in ms
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/22 12:37:34 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:37:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:34 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:37:34 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123732236__commit__REQUESTED__20260222123732591]}
26/02/22 12:37:34 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/22 12:37:34 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123732236__commit__REQUESTED__20260222123732591]}
26/02/22 12:37:34 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:37:34 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:34 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:34 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:34 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:37:34 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:37:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:34 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:34 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:38955 in memory (size: 91.4 KiB, free: 434.3 MiB)
26/02/22 12:37:34 INFO DAGScheduler: Registering RDD 26 (start at <unknown>:0) as input to shuffle 1
26/02/22 12:37:34 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 6 output partitions
26/02/22 12:37:34 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
26/02/22 12:37:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/22 12:37:34 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
26/02/22 12:37:34 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:38955 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:37:34 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:38955 in memory (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:37:34 INFO BlockManager: Removing RDD 19
26/02/22 12:37:34 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 90.9 KiB, free 434.3 MiB)
26/02/22 12:37:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 434.3 MiB)
26/02/22 12:37:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:38955 (size: 31.0 KiB, free: 434.4 MiB)
26/02/22 12:37:34 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:34 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:37:34 INFO TaskSchedulerImpl: Adding task set 9.0 with 6 tasks resource profile 0
26/02/22 12:37:34 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 9) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 10) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 11) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 12) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 13) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:34 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
26/02/22 12:37:34 INFO Executor: Running task 1.0 in stage 9.0 (TID 9)
26/02/22 12:37:34 INFO Executor: Running task 2.0 in stage 9.0 (TID 10)
26/02/22 12:37:34 INFO Executor: Running task 5.0 in stage 9.0 (TID 13)
26/02/22 12:37:34 INFO Executor: Running task 4.0 in stage 9.0 (TID 12)
26/02/22 12:37:34 INFO Executor: Running task 3.0 in stage 9.0 (TID 11)
26/02/22 12:37:34 INFO CodeGenerator: Code generated in 99.776484 ms
26/02/22 12:37:34 INFO CodeGenerator: Code generated in 11.794306 ms
26/02/22 12:37:34 INFO CodeGenerator: Code generated in 13.693634 ms
26/02/22 12:37:34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:37:34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:37:34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:37:34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:37:34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:37:34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:37:34 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:37:34 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:37:34 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:37:34 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:37:34 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:37:34 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:37:34 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:37:34 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:37:34 INFO AppInfoParser: Kafka startTimeMs: 1771763854756
26/02/22 12:37:34 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:37:34 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Assigned to partition(s): taxi.raw_trips-3
26/02/22 12:37:34 INFO AppInfoParser: Kafka startTimeMs: 1771763854756
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Assigned to partition(s): taxi.raw_trips-4
26/02/22 12:37:34 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:37:34 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:37:34 INFO AppInfoParser: Kafka startTimeMs: 1771763854757
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Assigned to partition(s): taxi.raw_trips-5
26/02/22 12:37:34 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:37:34 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:37:34 INFO AppInfoParser: Kafka startTimeMs: 1771763854757
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Assigned to partition(s): taxi.raw_trips-2
26/02/22 12:37:34 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:37:34 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:37:34 INFO AppInfoParser: Kafka startTimeMs: 1771763854757
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Assigned to partition(s): taxi.raw_trips-0
26/02/22 12:37:34 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:37:34 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:37:34 INFO AppInfoParser: Kafka startTimeMs: 1771763854758
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Assigned to partition(s): taxi.raw_trips-1
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-3
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-2
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-1
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-5
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-4
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-0
26/02/22 12:37:34 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:37:34 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:37:34 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:37:34 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:37:34 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:37:34 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-2
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-0
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-4
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-3
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-1
26/02/22 12:37:34 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-5
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-1
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-2
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-3
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-5
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-4
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-1
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-3
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 2000 for partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to offset 2500 for partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO MemoryStore: Block rdd_6_3 stored as values in memory (estimated size 49.2 KiB, free 434.2 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added rdd_6_3 in memory on spark-master:38955 (size: 49.2 KiB, free: 434.3 MiB)
26/02/22 12:37:35 INFO Executor: Finished task 3.0 in stage 9.0 (TID 11). 1753 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 11) in 1092 ms on spark-master (executor driver) (1/6)
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO MemoryStore: Block rdd_6_2 stored as values in memory (estimated size 79.0 KiB, free 434.2 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added rdd_6_2 in memory on spark-master:38955 (size: 79.0 KiB, free: 434.2 MiB)
26/02/22 12:37:35 INFO MemoryStore: Block rdd_6_1 stored as values in memory (estimated size 94.1 KiB, free 434.1 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added rdd_6_1 in memory on spark-master:38955 (size: 94.1 KiB, free: 434.2 MiB)
26/02/22 12:37:35 INFO Executor: Finished task 2.0 in stage 9.0 (TID 10). 1753 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 10) in 1113 ms on spark-master (executor driver) (2/6)
26/02/22 12:37:35 INFO Executor: Finished task 1.0 in stage 9.0 (TID 9). 1753 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 9) in 1119 ms on spark-master (executor driver) (3/6)
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO MemoryStore: Block rdd_6_0 stored as values in memory (estimated size 126.8 KiB, free 433.9 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added rdd_6_0 in memory on spark-master:38955 (size: 126.8 KiB, free: 434.0 MiB)
26/02/22 12:37:35 INFO MemoryStore: Block rdd_6_5 stored as values in memory (estimated size 132.7 KiB, free 433.8 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added rdd_6_5 in memory on spark-master:38955 (size: 132.7 KiB, free: 433.9 MiB)
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:37:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:37:35 INFO MemoryStore: Block rdd_6_4 stored as values in memory (estimated size 191.4 KiB, free 433.6 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added rdd_6_4 in memory on spark-master:38955 (size: 191.4 KiB, free: 433.7 MiB)
26/02/22 12:37:35 INFO Executor: Finished task 5.0 in stage 9.0 (TID 13). 1753 bytes result sent to driver
26/02/22 12:37:35 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1753 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 1155 ms on spark-master (executor driver) (4/6)
26/02/22 12:37:35 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 13) in 1155 ms on spark-master (executor driver) (5/6)
26/02/22 12:37:35 INFO Executor: Finished task 4.0 in stage 9.0 (TID 12). 1753 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 12) in 1164 ms on spark-master (executor driver) (6/6)
26/02/22 12:37:35 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
26/02/22 12:37:35 INFO DAGScheduler: ShuffleMapStage 9 (start at <unknown>:0) finished in 1.196 s
26/02/22 12:37:35 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:35 INFO DAGScheduler: running: Set()
26/02/22 12:37:35 INFO DAGScheduler: waiting: Set(ResultStage 10)
26/02/22 12:37:35 INFO DAGScheduler: failed: Set()
26/02/22 12:37:35 INFO DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.5 KiB, free 433.6 MiB)
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.6 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:38955 (size: 3.1 KiB, free: 433.7 MiB)
26/02/22 12:37:35 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:35 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:37:35 INFO TaskSchedulerImpl: Adding task set 10.0 with 6 tasks resource profile 0
26/02/22 12:37:35 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14) (spark-master, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 15) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 16) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 17) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 18) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 19) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO Executor: Running task 1.0 in stage 10.0 (TID 14)
26/02/22 12:37:35 INFO Executor: Running task 2.0 in stage 10.0 (TID 16)
26/02/22 12:37:35 INFO Executor: Running task 3.0 in stage 10.0 (TID 17)
26/02/22 12:37:35 INFO Executor: Running task 0.0 in stage 10.0 (TID 15)
26/02/22 12:37:35 INFO Executor: Running task 4.0 in stage 10.0 (TID 18)
26/02/22 12:37:35 INFO Executor: Running task 5.0 in stage 10.0 (TID 19)
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Getting 6 (528.0 B) non-empty blocks including 6 (528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:35 INFO Executor: Finished task 2.0 in stage 10.0 (TID 16). 1235 bytes result sent to driver
26/02/22 12:37:35 INFO Executor: Finished task 5.0 in stage 10.0 (TID 19). 1235 bytes result sent to driver
26/02/22 12:37:35 INFO Executor: Finished task 3.0 in stage 10.0 (TID 17). 1235 bytes result sent to driver
26/02/22 12:37:35 INFO Executor: Finished task 0.0 in stage 10.0 (TID 15). 1235 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 16) in 12 ms on spark-master (executor driver) (1/6)
26/02/22 12:37:35 INFO Executor: Finished task 4.0 in stage 10.0 (TID 18). 1235 bytes result sent to driver
26/02/22 12:37:35 INFO Executor: Finished task 1.0 in stage 10.0 (TID 14). 1284 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 19) in 14 ms on spark-master (executor driver) (2/6)
26/02/22 12:37:35 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 15) in 16 ms on spark-master (executor driver) (3/6)
26/02/22 12:37:35 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 17) in 15 ms on spark-master (executor driver) (4/6)
26/02/22 12:37:35 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 18) in 16 ms on spark-master (executor driver) (5/6)
26/02/22 12:37:35 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 19 ms on spark-master (executor driver) (6/6)
26/02/22 12:37:35 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/22 12:37:35 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.024 s
26/02/22 12:37:35 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/22 12:37:35 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 1.225608 s
26/02/22 12:37:35 INFO BaseSparkCommitActionExecutor: Source read and index timer 1251
26/02/22 12:37:35 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:37:35 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:35 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:35 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
26/02/22 12:37:35 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:35 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:35 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 269.4 KiB, free 433.4 MiB)
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 93.4 KiB, free 433.3 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:38955 (size: 93.4 KiB, free: 433.6 MiB)
26/02/22 12:37:35 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:35 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/22 12:37:35 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO Executor: Running task 0.0 in stage 11.0 (TID 20)
26/02/22 12:37:35 INFO Executor: Finished task 0.0 in stage 11.0 (TID 20). 790 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 20) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:35 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/22 12:37:35 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.024 s
26/02/22 12:37:35 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/22 12:37:35 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.026796 s
26/02/22 12:37:35 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:35 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:35 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/22 12:37:35 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 10000, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/22 12:37:35 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/22 12:37:35 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/22 12:37:35 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260222123732236.inflight
26/02/22 12:37:35 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:37:35 INFO BaseCommitActionExecutor: Auto commit disabled for 20260222123732236
26/02/22 12:37:35 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:35 INFO DAGScheduler: Registering RDD 30 (start at <unknown>:0) as input to shuffle 2
26/02/22 12:37:35 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:35 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
26/02/22 12:37:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
26/02/22 12:37:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
26/02/22 12:37:35 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 299.7 KiB, free 433.0 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:38955 in memory (size: 3.1 KiB, free: 433.6 MiB)
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.0 KiB, free 432.9 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:38955 (size: 106.0 KiB, free: 433.5 MiB)
26/02/22 12:37:35 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:35 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:37:35 INFO TaskSchedulerImpl: Adding task set 12.0 with 6 tasks resource profile 0
26/02/22 12:37:35 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:38955 in memory (size: 93.4 KiB, free: 433.6 MiB)
26/02/22 12:37:35 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 21) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 22) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 23) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 24) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 25) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 26) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO Executor: Running task 0.0 in stage 12.0 (TID 21)
26/02/22 12:37:35 INFO Executor: Running task 4.0 in stage 12.0 (TID 25)
26/02/22 12:37:35 INFO Executor: Running task 2.0 in stage 12.0 (TID 23)
26/02/22 12:37:35 INFO Executor: Running task 3.0 in stage 12.0 (TID 24)
26/02/22 12:37:35 INFO Executor: Running task 1.0 in stage 12.0 (TID 22)
26/02/22 12:37:35 INFO Executor: Running task 5.0 in stage 12.0 (TID 26)
26/02/22 12:37:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:38955 in memory (size: 31.0 KiB, free: 433.6 MiB)
26/02/22 12:37:35 INFO BlockManager: Found block rdd_6_4 locally
26/02/22 12:37:35 INFO BlockManager: Found block rdd_6_0 locally
26/02/22 12:37:35 INFO BlockManager: Found block rdd_6_1 locally
26/02/22 12:37:35 INFO BlockManager: Found block rdd_6_3 locally
26/02/22 12:37:35 INFO BlockManager: Found block rdd_6_2 locally
26/02/22 12:37:35 INFO BlockManager: Found block rdd_6_5 locally
26/02/22 12:37:35 INFO Executor: Finished task 3.0 in stage 12.0 (TID 24). 1619 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 24) in 54 ms on spark-master (executor driver) (1/6)
26/02/22 12:37:35 INFO Executor: Finished task 2.0 in stage 12.0 (TID 23). 1619 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 23) in 60 ms on spark-master (executor driver) (2/6)
26/02/22 12:37:35 INFO Executor: Finished task 1.0 in stage 12.0 (TID 22). 1619 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 22) in 63 ms on spark-master (executor driver) (3/6)
26/02/22 12:37:35 INFO Executor: Finished task 0.0 in stage 12.0 (TID 21). 1619 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 21) in 65 ms on spark-master (executor driver) (4/6)
26/02/22 12:37:35 INFO Executor: Finished task 5.0 in stage 12.0 (TID 26). 1619 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 26) in 67 ms on spark-master (executor driver) (5/6)
26/02/22 12:37:35 INFO Executor: Finished task 4.0 in stage 12.0 (TID 25). 1619 bytes result sent to driver
26/02/22 12:37:35 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 25) in 72 ms on spark-master (executor driver) (6/6)
26/02/22 12:37:35 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/22 12:37:35 INFO DAGScheduler: ShuffleMapStage 12 (start at <unknown>:0) finished in 0.097 s
26/02/22 12:37:35 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:35 INFO DAGScheduler: running: Set()
26/02/22 12:37:35 INFO DAGScheduler: waiting: Set(ResultStage 13)
26/02/22 12:37:35 INFO DAGScheduler: failed: Set()
26/02/22 12:37:35 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 302.5 KiB, free 433.1 MiB)
26/02/22 12:37:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 107.4 KiB, free 432.9 MiB)
26/02/22 12:37:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:38955 (size: 107.4 KiB, free: 433.5 MiB)
26/02/22 12:37:35 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:35 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/22 12:37:35 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:35 INFO Executor: Running task 0.0 in stage 13.0 (TID 27)
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Getting 6 (837.5 KiB) non-empty blocks including 6 (837.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:35 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:37:35 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222123732236/eaecbe85-4de2-4834-9ac6-ab300eafdb29-0_0-13-27_20260222123732236.parquet.marker.CREATE
26/02/22 12:37:35 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222123732236/eaecbe85-4de2-4834-9ac6-ab300eafdb29-0_0-13-27_20260222123732236.parquet.marker.CREATE in 24 ms
26/02/22 12:37:35 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:35 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:38955 in memory (size: 106.0 KiB, free: 433.6 MiB)
26/02/22 12:37:36 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId eaecbe85-4de2-4834-9ac6-ab300eafdb29-0
26/02/22 12:37:36 INFO HoodieCreateHandle: Closing the file eaecbe85-4de2-4834-9ac6-ab300eafdb29-0 as we are done with all the records 10000
26/02/22 12:37:36 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID eaecbe85-4de2-4834-9ac6-ab300eafdb29-0, took 618 ms.
26/02/22 12:37:36 INFO MemoryStore: Block rdd_34_0 stored as values in memory (estimated size 295.0 B, free 433.3 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Added rdd_34_0 in memory on spark-master:38955 (size: 295.0 B, free: 433.6 MiB)
26/02/22 12:37:36 INFO Executor: Finished task 0.0 in stage 13.0 (TID 27). 1716 bytes result sent to driver
26/02/22 12:37:36 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 27) in 648 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:36 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/22 12:37:36 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.661 s
26/02/22 12:37:36 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
26/02/22 12:37:36 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.763443 s
26/02/22 12:37:36 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/22 12:37:36 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:36 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:36 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
26/02/22 12:37:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/22 12:37:36 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:36 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 303.2 KiB, free 433.0 MiB)
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 107.8 KiB, free 432.9 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:38955 (size: 107.8 KiB, free: 433.5 MiB)
26/02/22 12:37:36 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:36 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/22 12:37:36 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:36 INFO Executor: Running task 0.0 in stage 15.0 (TID 28)
26/02/22 12:37:36 INFO BlockManager: Found block rdd_34_0 locally
26/02/22 12:37:36 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:38955 in memory (size: 107.4 KiB, free: 433.6 MiB)
26/02/22 12:37:36 INFO Executor: Finished task 0.0 in stage 15.0 (TID 28). 1720 bytes result sent to driver
26/02/22 12:37:36 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 28) in 27 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:36 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/22 12:37:36 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.044 s
26/02/22 12:37:36 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/22 12:37:36 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.046472 s
26/02/22 12:37:36 INFO BaseHoodieWriteClient: Committing 20260222123732236 action commit
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123732236__commit__INFLIGHT__20260222123735714]}
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:36 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:36 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:36 INFO CommitUtils: Creating  metadata for INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123732236__commit__INFLIGHT__20260222123735714]}
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:36 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:36 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:36 INFO BaseHoodieWriteClient: Committing 20260222123732236 action commit
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:36 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:36 INFO HoodieTableMetadataUtil: Updating at 20260222123732236 from Commit/INSERT. #partitions_updated=2, #files_added=1
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:36 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:37:36 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:36 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:36 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:36 INFO HoodieBackedTableMetadataWriter: New commit at 20260222123732236 being applied to MDT.
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:36 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123734056]}
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:36 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:36 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123732236 action: deltacommit
26/02/22 12:37:36 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123732236__deltacommit__REQUESTED]
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123732236__deltacommit__REQUESTED__20260222123736723]}
26/02/22 12:37:36 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:36 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:36 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:36 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:37:36 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:36 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:36 INFO DAGScheduler: Registering RDD 45 (start at <unknown>:0) as input to shuffle 3
26/02/22 12:37:36 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:36 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
26/02/22 12:37:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/22 12:37:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)
26/02/22 12:37:36 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 10.1 KiB, free 433.3 MiB)
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 433.3 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:38955 (size: 5.4 KiB, free: 433.6 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:38955 in memory (size: 107.8 KiB, free: 433.7 MiB)
26/02/22 12:37:36 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:36 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/22 12:37:36 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:37:36 INFO Executor: Running task 0.0 in stage 16.0 (TID 29)
26/02/22 12:37:36 INFO MemoryStore: Block rdd_43_0 stored as values in memory (estimated size 342.0 B, free 433.7 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Added rdd_43_0 in memory on spark-master:38955 (size: 342.0 B, free: 433.7 MiB)
26/02/22 12:37:36 INFO Executor: Finished task 0.0 in stage 16.0 (TID 29). 1093 bytes result sent to driver
26/02/22 12:37:36 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 29) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:36 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/22 12:37:36 INFO DAGScheduler: ShuffleMapStage 16 (start at <unknown>:0) finished in 0.041 s
26/02/22 12:37:36 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:36 INFO DAGScheduler: running: Set()
26/02/22 12:37:36 INFO DAGScheduler: waiting: Set(ResultStage 17)
26/02/22 12:37:36 INFO DAGScheduler: failed: Set()
26/02/22 12:37:36 INFO DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KiB, free 433.7 MiB)
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.7 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:38955 (size: 3.1 KiB, free: 433.7 MiB)
26/02/22 12:37:36 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:36 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
26/02/22 12:37:36 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 30) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:36 INFO Executor: Running task 0.0 in stage 17.0 (TID 30)
26/02/22 12:37:36 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:36 INFO Executor: Finished task 0.0 in stage 17.0 (TID 30). 1366 bytes result sent to driver
26/02/22 12:37:36 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 30) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:36 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/22 12:37:36 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.017 s
26/02/22 12:37:36 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/22 12:37:36 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.062214 s
26/02/22 12:37:36 INFO BaseSparkCommitActionExecutor: Source read and index timer 70
26/02/22 12:37:36 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:37:36 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:36 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:36 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
26/02/22 12:37:36 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:36 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:36 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 262.2 KiB, free 433.5 MiB)
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 433.4 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:38955 in memory (size: 5.4 KiB, free: 433.7 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:38955 (size: 90.5 KiB, free: 433.7 MiB)
26/02/22 12:37:36 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:36 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/22 12:37:36 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/22 12:37:36 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:38955 in memory (size: 3.1 KiB, free: 433.7 MiB)
26/02/22 12:37:36 INFO Executor: Running task 0.0 in stage 18.0 (TID 31)
26/02/22 12:37:36 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:36 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:36 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:37:36 INFO Executor: Finished task 0.0 in stage 18.0 (TID 31). 837 bytes result sent to driver
26/02/22 12:37:36 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 31) in 22 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:36 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/22 12:37:36 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.036 s
26/02/22 12:37:36 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/22 12:37:36 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.039150 s
26/02/22 12:37:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:36 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/22 12:37:36 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260222123732236.deltacommit.inflight
26/02/22 12:37:36 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:38955 in memory (size: 90.5 KiB, free: 433.7 MiB)
26/02/22 12:37:36 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:37:36 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260222123732236
26/02/22 12:37:36 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:36 INFO DAGScheduler: Registering RDD 49 (start at <unknown>:0) as input to shuffle 4
26/02/22 12:37:36 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:36 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
26/02/22 12:37:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/22 12:37:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
26/02/22 12:37:36 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 266.9 KiB, free 433.5 MiB)
26/02/22 12:37:36 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 93.0 KiB, free 433.4 MiB)
26/02/22 12:37:36 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:38955 (size: 93.0 KiB, free: 433.7 MiB)
26/02/22 12:37:36 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:36 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/22 12:37:36 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 32) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:37:36 INFO Executor: Running task 0.0 in stage 19.0 (TID 32)
26/02/22 12:37:37 INFO BlockManager: Found block rdd_43_0 locally
26/02/22 12:37:37 INFO Executor: Finished task 0.0 in stage 19.0 (TID 32). 1050 bytes result sent to driver
26/02/22 12:37:37 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 32) in 106 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:37 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/22 12:37:37 INFO DAGScheduler: ShuffleMapStage 19 (start at <unknown>:0) finished in 0.120 s
26/02/22 12:37:37 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:37 INFO DAGScheduler: running: Set()
26/02/22 12:37:37 INFO DAGScheduler: waiting: Set(ResultStage 20)
26/02/22 12:37:37 INFO DAGScheduler: failed: Set()
26/02/22 12:37:37 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 372.6 KiB, free 433.0 MiB)
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.9 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:38955 (size: 132.2 KiB, free: 433.5 MiB)
26/02/22 12:37:37 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:37 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/22 12:37:37 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 33) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:37 INFO Executor: Running task 0.0 in stage 20.0 (TID 33)
26/02/22 12:37:37 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:37 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260222123732236 for file files-0000-0
26/02/22 12:37:37 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:37 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:37 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/22 12:37:37 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:37:37 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:37:37 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222123732236/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND
26/02/22 12:37:37 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222123732236/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND in 31 ms
26/02/22 12:37:37 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-20-33', fileLen=-1}
26/02/22 12:37:37 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:37 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:37 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-20-33, took 363 ms.
26/02/22 12:37:37 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 342.0 B, free 432.9 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:38955 (size: 342.0 B, free: 433.5 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:38955 in memory (size: 93.0 KiB, free: 433.6 MiB)
26/02/22 12:37:37 INFO Executor: Finished task 0.0 in stage 20.0 (TID 33). 1669 bytes result sent to driver
26/02/22 12:37:37 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 33) in 410 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:37 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/22 12:37:37 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.429 s
26/02/22 12:37:37 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/22 12:37:37 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.468318 s
26/02/22 12:37:37 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/22 12:37:37 INFO BaseSparkCommitActionExecutor: Committing 20260222123732236, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/22 12:37:37 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:37 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:37 INFO DAGScheduler: Final stage: ResultStage 21 (start at <unknown>:0)
26/02/22 12:37:37 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:37 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:37 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 101.2 KiB, free 433.1 MiB)
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.1 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:38955 (size: 36.1 KiB, free: 433.6 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:38955 in memory (size: 132.2 KiB, free: 433.7 MiB)
26/02/22 12:37:37 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:37 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/22 12:37:37 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:37:37 INFO Executor: Running task 0.0 in stage 21.0 (TID 34)
26/02/22 12:37:37 INFO Executor: Finished task 0.0 in stage 21.0 (TID 34). 804 bytes result sent to driver
26/02/22 12:37:37 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 34) in 7 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:37 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/22 12:37:37 INFO DAGScheduler: ResultStage 21 (start at <unknown>:0) finished in 0.022 s
26/02/22 12:37:37 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/22 12:37:37 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.026304 s
26/02/22 12:37:37 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:37 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:37 INFO DAGScheduler: Final stage: ResultStage 22 (start at <unknown>:0)
26/02/22 12:37:37 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:37 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:37 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 101.1 KiB, free 433.5 MiB)
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.5 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:38955 (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:37:37 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:37 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:38955 in memory (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:37:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:37 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/22 12:37:37 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 35) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:37:37 INFO Executor: Running task 0.0 in stage 22.0 (TID 35)
26/02/22 12:37:37 INFO Executor: Finished task 0.0 in stage 22.0 (TID 35). 857 bytes result sent to driver
26/02/22 12:37:37 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 35) in 16 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:37 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/22 12:37:37 INFO DAGScheduler: ResultStage 22 (start at <unknown>:0) finished in 0.029 s
26/02/22 12:37:37 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/22 12:37:37 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.030970 s
26/02/22 12:37:37 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123732236__deltacommit__INFLIGHT]
26/02/22 12:37:37 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260222123732236.deltacommit
26/02/22 12:37:37 INFO HoodieActiveTimeline: Completed [==>20260222123732236__deltacommit__INFLIGHT]
26/02/22 12:37:37 INFO BaseSparkCommitActionExecutor: Committed 20260222123732236
26/02/22 12:37:37 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:37 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:37 INFO DAGScheduler: Final stage: ResultStage 23 (start at <unknown>:0)
26/02/22 12:37:37 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:37 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:37 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 101.3 KiB, free 433.5 MiB)
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:38955 in memory (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:37:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:38955 (size: 36.2 KiB, free: 433.7 MiB)
26/02/22 12:37:37 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:37 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
26/02/22 12:37:37 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 36) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:37:37 INFO Executor: Running task 0.0 in stage 23.0 (TID 36)
26/02/22 12:37:37 INFO Executor: Finished task 0.0 in stage 23.0 (TID 36). 900 bytes result sent to driver
26/02/22 12:37:37 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 36) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:37 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/22 12:37:37 INFO DAGScheduler: ResultStage 23 (start at <unknown>:0) finished in 0.039 s
26/02/22 12:37:37 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/22 12:37:37 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 0.041048 s
26/02/22 12:37:37 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222123732236
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__deltacommit__COMPLETED__20260222123737644]}
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:37 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__deltacommit__COMPLETED__20260222123737644]}
26/02/22 12:37:37 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123732236__commit__INFLIGHT]
26/02/22 12:37:37 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260222123732236.commit
26/02/22 12:37:37 INFO HoodieActiveTimeline: Completed [==>20260222123732236__commit__INFLIGHT]
26/02/22 12:37:37 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:37:37 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
26/02/22 12:37:37 INFO DAGScheduler: Final stage: ResultStage 24 (start at <unknown>:0)
26/02/22 12:37:37 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:37 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:37 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0), which has no missing parents
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 101.3 KiB, free 433.5 MiB)
26/02/22 12:37:37 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:37:37 INFO BlockManager: Removing RDD 43
26/02/22 12:37:37 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:38955 (size: 36.2 KiB, free: 433.7 MiB)
26/02/22 12:37:37 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:37 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
26/02/22 12:37:37 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 37) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:37:37 INFO Executor: Running task 0.0 in stage 24.0 (TID 37)
26/02/22 12:37:37 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:38955 in memory (size: 36.2 KiB, free: 433.7 MiB)
26/02/22 12:37:37 INFO BlockManager: Removing RDD 53
26/02/22 12:37:37 INFO Executor: Finished task 0.0 in stage 24.0 (TID 37). 964 bytes result sent to driver
26/02/22 12:37:37 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 37) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:37 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/22 12:37:37 INFO DAGScheduler: ResultStage 24 (start at <unknown>:0) finished in 0.039 s
26/02/22 12:37:37 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/22 12:37:37 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.041233 s
26/02/22 12:37:37 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222123732236
26/02/22 12:37:37 INFO BaseHoodieWriteClient: Committed 20260222123732236
26/02/22 12:37:37 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
26/02/22 12:37:37 INFO MapPartitionsRDD: Removing RDD 34 from persistence list
26/02/22 12:37:37 INFO BlockManager: Removing RDD 6
26/02/22 12:37:37 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/22 12:37:37 INFO BlockManager: Removing RDD 34
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__deltacommit__COMPLETED__20260222123737644]}
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:37 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:37 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__deltacommit__COMPLETED__20260222123737644]}
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:37 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:37 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260222123737817
26/02/22 12:37:37 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:37:37 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__deltacommit__COMPLETED__20260222123737644]}
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:37 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:37 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__deltacommit__COMPLETED__20260222123737644]}
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/22 12:37:37 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:37:37 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__deltacommit__COMPLETED__20260222123737644]}
26/02/22 12:37:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Commit 20260222123732236 successful!
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:37:37 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:37:37 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/22 12:37:37 INFO HoodieStreamingSink: Micro batch id=0 succeeded for commit=20260222123732236
26/02/22 12:37:37 INFO HoodieStreamingSink: Current value of latestCommittedBatchId: -1. Setting latestCommittedBatchId to batchId 0.
26/02/22 12:37:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:37 INFO HoodieStreamingSink: Micro batch id=0 succeeded
26/02/22 12:37:37 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/commits/0 using temp file s3a://warehouse/checkpoints/bronze/commits/.0.d8ebb337-72bf-448f-8409-2ff43aba7220.tmp
26/02/22 12:37:37 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/commits/.0.d8ebb337-72bf-448f-8409-2ff43aba7220.tmp to s3a://warehouse/checkpoints/bronze/commits/0
26/02/22 12:37:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c44f2a56-fbf1-400e-879d-db826dcc4c57",
  "runId" : "f18f3cb5-4351-458b-bdf5-b88b0f13203e",
  "name" : null,
  "timestamp" : "2026-02-22T12:37:31.635Z",
  "batchId" : 0,
  "numInputRows" : 10000,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 1583.531274742676,
  "durationMs" : {
    "addBatch" : 5897,
    "getBatch" : 14,
    "latestOffset" : 27,
    "queryPlanning" : 238,
    "triggerExecution" : 6313,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[taxi.raw_trips]]",
    "startOffset" : null,
    "endOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "latestOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "numInputRows" : 10000,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 1583.531274742676,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "HoodieStreamingSink[s3a://warehouse/bronze/raw_trips]",
    "numOutputRows" : -1
  }
}
26/02/22 12:37:37 INFO MicroBatchExecution: Finished processing all available data for the trigger, terminating this Trigger.AvailableNow query
26/02/22 12:37:37 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Revoke previously assigned partitions taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/22 12:37:37 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] The pause flag in partitions [taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5] will be removed due to revocation.
26/02/22 12:37:37 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Member consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1-ec596a27-76a1-4445-9f8f-4db81c4928e9 sending LeaveGroup request to coordinator kafka:9092 (id: 2147483646 rack: null) due to the consumer is being closed
26/02/22 12:37:37 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:37:37 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:37:37 INFO Metrics: Metrics scheduler closed
26/02/22 12:37:37 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:37:37 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:37:37 INFO Metrics: Metrics reporters closed
26/02/22 12:37:37 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-driver-0-1 unregistered
26/02/22 12:37:38 INFO DataSourceUtils: Getting table path..
26/02/22 12:37:38 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/22 12:37:38 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/22 12:37:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:38 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:38 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:38 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:37:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:38 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:38 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:38 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 10 ms
26/02/22 12:37:38 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:37:38 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:37:38 INFO DAGScheduler: Got job 18 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:37:38 INFO DAGScheduler: Final stage: ResultStage 25 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:37:38 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:38 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:38 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 101.1 KiB, free 434.2 MiB)
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.1 MiB)
26/02/22 12:37:38 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:38955 (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:37:38 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:38955 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:37:38 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:38 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
26/02/22 12:37:38 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 38) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/22 12:37:38 INFO Executor: Running task 0.0 in stage 25.0 (TID 38)
26/02/22 12:37:38 INFO Executor: Finished task 0.0 in stage 25.0 (TID 38). 1166 bytes result sent to driver
26/02/22 12:37:38 INFO BlockManager: Removing RDD 34
26/02/22 12:37:38 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 38) in 20 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:38 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
26/02/22 12:37:38 INFO DAGScheduler: ResultStage 25 (collect at HoodieSparkEngineContext.java:116) finished in 0.031 s
26/02/22 12:37:38 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
26/02/22 12:37:38 INFO BlockManager: Removing RDD 6
26/02/22 12:37:38 INFO DAGScheduler: Job 18 finished: collect at HoodieSparkEngineContext.java:116, took 0.033295 s
26/02/22 12:37:38 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:38 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:38 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:37:38 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:37:38 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:37:38 INFO CodeGenerator: Code generated in 7.845418 ms
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/22 12:37:38 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:38955 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.2 MiB)
26/02/22 12:37:38 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:38955 (size: 35.2 KiB, free: 434.4 MiB)
26/02/22 12:37:38 INFO SparkContext: Created broadcast 24 from count at <unknown>:0
26/02/22 12:37:38 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:37:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:37:38 INFO DAGScheduler: Registering RDD 68 (count at <unknown>:0) as input to shuffle 5
26/02/22 12:37:38 INFO DAGScheduler: Got map stage job 19 (count at <unknown>:0) with 1 output partitions
26/02/22 12:37:38 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (count at <unknown>:0)
26/02/22 12:37:38 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:38 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:38 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0), which has no missing parents
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.1 MiB)
26/02/22 12:37:38 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:38955 (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:37:38 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:38 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/22 12:37:38 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 39) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:37:38 INFO Executor: Running task 0.0 in stage 26.0 (TID 39)
26/02/22 12:37:38 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/eaecbe85-4de2-4834-9ac6-ab300eafdb29-0_0-13-27_20260222123732236.parquet, range: 0-681070, partition values: [empty row]
26/02/22 12:37:38 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:38 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:38 INFO Executor: Finished task 0.0 in stage 26.0 (TID 39). 2056 bytes result sent to driver
26/02/22 12:37:38 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 39) in 112 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:38 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/22 12:37:38 INFO DAGScheduler: ShuffleMapStage 26 (count at <unknown>:0) finished in 0.133 s
26/02/22 12:37:38 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:38 INFO DAGScheduler: running: Set()
26/02/22 12:37:38 INFO DAGScheduler: waiting: Set()
26/02/22 12:37:38 INFO DAGScheduler: failed: Set()
26/02/22 12:37:38 INFO CodeGenerator: Code generated in 5.350739 ms
26/02/22 12:37:38 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:37:38 INFO DAGScheduler: Got job 20 (count at <unknown>:0) with 1 output partitions
26/02/22 12:37:38 INFO DAGScheduler: Final stage: ResultStage 28 (count at <unknown>:0)
26/02/22 12:37:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
26/02/22 12:37:38 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:38 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0), which has no missing parents
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/22 12:37:38 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/22 12:37:38 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:38955 (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:37:38 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:38955 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:37:38 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:38 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
26/02/22 12:37:38 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 40) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:37:38 INFO Executor: Running task 0.0 in stage 28.0 (TID 40)
26/02/22 12:37:38 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:38 INFO Executor: Finished task 0.0 in stage 28.0 (TID 40). 2458 bytes result sent to driver
26/02/22 12:37:38 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 40) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:38 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
26/02/22 12:37:38 INFO DAGScheduler: ResultStage 28 (count at <unknown>:0) finished in 0.040 s
26/02/22 12:37:38 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
26/02/22 12:37:38 INFO DAGScheduler: Job 20 finished: count at <unknown>:0, took 0.043066 s
Bronze ingest complete. Total rows in Hudi table: 10,000
============================================================
  Bronze Ingest (Hudi COW) COMPLETE
============================================================
26/02/22 12:37:38 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/22 12:37:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/22 12:37:38 INFO MemoryStore: MemoryStore cleared
26/02/22 12:37:38 INFO BlockManager: BlockManager stopped
26/02/22 12:37:38 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/22 12:37:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/22 12:37:38 INFO SparkContext: Successfully stopped SparkContext
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO Metrics: Metrics scheduler closed
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:37:39 INFO Metrics: Metrics reporters closed
26/02/22 12:37:39 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-7 unregistered
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO Metrics: Metrics scheduler closed
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:37:39 INFO Metrics: Metrics reporters closed
26/02/22 12:37:39 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-4 unregistered
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO Metrics: Metrics scheduler closed
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:37:39 INFO Metrics: Metrics reporters closed
26/02/22 12:37:39 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-6 unregistered
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO Metrics: Metrics scheduler closed
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:37:39 INFO Metrics: Metrics reporters closed
26/02/22 12:37:39 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-2 unregistered
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO Metrics: Metrics scheduler closed
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:37:39 INFO Metrics: Metrics reporters closed
26/02/22 12:37:39 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-3 unregistered
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5, groupId=spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:37:39 INFO Metrics: Metrics scheduler closed
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:37:39 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:37:39 INFO Metrics: Metrics reporters closed
26/02/22 12:37:39 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-c60ee8dd-ec29-4f87-bb44-53af6a2a09a3--1125050931-executor-5 unregistered
26/02/22 12:37:39 INFO ShutdownHookManager: Shutdown hook called
26/02/22 12:37:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-950799bb-808f-43e4-8687-0483ea4aa3f2/pyspark-4a3dde38-7227-4f03-930e-1c40ac901ca7
26/02/22 12:37:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-950799bb-808f-43e4-8687-0483ea4aa3f2
26/02/22 12:37:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f936172-8cee-4d2a-b766-00d2535171b3
Bronze ingest complete.
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/silver_transform.py
26/02/22 12:37:40 INFO SparkContext: Running Spark version 3.3.3
26/02/22 12:37:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/22 12:37:40 INFO ResourceUtils: ==============================================================
26/02/22 12:37:40 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/22 12:37:40 INFO ResourceUtils: ==============================================================
26/02/22 12:37:40 INFO SparkContext: Submitted application: P22-Silver-Transform-Hudi
26/02/22 12:37:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/22 12:37:40 INFO ResourceProfile: Limiting resource is cpu
26/02/22 12:37:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/22 12:37:40 INFO SecurityManager: Changing view acls to: spark
26/02/22 12:37:40 INFO SecurityManager: Changing modify acls to: spark
26/02/22 12:37:40 INFO SecurityManager: Changing view acls groups to: 
26/02/22 12:37:40 INFO SecurityManager: Changing modify acls groups to: 
26/02/22 12:37:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/22 12:37:41 INFO Utils: Successfully started service 'sparkDriver' on port 41065.
26/02/22 12:37:41 INFO SparkEnv: Registering MapOutputTracker
26/02/22 12:37:41 INFO SparkEnv: Registering BlockManagerMaster
26/02/22 12:37:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/22 12:37:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/22 12:37:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/22 12:37:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b6e6e6ac-fab9-41fb-9aa2-8dddaf120a00
26/02/22 12:37:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/22 12:37:41 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/22 12:37:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/22 12:37:41 INFO Executor: Starting executor ID driver on host spark-master
26/02/22 12:37:41 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/22 12:37:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36615.
26/02/22 12:37:41 INFO NettyBlockTransferService: Server created on spark-master:36615
26/02/22 12:37:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/22 12:37:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 36615, None)
26/02/22 12:37:41 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:36615 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 36615, None)
26/02/22 12:37:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 36615, None)
26/02/22 12:37:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 36615, None)
26/02/22 12:37:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/22 12:37:41 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/22 12:37:41 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/22 12:37:41 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:37:41 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/22 12:37:42 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/22 12:37:42 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/22 12:37:42 INFO DataSourceUtils: Getting table path..
26/02/22 12:37:42 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/22 12:37:42 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/22 12:37:42 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:42 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:42 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:37:42 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:37:42 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:42 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:42 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:37:42 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123732236__commit__COMPLETED__20260222123737739]}
26/02/22 12:37:42 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 15 ms
26/02/22 12:37:43 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:37:43 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:37:43 INFO DAGScheduler: Got job 0 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:37:43 INFO DAGScheduler: Final stage: ResultStage 0 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:37:43 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:43 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:37:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
26/02/22 12:37:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/22 12:37:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:36615 (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:37:44 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/22 12:37:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/22 12:37:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/22 12:37:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1123 bytes result sent to driver
26/02/22 12:37:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 89 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/22 12:37:44 INFO DAGScheduler: ResultStage 0 (collect at HoodieSparkEngineContext.java:116) finished in 0.292 s
26/02/22 12:37:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/22 12:37:44 INFO DAGScheduler: Job 0 finished: collect at HoodieSparkEngineContext.java:116, took 0.312142 s
26/02/22 12:37:44 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:44 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:44 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:37:44 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:37:44 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:37:44 INFO CodeGenerator: Code generated in 62.193777 ms
26/02/22 12:37:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/22 12:37:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.0 MiB)
26/02/22 12:37:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:36615 (size: 35.2 KiB, free: 434.3 MiB)
26/02/22 12:37:44 INFO SparkContext: Created broadcast 1 from count at <unknown>:0
26/02/22 12:37:44 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:37:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:37:44 INFO DAGScheduler: Registering RDD 5 (count at <unknown>:0) as input to shuffle 0
26/02/22 12:37:44 INFO DAGScheduler: Got map stage job 1 (count at <unknown>:0) with 1 output partitions
26/02/22 12:37:44 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at <unknown>:0)
26/02/22 12:37:44 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:44 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:44 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0), which has no missing parents
26/02/22 12:37:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:36615 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:37:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
26/02/22 12:37:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.1 MiB)
26/02/22 12:37:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:36615 (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:37:44 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:44 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/22 12:37:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:37:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/22 12:37:44 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/eaecbe85-4de2-4834-9ac6-ab300eafdb29-0_0-13-27_20260222123732236.parquet, range: 0-681070, partition values: [empty row]
26/02/22 12:37:44 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:44 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2056 bytes result sent to driver
26/02/22 12:37:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 125 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/22 12:37:44 INFO DAGScheduler: ShuffleMapStage 1 (count at <unknown>:0) finished in 0.165 s
26/02/22 12:37:44 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:44 INFO DAGScheduler: running: Set()
26/02/22 12:37:44 INFO DAGScheduler: waiting: Set()
26/02/22 12:37:44 INFO DAGScheduler: failed: Set()
26/02/22 12:37:44 INFO CodeGenerator: Code generated in 6.669582 ms
26/02/22 12:37:44 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:37:44 INFO DAGScheduler: Got job 2 (count at <unknown>:0) with 1 output partitions
26/02/22 12:37:44 INFO DAGScheduler: Final stage: ResultStage 3 (count at <unknown>:0)
26/02/22 12:37:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
26/02/22 12:37:44 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:44 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0), which has no missing parents
26/02/22 12:37:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/22 12:37:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/22 12:37:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:36615 (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:37:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:44 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/22 12:37:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:37:44 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
26/02/22 12:37:44 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
26/02/22 12:37:44 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2458 bytes result sent to driver
26/02/22 12:37:44 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 37 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/22 12:37:44 INFO DAGScheduler: ResultStage 3 (count at <unknown>:0) finished in 0.042 s
26/02/22 12:37:44 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
26/02/22 12:37:44 INFO DAGScheduler: Job 2 finished: count at <unknown>:0, took 0.048758 s
Bronze rows read: 10,000
26/02/22 12:37:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/22 12:37:45 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips as hoodie table s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:36615 in memory (size: 35.2 KiB, free: 434.4 MiB)
26/02/22 12:37:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:36615 in memory (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:37:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:36615 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:37:45 INFO EmbeddedTimelineService: Overriding hostIp to (spark-master) found in spark-conf. It was null
26/02/22 12:37:45 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:45 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:45 INFO log: Logging initialized @5602ms to org.apache.hudi.org.apache.jetty.util.log.Slf4jLog
26/02/22 12:37:45 INFO Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

26/02/22 12:37:45 INFO Javalin: Starting Javalin ...
26/02/22 12:37:45 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1217 days old. Consider checking for a newer version.).
26/02/22 12:37:45 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.20+8
26/02/22 12:37:45 INFO Server: Started @5761ms
26/02/22 12:37:45 INFO Javalin: Listening on http://localhost:39203/
26/02/22 12:37:45 INFO Javalin: Javalin started in 81ms \o/
26/02/22 12:37:45 INFO TimelineService: Starting Timeline server on port :39203
26/02/22 12:37:45 INFO EmbeddedTimelineService: Started embedded timeline server at spark-master:39203
26/02/22 12:37:45 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
26/02/22 12:37:45 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:37:45 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:37:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(tpep_pickup_datetime),IsNotNull(tpep_dropoff_datetime),IsNotNull(trip_distance),IsNotNull(fare_amount),GreaterThanOrEqual(trip_distance,0.0),GreaterThanOrEqual(fare_amount,0.0)
26/02/22 12:37:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(tpep_pickup_datetime#6),isnotnull(tpep_dropoff_datetime#7),isnotnull(trip_distance#9),isnotnull(fare_amount#15),isnotnull(cast(tpep_pickup_datetime#6 as timestamp)),isnotnull(cast(tpep_dropoff_datetime#7 as timestamp)),(trip_distance#9 >= 0.0),(fare_amount#15 >= 0.0),(cast(tpep_pickup_datetime#6 as timestamp) >= 2024-01-01 00:00:00)
26/02/22 12:37:45 INFO FileSourceStrategy: Output Data Schema: struct<VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: double, trip_distance: double ... 17 more fields>
26/02/22 12:37:45 INFO CodeGenerator: Code generated in 41.530401 ms
26/02/22 12:37:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.1 KiB, free 434.2 MiB)
26/02/22 12:37:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.2 MiB)
26/02/22 12:37:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:36615 (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:37:45 INFO SparkContext: Created broadcast 4 from toRdd at HoodieSparkUtils.scala:111
26/02/22 12:37:45 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:37:45 INFO HoodieFileIndex: Total file slices: 1; candidate file slices after data skipping: 1; skipping percentage 0.0
26/02/22 12:37:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:45 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:37:45 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:37:45 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123745243 action: commit
26/02/22 12:37:45 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123745243__commit__REQUESTED]
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123745243__commit__REQUESTED__20260222123745726]}
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:45 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/22 12:37:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123745243__commit__REQUESTED__20260222123745726]}
26/02/22 12:37:45 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips/.hoodie/metadata as hoodie table s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:45 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:45 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:45 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:37:45 INFO DAGScheduler: Got job 3 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:37:45 INFO DAGScheduler: Final stage: ResultStage 4 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:37:45 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:45 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:37:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 101.5 KiB, free 434.1 MiB)
26/02/22 12:37:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 434.0 MiB)
26/02/22 12:37:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:36615 (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:37:45 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/22 12:37:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4405 bytes) taskResourceAssignments Map()
26/02/22 12:37:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
26/02/22 12:37:45 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 926 bytes result sent to driver
26/02/22 12:37:45 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 13 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:45 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/22 12:37:45 INFO DAGScheduler: ResultStage 4 (collect at HoodieSparkEngineContext.java:116) finished in 0.022 s
26/02/22 12:37:45 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/22 12:37:45 INFO DAGScheduler: Job 3 finished: collect at HoodieSparkEngineContext.java:116, took 0.025376 s
26/02/22 12:37:45 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:45 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/22 12:37:45 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/22 12:37:45 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
26/02/22 12:37:45 INFO DAGScheduler: Got job 4 (count at HoodieJavaRDD.java:115) with 1 output partitions
26/02/22 12:37:45 INFO DAGScheduler: Final stage: ResultStage 5 (count at HoodieJavaRDD.java:115)
26/02/22 12:37:45 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:45 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:45 INFO DAGScheduler: Submitting ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
26/02/22 12:37:45 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.0 KiB, free 434.0 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.0 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:36615 (size: 1866.0 B, free: 434.3 MiB)
26/02/22 12:37:46 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:46 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
26/02/22 12:37:46 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:37:46 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
26/02/22 12:37:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 760 bytes result sent to driver
26/02/22 12:37:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 7 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
26/02/22 12:37:46 INFO DAGScheduler: ResultStage 5 (count at HoodieJavaRDD.java:115) finished in 0.016 s
26/02/22 12:37:46 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
26/02/22 12:37:46 INFO DAGScheduler: Job 4 finished: count at HoodieJavaRDD.java:115, took 0.019729 s
26/02/22 12:37:46 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/22 12:37:46 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/22 12:37:46 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
26/02/22 12:37:46 INFO DAGScheduler: Got job 5 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
26/02/22 12:37:46 INFO DAGScheduler: Final stage: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155)
26/02/22 12:37:46 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:46 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:46 INFO DAGScheduler: Submitting ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 393.9 KiB, free 433.6 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 130.0 KiB, free 433.5 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:36615 (size: 130.0 KiB, free: 434.2 MiB)
26/02/22 12:37:46 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:46 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/22 12:37:46 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/22 12:37:46 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/22 12:37:46 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:37:46 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:37:46 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/22 12:37:46 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/22 12:37:46 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 803 bytes result sent to driver
26/02/22 12:37:46 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 56 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/22 12:37:46 INFO DAGScheduler: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155) finished in 0.090 s
26/02/22 12:37:46 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/22 12:37:46 INFO DAGScheduler: Job 5 finished: foreach at HoodieSparkEngineContext.java:155, took 0.093596 s
26/02/22 12:37:46 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:46 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:46 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:37:46 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:37:46 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:46 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:46 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:46 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:46 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:46 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:46 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/22 12:37:46 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260222123746219]}
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:46 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:46 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:37:46 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:37:46 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/22 12:37:46 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
26/02/22 12:37:46 INFO DAGScheduler: Registering RDD 23 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 1
26/02/22 12:37:46 INFO DAGScheduler: Got job 6 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
26/02/22 12:37:46 INFO DAGScheduler: Final stage: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
26/02/22 12:37:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
26/02/22 12:37:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)
26/02/22 12:37:46 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.5 KiB, free 433.5 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.5 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:36615 (size: 4.7 KiB, free: 434.2 MiB)
26/02/22 12:37:46 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:46 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/22 12:37:46 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/22 12:37:46 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/22 12:37:46 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 964 bytes result sent to driver
26/02/22 12:37:46 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 14 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:46 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/22 12:37:46 INFO DAGScheduler: ShuffleMapStage 7 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.024 s
26/02/22 12:37:46 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:46 INFO DAGScheduler: running: Set()
26/02/22 12:37:46 INFO DAGScheduler: waiting: Set(ResultStage 8)
26/02/22 12:37:46 INFO DAGScheduler: failed: Set()
26/02/22 12:37:46 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.3 KiB, free 433.5 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.5 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:36615 (size: 3.9 KiB, free: 434.2 MiB)
26/02/22 12:37:46 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:46 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/22 12:37:46 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:46 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/22 12:37:46 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:46 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 1246 bytes result sent to driver
26/02/22 12:37:46 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 25 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:46 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/22 12:37:46 INFO DAGScheduler: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.032 s
26/02/22 12:37:46 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/22 12:37:46 INFO DAGScheduler: Job 6 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.061631 s
26/02/22 12:37:46 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:37:46 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/22 12:37:46 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:36615 in memory (size: 3.9 KiB, free: 434.2 MiB)
26/02/22 12:37:46 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:37:46 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:36615 in memory (size: 130.0 KiB, free: 434.3 MiB)
26/02/22 12:37:46 INFO DAGScheduler: Got job 7 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:37:46 INFO DAGScheduler: Final stage: ResultStage 10 (collect at HoodieJavaRDD.java:177)
26/02/22 12:37:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/22 12:37:46 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:46 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/22 12:37:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:36615 in memory (size: 4.7 KiB, free: 434.3 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:36615 in memory (size: 36.3 KiB, free: 434.4 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:36615 in memory (size: 1866.0 B, free: 434.4 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 267.2 KiB, free 433.9 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 91.3 KiB, free 433.8 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:36615 (size: 91.3 KiB, free: 434.3 MiB)
26/02/22 12:37:46 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:46 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
26/02/22 12:37:46 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:46 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)
26/02/22 12:37:46 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:46 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:37:46 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE
26/02/22 12:37:46 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE in 30 ms
26/02/22 12:37:46 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/22 12:37:46 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:37:46 INFO MetricsSystemImpl: HBase metrics system started
26/02/22 12:37:46 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/22 12:37:46 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:46 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:46 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/22 12:37:46 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/22 12:37:46 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 250 ms.
26/02/22 12:37:46 INFO MemoryStore: Block rdd_28_0 stored as values in memory (estimated size 272.0 B, free 433.8 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added rdd_28_0 in memory on spark-master:36615 (size: 272.0 B, free: 434.3 MiB)
26/02/22 12:37:46 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1439 bytes result sent to driver
26/02/22 12:37:46 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 281 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:46 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/22 12:37:46 INFO DAGScheduler: ResultStage 10 (collect at HoodieJavaRDD.java:177) finished in 0.294 s
26/02/22 12:37:46 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/22 12:37:46 INFO DAGScheduler: Job 7 finished: collect at HoodieJavaRDD.java:177, took 0.298101 s
26/02/22 12:37:46 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:37:46 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/22 12:37:46 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:37:46 INFO DAGScheduler: Got job 8 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:37:46 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:37:46 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:46 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:46 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 101.2 KiB, free 433.7 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.7 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:36615 (size: 36.1 KiB, free: 434.2 MiB)
26/02/22 12:37:46 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:46 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/22 12:37:46 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:37:46 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)
26/02/22 12:37:46 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 855 bytes result sent to driver
26/02/22 12:37:46 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 12 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:46 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/22 12:37:46 INFO DAGScheduler: ResultStage 11 (collect at HoodieSparkEngineContext.java:150) finished in 0.019 s
26/02/22 12:37:46 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/22 12:37:46 INFO DAGScheduler: Job 8 finished: collect at HoodieSparkEngineContext.java:150, took 0.021638 s
26/02/22 12:37:46 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:37:46 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/22 12:37:46 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:37:46 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/22 12:37:46 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:37:46 INFO DAGScheduler: Got job 9 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/22 12:37:46 INFO DAGScheduler: Final stage: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:37:46 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:46 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:46 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 101.3 KiB, free 433.6 MiB)
26/02/22 12:37:46 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:36615 (size: 36.2 KiB, free: 434.2 MiB)
26/02/22 12:37:46 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:46 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
26/02/22 12:37:46 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:37:46 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
26/02/22 12:37:46 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:36615 in memory (size: 36.1 KiB, free: 434.2 MiB)
26/02/22 12:37:46 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:36615 in memory (size: 91.3 KiB, free: 434.3 MiB)
26/02/22 12:37:46 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 947 bytes result sent to driver
26/02/22 12:37:46 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 35 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:46 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/22 12:37:46 INFO DAGScheduler: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.041 s
26/02/22 12:37:46 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
26/02/22 12:37:46 INFO DAGScheduler: Job 9 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.044251 s
26/02/22 12:37:46 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:46 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieTableConfig: MDT s3a://warehouse/silver/cleaned_trips partition FILES has been enabled
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:46 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:46 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 929 in ms
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:46 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:46 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:46 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:46 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:46 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:46 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:46 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:47 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:47 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/22 12:37:47 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:37:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:47 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:37:47 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:47 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123745243__commit__REQUESTED__20260222123745726]}
26/02/22 12:37:47 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:47 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:47 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/22 12:37:47 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:47 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123745243__commit__REQUESTED__20260222123745726]}
26/02/22 12:37:47 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:37:47 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:47 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:47 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:37:47 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:37:47 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:37:47 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:37:47 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:37:47 INFO DAGScheduler: Registering RDD 34 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 3
26/02/22 12:37:47 INFO DAGScheduler: Registering RDD 40 (distinct at HoodieJavaRDD.java:157) as input to shuffle 2
26/02/22 12:37:47 INFO DAGScheduler: Got job 10 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:37:47 INFO DAGScheduler: Final stage: ResultStage 15 (collect at HoodieJavaRDD.java:177)
26/02/22 12:37:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/22 12:37:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 14)
26/02/22 12:37:47 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 104.0 KiB, free 433.9 MiB)
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KiB, free 433.9 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:36615 (size: 31.5 KiB, free: 434.3 MiB)
26/02/22 12:37:47 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:47 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/22 12:37:47 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:37:47 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)
26/02/22 12:37:47 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/eaecbe85-4de2-4834-9ac6-ab300eafdb29-0_0-13-27_20260222123732236.parquet, range: 0-681070, partition values: [empty row]
26/02/22 12:37:47 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:47 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(tpep_pickup_datetime, null), noteq(tpep_dropoff_datetime, null)), noteq(trip_distance, null)), noteq(fare_amount, null)), gteq(trip_distance, 0.0)), gteq(fare_amount, 0.0))
26/02/22 12:37:47 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:47 INFO CodecPool: Got brand-new decompressor [.gz]
26/02/22 12:37:47 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:36615 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:37:47 INFO BlockManager: Removing RDD 28
26/02/22 12:37:47 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 1824 bytes result sent to driver
26/02/22 12:37:47 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 463 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:47 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/22 12:37:47 INFO DAGScheduler: ShuffleMapStage 13 (mapToPair at HoodieJavaRDD.java:149) finished in 0.473 s
26/02/22 12:37:47 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:47 INFO DAGScheduler: running: Set()
26/02/22 12:37:47 INFO DAGScheduler: waiting: Set(ResultStage 15, ShuffleMapStage 14)
26/02/22 12:37:47 INFO DAGScheduler: failed: Set()
26/02/22 12:37:47 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 30.1 KiB, free 434.0 MiB)
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 434.0 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:36615 (size: 13.5 KiB, free: 434.3 MiB)
26/02/22 12:37:47 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:47 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
26/02/22 12:37:47 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 12) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/22 12:37:47 INFO Executor: Running task 0.0 in stage 14.0 (TID 12)
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:47 INFO MemoryStore: Block rdd_36_0 stored as values in memory (estimated size 994.0 KiB, free 433.0 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added rdd_36_0 in memory on spark-master:36615 (size: 994.0 KiB, free: 433.4 MiB)
26/02/22 12:37:47 INFO Executor: Finished task 0.0 in stage 14.0 (TID 12). 1394 bytes result sent to driver
26/02/22 12:37:47 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 12) in 108 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:47 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
26/02/22 12:37:47 INFO DAGScheduler: ShuffleMapStage 14 (distinct at HoodieJavaRDD.java:157) finished in 0.118 s
26/02/22 12:37:47 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:47 INFO DAGScheduler: running: Set()
26/02/22 12:37:47 INFO DAGScheduler: waiting: Set(ResultStage 15)
26/02/22 12:37:47 INFO DAGScheduler: failed: Set()
26/02/22 12:37:47 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 6.3 KiB, free 433.0 MiB)
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.0 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:36615 (size: 3.4 KiB, free: 433.3 MiB)
26/02/22 12:37:47 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:47 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/22 12:37:47 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:47 INFO Executor: Running task 0.0 in stage 15.0 (TID 13)
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:47 INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 1237 bytes result sent to driver
26/02/22 12:37:47 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:47 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/22 12:37:47 INFO DAGScheduler: ResultStage 15 (collect at HoodieJavaRDD.java:177) finished in 0.012 s
26/02/22 12:37:47 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/22 12:37:47 INFO DAGScheduler: Job 10 finished: collect at HoodieJavaRDD.java:177, took 0.609605 s
26/02/22 12:37:47 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:37:47 INFO DAGScheduler: Got job 11 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:37:47 INFO DAGScheduler: Final stage: ResultStage 16 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:37:47 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:47 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:47 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 268.6 KiB, free 432.7 MiB)
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 432.6 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:36615 (size: 93.3 KiB, free: 433.3 MiB)
26/02/22 12:37:47 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:47 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/22 12:37:47 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:37:47 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)
26/02/22 12:37:47 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 804 bytes result sent to driver
26/02/22 12:37:47 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 11 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:47 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/22 12:37:47 INFO DAGScheduler: ResultStage 16 (collect at HoodieSparkEngineContext.java:150) finished in 0.027 s
26/02/22 12:37:47 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
26/02/22 12:37:47 INFO DAGScheduler: Job 11 finished: collect at HoodieSparkEngineContext.java:150, took 0.030080 s
26/02/22 12:37:47 INFO MapPartitionsRDD: Removing RDD 36 from persistence list
26/02/22 12:37:47 INFO BlockManager: Removing RDD 36
26/02/22 12:37:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:47 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/22 12:37:47 INFO DAGScheduler: Registering RDD 37 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 4
26/02/22 12:37:47 INFO DAGScheduler: Registering RDD 47 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 5
26/02/22 12:37:47 INFO DAGScheduler: Registering RDD 55 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 6
26/02/22 12:37:47 INFO DAGScheduler: Got job 12 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/22 12:37:47 INFO DAGScheduler: Final stage: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105)
26/02/22 12:37:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
26/02/22 12:37:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
26/02/22 12:37:47 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 29.0 KiB, free 433.6 MiB)
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 433.6 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:36615 (size: 13.1 KiB, free: 434.2 MiB)
26/02/22 12:37:47 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:47 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/22 12:37:47 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:37:47 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/22 12:37:47 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 270.6 KiB, free 433.3 MiB)
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 94.4 KiB, free 433.3 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:36615 in memory (size: 13.5 KiB, free: 434.2 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:36615 (size: 94.4 KiB, free: 434.1 MiB)
26/02/22 12:37:47 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:47 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/22 12:37:47 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4319 bytes) taskResourceAssignments Map()
26/02/22 12:37:47 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
26/02/22 12:37:47 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:36615 in memory (size: 31.5 KiB, free: 434.2 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:36615 in memory (size: 3.4 KiB, free: 434.2 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:36615 in memory (size: 93.3 KiB, free: 434.3 MiB)
26/02/22 12:37:47 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 835 bytes result sent to driver
26/02/22 12:37:47 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 28 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:47 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/22 12:37:47 INFO DAGScheduler: ShuffleMapStage 19 (mapToPair at HoodieJavaRDD.java:149) finished in 0.051 s
26/02/22 12:37:47 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:47 INFO DAGScheduler: running: Set(ShuffleMapStage 18)
26/02/22 12:37:47 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/22 12:37:47 INFO DAGScheduler: failed: Set()
26/02/22 12:37:47 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 1437 bytes result sent to driver
26/02/22 12:37:47 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 99 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:47 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/22 12:37:47 INFO DAGScheduler: ShuffleMapStage 18 (mapToPair at HoodieJavaRDD.java:149) finished in 0.106 s
26/02/22 12:37:47 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:47 INFO DAGScheduler: running: Set()
26/02/22 12:37:47 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/22 12:37:47 INFO DAGScheduler: failed: Set()
26/02/22 12:37:47 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.8 KiB, free 433.7 MiB)
26/02/22 12:37:47 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.7 MiB)
26/02/22 12:37:47 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:36615 (size: 5.1 KiB, free: 434.3 MiB)
26/02/22 12:37:47 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:47 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/22 12:37:47 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/22 12:37:47 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:48 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 994.0 KiB, free 432.8 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:36615 (size: 994.0 KiB, free: 433.3 MiB)
26/02/22 12:37:48 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 1394 bytes result sent to driver
26/02/22 12:37:48 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 99 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:48 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/22 12:37:48 INFO DAGScheduler: ShuffleMapStage 20 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.106 s
26/02/22 12:37:48 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:48 INFO DAGScheduler: running: Set()
26/02/22 12:37:48 INFO DAGScheduler: waiting: Set(ResultStage 21)
26/02/22 12:37:48 INFO DAGScheduler: failed: Set()
26/02/22 12:37:48 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.5 KiB, free 432.8 MiB)
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.8 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:36615 (size: 3.1 KiB, free: 433.3 MiB)
26/02/22 12:37:48 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:48 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/22 12:37:48 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:48 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
26/02/22 12:37:48 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:48 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1284 bytes result sent to driver
26/02/22 12:37:48 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:48 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/22 12:37:48 INFO DAGScheduler: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.013 s
26/02/22 12:37:48 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/22 12:37:48 INFO DAGScheduler: Job 12 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.233596 s
26/02/22 12:37:48 INFO BaseSparkCommitActionExecutor: Source read and index timer 250
26/02/22 12:37:48 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:37:48 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/22 12:37:48 INFO DAGScheduler: Got job 13 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/22 12:37:48 INFO DAGScheduler: Final stage: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285)
26/02/22 12:37:48 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:48 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:48 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 269.4 KiB, free 432.5 MiB)
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 93.7 KiB, free 432.4 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:36615 (size: 93.7 KiB, free: 433.2 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:36615 in memory (size: 3.1 KiB, free: 433.2 MiB)
26/02/22 12:37:48 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:48 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/22 12:37:48 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:37:48 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
26/02/22 12:37:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:36615 in memory (size: 94.4 KiB, free: 433.3 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:36615 in memory (size: 5.1 KiB, free: 433.3 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:36615 in memory (size: 13.1 KiB, free: 433.3 MiB)
26/02/22 12:37:48 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 790 bytes result sent to driver
26/02/22 12:37:48 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 27 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:48 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/22 12:37:48 INFO DAGScheduler: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285) finished in 0.048 s
26/02/22 12:37:48 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/22 12:37:48 INFO DAGScheduler: Job 13 finished: collectAsMap at UpsertPartitioner.java:285, took 0.050488 s
26/02/22 12:37:48 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:48 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:48 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/22 12:37:48 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 9855, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/22 12:37:48 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/22 12:37:48 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/22 12:37:48 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260222123745243.inflight
26/02/22 12:37:48 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:37:48 INFO BaseCommitActionExecutor: Auto commit disabled for 20260222123745243
26/02/22 12:37:48 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1073
26/02/22 12:37:48 INFO DAGScheduler: Registering RDD 59 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 7
26/02/22 12:37:48 INFO DAGScheduler: Got job 14 (count at HoodieSparkSqlWriter.scala:1073) with 1 output partitions
26/02/22 12:37:48 INFO DAGScheduler: Final stage: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073)
26/02/22 12:37:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
26/02/22 12:37:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 26)
26/02/22 12:37:48 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 274.5 KiB, free 432.6 MiB)
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 95.3 KiB, free 432.5 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:36615 (size: 95.3 KiB, free: 433.2 MiB)
26/02/22 12:37:48 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:48 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/22 12:37:48 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/22 12:37:48 INFO Executor: Running task 0.0 in stage 26.0 (TID 20)
26/02/22 12:37:48 INFO BlockManager: Found block rdd_53_0 locally
26/02/22 12:37:48 INFO Executor: Finished task 0.0 in stage 26.0 (TID 20). 1050 bytes result sent to driver
26/02/22 12:37:48 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 53 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:48 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/22 12:37:48 INFO DAGScheduler: ShuffleMapStage 26 (mapToPair at HoodieJavaRDD.java:149) finished in 0.065 s
26/02/22 12:37:48 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:48 INFO DAGScheduler: running: Set()
26/02/22 12:37:48 INFO DAGScheduler: waiting: Set(ResultStage 27)
26/02/22 12:37:48 INFO DAGScheduler: failed: Set()
26/02/22 12:37:48 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073), which has no missing parents
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 313.8 KiB, free 432.2 MiB)
26/02/22 12:37:48 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 108.0 KiB, free 432.1 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:36615 (size: 108.0 KiB, free: 433.1 MiB)
26/02/22 12:37:48 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:48 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
26/02/22 12:37:48 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:48 INFO Executor: Running task 0.0 in stage 27.0 (TID 21)
26/02/22 12:37:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:48 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:37:48 INFO MarkerHandler: Request: create marker: edcb56fb-5cde-412a-a020-ac8bb8d14b37-0_0-27-21_20260222123745243.parquet.marker.CREATE
26/02/22 12:37:48 INFO TimelineServerBasedWriteMarkers: [timeline-server-based] Created marker file /edcb56fb-5cde-412a-a020-ac8bb8d14b37-0_0-27-21_20260222123745243.parquet.marker.CREATE in 230 ms
26/02/22 12:37:48 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:48 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:36615 in memory (size: 95.3 KiB, free: 433.2 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:36615 in memory (size: 93.7 KiB, free: 433.3 MiB)
26/02/22 12:37:48 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId edcb56fb-5cde-412a-a020-ac8bb8d14b37-0
26/02/22 12:37:48 INFO HoodieCreateHandle: Closing the file edcb56fb-5cde-412a-a020-ac8bb8d14b37-0 as we are done with all the records 9855
26/02/22 12:37:48 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID edcb56fb-5cde-412a-a020-ac8bb8d14b37-0, took 646 ms.
26/02/22 12:37:48 INFO MemoryStore: Block rdd_63_0 stored as values in memory (estimated size 295.0 B, free 432.8 MiB)
26/02/22 12:37:48 INFO BlockManagerInfo: Added rdd_63_0 in memory on spark-master:36615 (size: 295.0 B, free: 433.3 MiB)
26/02/22 12:37:48 INFO Executor: Finished task 0.0 in stage 27.0 (TID 21). 1749 bytes result sent to driver
26/02/22 12:37:48 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 675 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:48 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/22 12:37:48 INFO DAGScheduler: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073) finished in 0.690 s
26/02/22 12:37:48 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/22 12:37:48 INFO DAGScheduler: Job 14 finished: count at HoodieSparkSqlWriter.scala:1073, took 0.759509 s
26/02/22 12:37:48 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/22 12:37:49 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:107
26/02/22 12:37:49 INFO DAGScheduler: Got job 15 (collect at SparkRDDWriteClient.java:107) with 1 output partitions
26/02/22 12:37:49 INFO DAGScheduler: Final stage: ResultStage 32 (collect at SparkRDDWriteClient.java:107)
26/02/22 12:37:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/22 12:37:49 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:49 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 314.4 KiB, free 432.5 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 108.4 KiB, free 432.4 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:36615 (size: 108.4 KiB, free: 433.2 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 22) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 32.0 (TID 22)
26/02/22 12:37:49 INFO BlockManager: Found block rdd_63_0 locally
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 32.0 (TID 22). 1710 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 22) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ResultStage 32 (collect at SparkRDDWriteClient.java:107) finished in 0.021 s
26/02/22 12:37:49 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/22 12:37:49 INFO DAGScheduler: Job 15 finished: collect at SparkRDDWriteClient.java:107, took 0.023568 s
26/02/22 12:37:49 INFO BaseHoodieWriteClient: Committing 20260222123745243 action commit
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123745243__commit__INFLIGHT__20260222123748202]}
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:37:49 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123745243__commit__INFLIGHT__20260222123748202]}
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:37:49 INFO BaseHoodieWriteClient: Committing 20260222123745243 action commit
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:49 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO HoodieTableMetadataUtil: Updating at 20260222123745243 from Commit/UPSERT. #partitions_updated=2, #files_added=1
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:37:49 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:49 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:49 INFO HoodieBackedTableMetadataWriter: New commit at 20260222123745243 being applied to MDT.
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:49 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222123746766]}
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:49 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222123745243 action: deltacommit
26/02/22 12:37:49 INFO HoodieActiveTimeline: Creating a new instant [==>20260222123745243__deltacommit__REQUESTED]
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222123745243__deltacommit__REQUESTED__20260222123749178]}
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:49 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:37:49 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:36615 in memory (size: 108.4 KiB, free: 433.3 MiB)
26/02/22 12:37:49 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/22 12:37:49 INFO DAGScheduler: Registering RDD 74 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 8
26/02/22 12:37:49 INFO DAGScheduler: Got job 16 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/22 12:37:49 INFO DAGScheduler: Final stage: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105)
26/02/22 12:37:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
26/02/22 12:37:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 33)
26/02/22 12:37:49 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 10.1 KiB, free 432.8 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 432.8 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:36615 (size: 5.4 KiB, free: 433.3 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 23) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 33.0 (TID 23)
26/02/22 12:37:49 INFO MemoryStore: Block rdd_72_0 stored as values in memory (estimated size 342.0 B, free 432.8 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added rdd_72_0 in memory on spark-master:36615 (size: 342.0 B, free: 433.3 MiB)
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 33.0 (TID 23). 1093 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 23) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ShuffleMapStage 33 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.012 s
26/02/22 12:37:49 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:49 INFO DAGScheduler: running: Set()
26/02/22 12:37:49 INFO DAGScheduler: waiting: Set(ResultStage 34)
26/02/22 12:37:49 INFO DAGScheduler: failed: Set()
26/02/22 12:37:49 INFO DAGScheduler: Submitting ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 5.5 KiB, free 432.8 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.7 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:36615 (size: 3.1 KiB, free: 433.3 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 24) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 34.0 (TID 24)
26/02/22 12:37:49 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 34.0 (TID 24). 1323 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 24) in 5 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.007 s
26/02/22 12:37:49 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
26/02/22 12:37:49 INFO DAGScheduler: Job 16 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.021348 s
26/02/22 12:37:49 INFO BaseSparkCommitActionExecutor: Source read and index timer 33
26/02/22 12:37:49 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:37:49 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/22 12:37:49 INFO DAGScheduler: Got job 17 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/22 12:37:49 INFO DAGScheduler: Final stage: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285)
26/02/22 12:37:49 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:49 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:49 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 262.2 KiB, free 432.5 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 432.4 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:36615 (size: 90.5 KiB, free: 433.2 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:36615 in memory (size: 3.1 KiB, free: 433.2 MiB)
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 25) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 35.0 (TID 25)
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:36615 in memory (size: 5.4 KiB, free: 433.2 MiB)
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:49 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:36615 in memory (size: 108.0 KiB, free: 433.3 MiB)
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 35.0 (TID 25). 837 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 25) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285) finished in 0.044 s
26/02/22 12:37:49 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/22 12:37:49 INFO DAGScheduler: Job 17 finished: collectAsMap at UpsertPartitioner.java:285, took 0.046139 s
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/22 12:37:49 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260222123745243.deltacommit.inflight
26/02/22 12:37:49 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:37:49 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260222123745243
26/02/22 12:37:49 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:37:49 INFO DAGScheduler: Registering RDD 78 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 9
26/02/22 12:37:49 INFO DAGScheduler: Got job 18 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:37:49 INFO DAGScheduler: Final stage: ResultStage 37 (collect at HoodieJavaRDD.java:177)
26/02/22 12:37:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)
26/02/22 12:37:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 36)
26/02/22 12:37:49 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 267.0 KiB, free 432.6 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 93.0 KiB, free 432.5 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:36615 (size: 93.0 KiB, free: 433.2 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 26) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 36.0 (TID 26)
26/02/22 12:37:49 INFO BlockManager: Found block rdd_72_0 locally
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 36.0 (TID 26). 1050 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 26) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ShuffleMapStage 36 (mapToPair at HoodieJavaRDD.java:149) finished in 0.018 s
26/02/22 12:37:49 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:49 INFO DAGScheduler: running: Set()
26/02/22 12:37:49 INFO DAGScheduler: waiting: Set(ResultStage 37)
26/02/22 12:37:49 INFO DAGScheduler: failed: Set()
26/02/22 12:37:49 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:36615 in memory (size: 90.5 KiB, free: 433.3 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 372.6 KiB, free 432.5 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.3 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:36615 (size: 132.2 KiB, free: 433.2 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 37.0 (TID 27)
26/02/22 12:37:49 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:49 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260222123745243 for file files-0000-0
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:49 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:49 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/22 12:37:49 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:37:49 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:37:49 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222123745243/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND
26/02/22 12:37:49 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222123745243/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND in 28 ms
26/02/22 12:37:49 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-37-27', fileLen=-1}
26/02/22 12:37:49 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:49 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:37:49 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-37-27, took 366 ms.
26/02/22 12:37:49 INFO MemoryStore: Block rdd_82_0 stored as values in memory (estimated size 343.0 B, free 432.3 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added rdd_82_0 in memory on spark-master:36615 (size: 343.0 B, free: 433.2 MiB)
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 37.0 (TID 27). 1627 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 27) in 408 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ResultStage 37 (collect at HoodieJavaRDD.java:177) finished in 0.426 s
26/02/22 12:37:49 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
26/02/22 12:37:49 INFO DAGScheduler: Job 18 finished: collect at HoodieJavaRDD.java:177, took 0.447284 s
26/02/22 12:37:49 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/22 12:37:49 INFO BaseSparkCommitActionExecutor: Committing 20260222123745243, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/22 12:37:49 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:37:49 INFO DAGScheduler: Got job 19 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:37:49 INFO DAGScheduler: Final stage: ResultStage 38 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:37:49 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:49 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:49 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 101.2 KiB, free 432.2 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.2 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:36615 (size: 36.1 KiB, free: 433.1 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 38.0 (TID 28)
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 38.0 (TID 28). 804 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 28) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ResultStage 38 (collect at HoodieSparkEngineContext.java:150) finished in 0.013 s
26/02/22 12:37:49 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/22 12:37:49 INFO DAGScheduler: Job 19 finished: collect at HoodieSparkEngineContext.java:150, took 0.014811 s
26/02/22 12:37:49 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:37:49 INFO DAGScheduler: Got job 20 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:37:49 INFO DAGScheduler: Final stage: ResultStage 39 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:37:49 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:49 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:49 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 101.1 KiB, free 432.1 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.1 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:36615 (size: 36.1 KiB, free: 433.1 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:36615 in memory (size: 36.1 KiB, free: 433.1 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:36615 in memory (size: 132.2 KiB, free: 433.3 MiB)
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 39.0 (TID 29)
26/02/22 12:37:49 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:36615 in memory (size: 93.0 KiB, free: 433.4 MiB)
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 39.0 (TID 29). 857 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 29) in 16 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ResultStage 39 (collect at HoodieSparkEngineContext.java:150) finished in 0.028 s
26/02/22 12:37:49 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/22 12:37:49 INFO DAGScheduler: Job 20 finished: collect at HoodieSparkEngineContext.java:150, took 0.029547 s
26/02/22 12:37:49 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123745243__deltacommit__INFLIGHT]
26/02/22 12:37:49 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260222123745243.deltacommit
26/02/22 12:37:49 INFO HoodieActiveTimeline: Completed [==>20260222123745243__deltacommit__INFLIGHT]
26/02/22 12:37:49 INFO BaseSparkCommitActionExecutor: Committed 20260222123745243
26/02/22 12:37:49 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:37:49 INFO DAGScheduler: Got job 21 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/22 12:37:49 INFO DAGScheduler: Final stage: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:37:49 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:49 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:49 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 101.3 KiB, free 433.0 MiB)
26/02/22 12:37:49 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.9 MiB)
26/02/22 12:37:49 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:36615 (size: 36.2 KiB, free: 433.3 MiB)
26/02/22 12:37:49 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:49 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
26/02/22 12:37:49 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 30) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:37:49 INFO Executor: Running task 0.0 in stage 40.0 (TID 30)
26/02/22 12:37:49 INFO Executor: Finished task 0.0 in stage 40.0 (TID 30). 904 bytes result sent to driver
26/02/22 12:37:49 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 30) in 20 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
26/02/22 12:37:49 INFO DAGScheduler: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.027 s
26/02/22 12:37:49 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
26/02/22 12:37:49 INFO DAGScheduler: Job 21 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.029102 s
26/02/22 12:37:49 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222123745243
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:49 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:49 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:37:49 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:37:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:49 INFO HoodieActiveTimeline: Marking instant complete [==>20260222123745243__commit__INFLIGHT]
26/02/22 12:37:49 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260222123745243.commit
26/02/22 12:37:49 INFO HoodieActiveTimeline: Completed [==>20260222123745243__commit__INFLIGHT]
26/02/22 12:37:50 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:37:50 INFO DAGScheduler: Got job 22 (collectAsMap at HoodieSparkEngineContext.java:164) with 2 output partitions
26/02/22 12:37:50 INFO DAGScheduler: Final stage: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:37:50 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:50 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:50 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 101.3 KiB, free 432.8 MiB)
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.8 MiB)
26/02/22 12:37:50 INFO BlockManager: Removing RDD 72
26/02/22 12:37:50 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:36615 (size: 36.2 KiB, free: 433.3 MiB)
26/02/22 12:37:50 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1))
26/02/22 12:37:50 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks resource profile 0
26/02/22 12:37:50 INFO BlockManager: Removing RDD 82
26/02/22 12:37:50 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4415 bytes) taskResourceAssignments Map()
26/02/22 12:37:50 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 32) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 4411 bytes) taskResourceAssignments Map()
26/02/22 12:37:50 INFO Executor: Running task 0.0 in stage 41.0 (TID 31)
26/02/22 12:37:50 INFO Executor: Running task 1.0 in stage 41.0 (TID 32)
26/02/22 12:37:50 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:36615 in memory (size: 36.2 KiB, free: 433.3 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:36615 in memory (size: 36.1 KiB, free: 433.4 MiB)
26/02/22 12:37:50 INFO Executor: Finished task 1.0 in stage 41.0 (TID 32). 890 bytes result sent to driver
26/02/22 12:37:50 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 32) in 33 ms on spark-master (executor driver) (1/2)
26/02/22 12:37:50 INFO Executor: Finished task 0.0 in stage 41.0 (TID 31). 894 bytes result sent to driver
26/02/22 12:37:50 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 31) in 67 ms on spark-master (executor driver) (2/2)
26/02/22 12:37:50 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/22 12:37:50 INFO DAGScheduler: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.076 s
26/02/22 12:37:50 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/22 12:37:50 INFO DAGScheduler: Job 22 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.078831 s
26/02/22 12:37:50 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/.temp/20260222123745243
26/02/22 12:37:50 INFO BaseHoodieWriteClient: Committed 20260222123745243
26/02/22 12:37:50 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
26/02/22 12:37:50 INFO BlockManager: Removing RDD 53
26/02/22 12:37:50 INFO MapPartitionsRDD: Removing RDD 63 from persistence list
26/02/22 12:37:50 INFO BlockManager: Removing RDD 63
26/02/22 12:37:50 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:37:50 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:37:50 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:37:50 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:37:50 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260222123750098
26/02/22 12:37:50 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:39203, Timeout=300
26/02/22 12:37:50 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:39203/v1/hoodie/view/compactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222123745243&timelinehash=a94c523d94ea0323661bc1f8be94a3c58ef44ee56724e11876118a901b471e23)
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:39203/v1/hoodie/view/logcompactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222123745243&timelinehash=a94c523d94ea0323661bc1f8be94a3c58ef44ee56724e11876118a901b471e23)
26/02/22 12:37:50 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:37:50 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:37:50 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:37:50 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/22 12:37:50 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:37:50 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:39203, Timeout=300
26/02/22 12:37:50 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:39203/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222123745243&timelinehash=a94c523d94ea0323661bc1f8be94a3c58ef44ee56724e11876118a901b471e23)
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__deltacommit__COMPLETED__20260222123749880]}
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Commit 20260222123745243 successful!
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:37:50 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:37:50 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/22 12:37:50 INFO BaseHoodieClient: Stopping Timeline service !!
26/02/22 12:37:50 INFO EmbeddedTimelineService: Closing Timeline server
26/02/22 12:37:50 INFO TimelineService: Closing Timeline Service
26/02/22 12:37:50 INFO Javalin: Stopping Javalin ...
26/02/22 12:37:50 INFO Javalin: Javalin has stopped
26/02/22 12:37:50 INFO TimelineService: Closed Timeline Service
26/02/22 12:37:50 INFO EmbeddedTimelineService: Closed Timeline server
26/02/22 12:37:50 INFO DataSourceUtils: Getting table path..
26/02/22 12:37:50 INFO TablePathUtils: Getting table path from path : s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:37:50 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:50 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:37:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222123745243__commit__COMPLETED__20260222123749978]}
26/02/22 12:37:50 INFO BaseHoodieTableFileIndex: Refresh table silver_cleaned_trips, spent: 8 ms
26/02/22 12:37:50 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:37:50 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:37:50 INFO DAGScheduler: Got job 23 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:37:50 INFO DAGScheduler: Final stage: ResultStage 42 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:37:50 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:50 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:50 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 101.1 KiB, free 433.9 MiB)
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 433.9 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:36615 (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:37:50 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:50 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
26/02/22 12:37:50 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4368 bytes) taskResourceAssignments Map()
26/02/22 12:37:50 INFO Executor: Running task 0.0 in stage 42.0 (TID 33)
26/02/22 12:37:50 INFO Executor: Finished task 0.0 in stage 42.0 (TID 33). 1131 bytes result sent to driver
26/02/22 12:37:50 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 7 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:50 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/22 12:37:50 INFO DAGScheduler: ResultStage 42 (collect at HoodieSparkEngineContext.java:116) finished in 0.013 s
26/02/22 12:37:50 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/22 12:37:50 INFO DAGScheduler: Job 23 finished: collect at HoodieSparkEngineContext.java:116, took 0.013500 s
26/02/22 12:37:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:37:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:37:50 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:37:50 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:37:50 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 210.5 KiB, free 433.7 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:36615 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 433.9 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:36615 in memory (size: 36.3 KiB, free: 434.4 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:36615 (size: 35.2 KiB, free: 434.3 MiB)
26/02/22 12:37:50 INFO SparkContext: Created broadcast 35 from count at <unknown>:0
26/02/22 12:37:50 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:37:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:37:50 INFO DAGScheduler: Registering RDD 97 (count at <unknown>:0) as input to shuffle 10
26/02/22 12:37:50 INFO DAGScheduler: Got map stage job 24 (count at <unknown>:0) with 1 output partitions
26/02/22 12:37:50 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (count at <unknown>:0)
26/02/22 12:37:50 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:37:50 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:50 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0), which has no missing parents
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.9 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:36615 (size: 7.4 KiB, free: 434.3 MiB)
26/02/22 12:37:50 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:50 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
26/02/22 12:37:50 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()
26/02/22 12:37:50 INFO Executor: Running task 0.0 in stage 43.0 (TID 34)
26/02/22 12:37:50 INFO BlockManager: Removing RDD 63
26/02/22 12:37:50 INFO BlockManager: Removing RDD 53
26/02/22 12:37:50 INFO FileScanRDD: Reading File path: s3a://warehouse/silver/cleaned_trips/edcb56fb-5cde-412a-a020-ac8bb8d14b37-0_0-27-21_20260222123745243.parquet, range: 0-1019912, partition values: [empty row]
26/02/22 12:37:50 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:36615 in memory (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:37:50 INFO BlockManager: Removing RDD 36
26/02/22 12:37:50 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:50 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:37:50 INFO Executor: Finished task 0.0 in stage 43.0 (TID 34). 2056 bytes result sent to driver
26/02/22 12:37:50 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 34) in 40 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:50 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
26/02/22 12:37:50 INFO DAGScheduler: ShuffleMapStage 43 (count at <unknown>:0) finished in 0.046 s
26/02/22 12:37:50 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:37:50 INFO DAGScheduler: running: Set()
26/02/22 12:37:50 INFO DAGScheduler: waiting: Set()
26/02/22 12:37:50 INFO DAGScheduler: failed: Set()
26/02/22 12:37:50 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:37:50 INFO DAGScheduler: Got job 25 (count at <unknown>:0) with 1 output partitions
26/02/22 12:37:50 INFO DAGScheduler: Final stage: ResultStage 45 (count at <unknown>:0)
26/02/22 12:37:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
26/02/22 12:37:50 INFO DAGScheduler: Missing parents: List()
26/02/22 12:37:50 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0), which has no missing parents
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/22 12:37:50 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:36615 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:37:50 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:36615 (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:37:50 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1509
26/02/22 12:37:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:37:50 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
26/02/22 12:37:50 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:37:50 INFO Executor: Running task 0.0 in stage 45.0 (TID 35)
26/02/22 12:37:50 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:37:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:37:50 INFO Executor: Finished task 0.0 in stage 45.0 (TID 35). 2458 bytes result sent to driver
26/02/22 12:37:50 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 4 ms on spark-master (executor driver) (1/1)
26/02/22 12:37:50 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/22 12:37:50 INFO DAGScheduler: ResultStage 45 (count at <unknown>:0) finished in 0.013 s
26/02/22 12:37:50 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:37:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/22 12:37:50 INFO DAGScheduler: Job 25 finished: count at <unknown>:0, took 0.015099 s
Silver transform complete. Rows written: 9,855
Rows filtered out: 145
============================================================
  Silver Transform (Hudi Upsert) COMPLETE
============================================================
26/02/22 12:37:50 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/22 12:37:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/22 12:37:50 INFO MemoryStore: MemoryStore cleared
26/02/22 12:37:50 INFO BlockManager: BlockManager stopped
26/02/22 12:37:50 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/22 12:37:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/22 12:37:50 INFO SparkContext: Successfully stopped SparkContext
26/02/22 12:37:50 INFO ShutdownHookManager: Shutdown hook called
26/02/22 12:37:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-af9296f0-0289-46a7-81a6-21df12f2c049
26/02/22 12:37:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7661186-aa68-43a3-a251-c5c57b2bb0e3/pyspark-3b943dee-92aa-488e-9ca4-afa495d50836
26/02/22 12:37:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7661186-aa68-43a3-a251-c5c57b2bb0e3
26/02/22 12:37:50 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
26/02/22 12:37:50 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
26/02/22 12:37:50 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
Silver transform complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir ."
 Container p22-minio  Running
[0m12:37:53  Running with dbt=1.11.5
[0m12:37:53  Installing dbt-labs/dbt_utils
[0m12:37:56  Installed from version 1.3.3
[0m12:37:56  Up to date!
[0m12:37:58  Running with dbt=1.11.5
[0m12:37:58  Registered adapter: duckdb=1.10.0
[0m12:37:58  Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:38:00  Found 14 models, 74 data tests, 3 seeds, 1 source, 606 macros
[0m12:38:00  
[0m12:38:00  Concurrency: 4 threads (target='dev')
[0m12:38:00  
[0m12:38:01  1 of 91 START sql table model main.dim_dates ................................... [RUN]
[0m12:38:01  2 of 91 START sql view model main.stg_yellow_trips ............................. [RUN]
[0m12:38:01  3 of 91 START seed file main_main.payment_type_lookup .......................... [RUN]
[0m12:38:01  4 of 91 START seed file main_main.rate_code_lookup ............................. [RUN]
[0m12:38:01  3 of 91 OK loaded seed file main_main.payment_type_lookup ...................... [[32mCREATE 6[0m in 0.14s]
[0m12:38:01  5 of 91 START seed file main_main.taxi_zone_lookup ............................. [RUN]
[0m12:38:01  4 of 91 OK loaded seed file main_main.rate_code_lookup ......................... [[32mCREATE 7[0m in 0.15s]
[0m12:38:01  6 of 91 START test not_null_payment_type_lookup_payment_type_id ................ [RUN]
[0m12:38:01  2 of 91 OK created sql view model main.stg_yellow_trips ........................ [[32mOK[0m in 0.17s]
[0m12:38:01  7 of 91 START test unique_payment_type_lookup_payment_type_id .................. [RUN]
[0m12:38:01  1 of 91 OK created sql table model main.dim_dates .............................. [[32mOK[0m in 0.21s]
[0m12:38:01  8 of 91 START test not_null_rate_code_lookup_rate_code_id ...................... [RUN]
[0m12:38:01  6 of 91 PASS not_null_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.06s]
[0m12:38:01  7 of 91 PASS unique_payment_type_lookup_payment_type_id ........................ [[32mPASS[0m in 0.05s]
[0m12:38:01  9 of 91 START test unique_rate_code_lookup_rate_code_id ........................ [RUN]
[0m12:38:01  10 of 91 START test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [RUN]
[0m12:38:01  5 of 91 OK loaded seed file main_main.taxi_zone_lookup ......................... [[32mCREATE 265[0m in 0.09s]
[0m12:38:01  11 of 91 START test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [RUN]
[0m12:38:01  8 of 91 PASS not_null_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.11s]
[0m12:38:01  12 of 91 START test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ........ [RUN]
[0m12:38:01  9 of 91 PASS unique_rate_code_lookup_rate_code_id .............................. [[32mPASS[0m in 0.11s]
[0m12:38:01  13 of 91 START test assert_fare_not_exceeds_total .............................. [RUN]
[0m12:38:01  10 of 91 PASS accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[32mPASS[0m in 0.12s]
[0m12:38:01  14 of 91 START test not_null_stg_yellow_trips_dropoff_datetime ................. [RUN]
[0m12:38:01  11 of 91 PASS accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[32mPASS[0m in 0.07s]
[0m12:38:01  15 of 91 START test not_null_stg_yellow_trips_dropoff_location_id .............. [RUN]
[0m12:38:01  12 of 91 PASS accepted_values_stg_yellow_trips_vendor_id__1__2__6 .............. [[32mPASS[0m in 0.07s]
[0m12:38:01  16 of 91 START test not_null_stg_yellow_trips_fare_amount ...................... [RUN]
[0m12:38:01  13 of 91 PASS assert_fare_not_exceeds_total .................................... [[32mPASS[0m in 0.06s]
[0m12:38:01  17 of 91 START test not_null_stg_yellow_trips_pickup_datetime .................. [RUN]
[0m12:38:01  14 of 91 PASS not_null_stg_yellow_trips_dropoff_datetime ....................... [[32mPASS[0m in 0.07s]
[0m12:38:01  18 of 91 START test not_null_stg_yellow_trips_pickup_location_id ............... [RUN]
[0m12:38:01  15 of 91 PASS not_null_stg_yellow_trips_dropoff_location_id .................... [[32mPASS[0m in 0.06s]
[0m12:38:01  19 of 91 START test not_null_stg_yellow_trips_total_amount ..................... [RUN]
[0m12:38:01  16 of 91 PASS not_null_stg_yellow_trips_fare_amount ............................ [[32mPASS[0m in 0.07s]
[0m12:38:01  20 of 91 START test not_null_stg_yellow_trips_trip_distance_miles .............. [RUN]
[0m12:38:01  17 of 91 PASS not_null_stg_yellow_trips_pickup_datetime ........................ [[32mPASS[0m in 0.05s]
[0m12:38:01  21 of 91 START test not_null_stg_yellow_trips_trip_id .......................... [RUN]
[0m12:38:01  18 of 91 PASS not_null_stg_yellow_trips_pickup_location_id ..................... [[32mPASS[0m in 0.05s]
[0m12:38:01  22 of 91 START test not_null_stg_yellow_trips_vendor_id ........................ [RUN]
[0m12:38:01  19 of 91 PASS not_null_stg_yellow_trips_total_amount ........................... [[32mPASS[0m in 0.06s]
[0m12:38:01  23 of 91 START test unique_stg_yellow_trips_trip_id ............................ [RUN]
[0m12:38:01  20 of 91 PASS not_null_stg_yellow_trips_trip_distance_miles .................... [[32mPASS[0m in 0.06s]
[0m12:38:01  21 of 91 PASS not_null_stg_yellow_trips_trip_id ................................ [[32mPASS[0m in 0.05s]
[0m12:38:01  24 of 91 START test not_null_dim_dates_date_key ................................ [RUN]
[0m12:38:01  25 of 91 START test not_null_dim_dates_day_of_week_name ........................ [RUN]
[0m12:38:01  22 of 91 PASS not_null_stg_yellow_trips_vendor_id .............................. [[32mPASS[0m in 0.06s]
[0m12:38:01  26 of 91 START test not_null_dim_dates_is_holiday .............................. [RUN]
[0m12:38:01  23 of 91 PASS unique_stg_yellow_trips_trip_id .................................. [[32mPASS[0m in 0.06s]
[0m12:38:01  27 of 91 START test not_null_dim_dates_is_weekend .............................. [RUN]
[0m12:38:01  24 of 91 PASS not_null_dim_dates_date_key ...................................... [[32mPASS[0m in 0.06s]
[0m12:38:01  25 of 91 PASS not_null_dim_dates_day_of_week_name .............................. [[32mPASS[0m in 0.06s]
[0m12:38:01  28 of 91 START test unique_dim_dates_date_key .................................. [RUN]
[0m12:38:01  29 of 91 START sql view model main.stg_payment_types ........................... [RUN]
[0m12:38:01  26 of 91 PASS not_null_dim_dates_is_holiday .................................... [[32mPASS[0m in 0.06s]
[0m12:38:01  30 of 91 START test not_null_taxi_zone_lookup_Borough .......................... [RUN]
[0m12:38:01  27 of 91 PASS not_null_dim_dates_is_weekend .................................... [[32mPASS[0m in 0.05s]
[0m12:38:01  31 of 91 START test not_null_taxi_zone_lookup_LocationID ....................... [RUN]
[0m12:38:01  28 of 91 PASS unique_dim_dates_date_key ........................................ [[32mPASS[0m in 0.06s]
[0m12:38:01  30 of 91 PASS not_null_taxi_zone_lookup_Borough ................................ [[32mPASS[0m in 0.04s]
[0m12:38:01  32 of 91 START test unique_taxi_zone_lookup_LocationID ......................... [RUN]
[0m12:38:01  33 of 91 START sql view model main.stg_rate_codes .............................. [RUN]
[0m12:38:01  29 of 91 OK created sql view model main.stg_payment_types ...................... [[32mOK[0m in 0.07s]
[0m12:38:01  34 of 91 START sql view model main.int_trip_metrics ............................ [RUN]
[0m12:38:01  31 of 91 PASS not_null_taxi_zone_lookup_LocationID ............................. [[32mPASS[0m in 0.05s]
[0m12:38:01  35 of 91 START test not_null_stg_payment_types_payment_type_id ................. [RUN]
[0m12:38:01  32 of 91 PASS unique_taxi_zone_lookup_LocationID ............................... [[32mPASS[0m in 0.06s]
[0m12:38:01  36 of 91 START test not_null_stg_payment_types_payment_type_name ............... [RUN]
[0m12:38:01  33 of 91 OK created sql view model main.stg_rate_codes ......................... [[32mOK[0m in 0.07s]
[0m12:38:01  37 of 91 START test unique_stg_payment_types_payment_type_id ................... [RUN]
[0m12:38:01  35 of 91 PASS not_null_stg_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m12:38:01  34 of 91 OK created sql view model main.int_trip_metrics ....................... [[32mOK[0m in 0.08s]
[0m12:38:01  38 of 91 START sql view model main.stg_taxi_zones .............................. [RUN]
[0m12:38:01  39 of 91 START test not_null_stg_rate_codes_rate_code_id ....................... [RUN]
[0m12:38:01  36 of 91 PASS not_null_stg_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m12:38:01  37 of 91 PASS unique_stg_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.04s]
[0m12:38:01  40 of 91 START test not_null_stg_rate_codes_rate_code_name ..................... [RUN]
[0m12:38:01  41 of 91 START test unique_stg_rate_codes_rate_code_id ......................... [RUN]
[0m12:38:01  39 of 91 PASS not_null_stg_rate_codes_rate_code_id ............................. [[32mPASS[0m in 0.05s]
[0m12:38:01  42 of 91 START test assert_trip_duration_positive .............................. [RUN]
[0m12:38:01  38 of 91 OK created sql view model main.stg_taxi_zones ......................... [[32mOK[0m in 0.06s]
[0m12:38:01  40 of 91 PASS not_null_stg_rate_codes_rate_code_name ........................... [[32mPASS[0m in 0.04s]
[0m12:38:01  43 of 91 START test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [RUN]
[0m12:38:01  44 of 91 START test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [RUN]
[0m12:38:01  41 of 91 PASS unique_stg_rate_codes_rate_code_id ............................... [[32mPASS[0m in 0.05s]
[0m12:38:01  45 of 91 START test not_null_int_trip_metrics_is_weekend ....................... [RUN]
[0m12:38:02  42 of 91 PASS assert_trip_duration_positive .................................... [[32mPASS[0m in 0.05s]
[0m12:38:02  46 of 91 START test not_null_int_trip_metrics_pickup_date ...................... [RUN]
[0m12:38:02  43 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0 ..... [[32mPASS[0m in 0.05s]
[0m12:38:02  47 of 91 START test not_null_int_trip_metrics_pickup_hour ...................... [RUN]
[0m12:38:02  45 of 91 PASS not_null_int_trip_metrics_is_weekend ............................. [[32mPASS[0m in 0.05s]
[0m12:38:02  44 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[32mPASS[0m in 0.06s]
[0m12:38:02  48 of 91 START test not_null_int_trip_metrics_trip_duration_minutes ............ [RUN]
[0m12:38:02  49 of 91 START test not_null_int_trip_metrics_trip_id .......................... [RUN]
[0m12:38:02  46 of 91 PASS not_null_int_trip_metrics_pickup_date ............................ [[32mPASS[0m in 0.06s]
[0m12:38:02  49 of 91 PASS not_null_int_trip_metrics_trip_id ................................ [[32mPASS[0m in 0.04s]
[0m12:38:02  47 of 91 PASS not_null_int_trip_metrics_pickup_hour ............................ [[32mPASS[0m in 0.06s]
[0m12:38:02  50 of 91 START sql table model main.dim_payment_types .......................... [RUN]
[0m12:38:02  48 of 91 PASS not_null_int_trip_metrics_trip_duration_minutes .................. [[32mPASS[0m in 0.05s]
[0m12:38:02  51 of 91 START test not_null_stg_taxi_zones_borough ............................ [RUN]
[0m12:38:02  52 of 91 START test not_null_stg_taxi_zones_location_id ........................ [RUN]
[0m12:38:02  53 of 91 START test not_null_stg_taxi_zones_zone_name .......................... [RUN]
[0m12:38:02  53 of 91 PASS not_null_stg_taxi_zones_zone_name ................................ [[32mPASS[0m in 0.04s]
[0m12:38:02  54 of 91 START test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:38:02  51 of 91 PASS not_null_stg_taxi_zones_borough .................................. [[32mPASS[0m in 0.05s]
[0m12:38:02  52 of 91 PASS not_null_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.06s]
[0m12:38:02  55 of 91 START test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:38:02  56 of 91 START test unique_stg_taxi_zones_location_id .......................... [RUN]
[0m12:38:02  50 of 91 OK created sql table model main.dim_payment_types ..................... [[32mOK[0m in 0.08s]
[0m12:38:02  57 of 91 START sql view model main.int_daily_summary ........................... [RUN]
[0m12:38:02  55 of 91 PASS relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.04s]
[0m12:38:02  54 of 91 PASS relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.05s]
[0m12:38:02  58 of 91 START sql view model main.int_hourly_patterns ......................... [RUN]
[0m12:38:02  59 of 91 START test not_null_dim_payment_types_payment_type_id ................. [RUN]
[0m12:38:02  56 of 91 PASS unique_stg_taxi_zones_location_id ................................ [[32mPASS[0m in 0.05s]
[0m12:38:02  60 of 91 START test not_null_dim_payment_types_payment_type_name ............... [RUN]
[0m12:38:02  57 of 91 OK created sql view model main.int_daily_summary ...................... [[32mOK[0m in 0.06s]
[0m12:38:02  61 of 91 START test unique_dim_payment_types_payment_type_id ................... [RUN]
[0m12:38:02  59 of 91 PASS not_null_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.05s]
[0m12:38:02  60 of 91 PASS not_null_dim_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m12:38:02  62 of 91 START sql table model main.dim_locations .............................. [RUN]
[0m12:38:02  63 of 91 START test dbt_utils_accepted_range_int_daily_summary_total_trips__0 .. [RUN]
[0m12:38:02  58 of 91 OK created sql view model main.int_hourly_patterns .................... [[32mOK[0m in 0.07s]
[0m12:38:02  64 of 91 START test not_null_int_daily_summary_pickup_date ..................... [RUN]
[0m12:38:02  61 of 91 PASS unique_dim_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m12:38:02  65 of 91 START test not_null_int_daily_summary_total_revenue ................... [RUN]
[0m12:38:02  62 of 91 OK created sql table model main.dim_locations ......................... [[32mOK[0m in 0.13s]
[0m12:38:02  66 of 91 START test not_null_int_daily_summary_total_trips ..................... [RUN]
[0m12:38:02  64 of 91 PASS not_null_int_daily_summary_pickup_date ........................... [[32mPASS[0m in 0.12s]
[0m12:38:02  63 of 91 PASS dbt_utils_accepted_range_int_daily_summary_total_trips__0 ........ [[32mPASS[0m in 0.14s]
[0m12:38:02  67 of 91 START test unique_int_daily_summary_pickup_date ....................... [RUN]
[0m12:38:02  68 of 91 START test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [RUN]
[0m12:38:02  65 of 91 PASS not_null_int_daily_summary_total_revenue ......................... [[32mPASS[0m in 0.08s]
[0m12:38:02  68 of 91 PASS dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0 .. [[32mPASS[0m in 0.04s]
[0m12:38:02  69 of 91 START test not_null_int_hourly_patterns_pickup_date ................... [RUN]
[0m12:38:02  70 of 91 START test not_null_int_hourly_patterns_pickup_hour ................... [RUN]
[0m12:38:02  66 of 91 PASS not_null_int_daily_summary_total_trips ........................... [[32mPASS[0m in 0.08s]
[0m12:38:02  71 of 91 START test not_null_int_hourly_patterns_total_trips ................... [RUN]
[0m12:38:02  67 of 91 PASS unique_int_daily_summary_pickup_date ............................. [[32mPASS[0m in 0.08s]
[0m12:38:02  72 of 91 START test not_null_dim_locations_borough ............................. [RUN]
[0m12:38:02  70 of 91 PASS not_null_int_hourly_patterns_pickup_hour ......................... [[32mPASS[0m in 0.05s]
[0m12:38:02  73 of 91 START test not_null_dim_locations_location_id ......................... [RUN]
[0m12:38:02  69 of 91 PASS not_null_int_hourly_patterns_pickup_date ......................... [[32mPASS[0m in 0.06s]
[0m12:38:02  74 of 91 START test not_null_dim_locations_zone_name ........................... [RUN]
[0m12:38:02  72 of 91 PASS not_null_dim_locations_borough ................................... [[32mPASS[0m in 0.05s]
[0m12:38:02  71 of 91 PASS not_null_int_hourly_patterns_total_trips ......................... [[32mPASS[0m in 0.07s]
[0m12:38:02  75 of 91 START test unique_dim_locations_location_id ........................... [RUN]
[0m12:38:02  76 of 91 START sql table model main.mart_daily_revenue ......................... [RUN]
[0m12:38:02  73 of 91 PASS not_null_dim_locations_location_id ............................... [[32mPASS[0m in 0.05s]
[0m12:38:02  74 of 91 PASS not_null_dim_locations_zone_name ................................. [[32mPASS[0m in 0.04s]
[0m12:38:02  77 of 91 START sql table model main.mart_hourly_demand ......................... [RUN]
[0m12:38:02  75 of 91 PASS unique_dim_locations_location_id ................................. [[32mPASS[0m in 0.04s]
[0m12:38:02  78 of 91 START sql incremental model main.fct_trips ............................ [RUN]
[0m12:38:02  78 of 91 ERROR creating sql incremental model main.fct_trips ................... [[31mERROR[0m in 0.05s]
[0m12:38:02  79 of 91 SKIP test not_null_fct_trips_pickup_datetime .......................... [[33mSKIP[0m]
[0m12:38:02  80 of 91 SKIP test not_null_fct_trips_total_amount ............................. [[33mSKIP[0m]
[0m12:38:02  81 of 91 SKIP test not_null_fct_trips_trip_id .................................. [[33mSKIP[0m]
[0m12:38:02  82 of 91 SKIP test unique_fct_trips_trip_id .................................... [[33mSKIP[0m]
[0m12:38:02  83 of 91 SKIP relation main.mart_location_performance .......................... [[33mSKIP[0m]
[0m12:38:02  84 of 91 SKIP test not_null_mart_location_performance_pickup_location_id ....... [[33mSKIP[0m]
[0m12:38:02  85 of 91 SKIP test not_null_mart_location_performance_total_pickups ............ [[33mSKIP[0m]
[0m12:38:02  86 of 91 SKIP test unique_mart_location_performance_pickup_location_id ......... [[33mSKIP[0m]
[0m12:38:02  76 of 91 OK created sql table model main.mart_daily_revenue .................... [[32mOK[0m in 0.14s]
[0m12:38:02  77 of 91 OK created sql table model main.mart_hourly_demand .................... [[32mOK[0m in 0.12s]
[0m12:38:02  87 of 91 START test not_null_mart_daily_revenue_date_key ....................... [RUN]
[0m12:38:02  88 of 91 START test not_null_mart_daily_revenue_total_revenue .................. [RUN]
[0m12:38:02  89 of 91 START test unique_mart_daily_revenue_date_key ......................... [RUN]
[0m12:38:02  90 of 91 START test not_null_mart_hourly_demand_is_weekend ..................... [RUN]
[0m12:38:02  87 of 91 PASS not_null_mart_daily_revenue_date_key ............................. [[32mPASS[0m in 0.04s]
[0m12:38:02  88 of 91 PASS not_null_mart_daily_revenue_total_revenue ........................ [[32mPASS[0m in 0.04s]
[0m12:38:02  91 of 91 START test not_null_mart_hourly_demand_pickup_hour .................... [RUN]
[0m12:38:02  89 of 91 PASS unique_mart_daily_revenue_date_key ............................... [[32mPASS[0m in 0.05s]
[0m12:38:02  90 of 91 PASS not_null_mart_hourly_demand_is_weekend ........................... [[32mPASS[0m in 0.05s]
[0m12:38:02  91 of 91 PASS not_null_mart_hourly_demand_pickup_hour .......................... [[32mPASS[0m in 0.03s]
[0m12:38:02  
[0m12:38:02  Finished running 1 incremental model, 3 seeds, 6 table models, 74 data tests, 7 view models in 0 hours 0 minutes and 2.68 seconds (2.68s).
[0m12:38:02  
[0m12:38:02  [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:38:02  
[0m12:38:02  [31mFailure in model fct_trips (models/marts/core/fct_trips.sql)[0m
[0m12:38:02    Compilation Error in model fct_trips (models/marts/core/fct_trips.sql)
  This model has an enforced contract that failed.
  Please ensure the name, data_type, and number of columns in your contract match the columns in your model's definition.
  
  | column_name  | definition_type | contract_type | mismatch_reason    |
  | ------------ | --------------- | ------------- | ------------------ |
  | extra_amount | DECIMAL(10,2)   | DOUBLE        | data type mismatch |
  
  
  > in macro assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro default__get_assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro get_assert_columns_equivalent (macros/relations/column/columns_spec_ddl.sql)
  > called by macro duckdb__create_table_as (macros/adapters.sql)
  > called by macro create_table_as (macros/relations/table/create.sql)
  > called by macro materialization_incremental_duckdb (macros/materializations/incremental.sql)
  > called by model fct_trips (models/marts/core/fct_trips.sql)
[0m12:38:02  
[0m12:38:02    compiled code at target/compiled/nyc_taxi_pipeline_22/models/marts/core/fct_trips.sql
[0m12:38:02  
[0m12:38:02  Done. PASS=82 WARN=0 ERROR=1 SKIP=8 NO-OP=0 TOTAL=91

make[1]: *** [Makefile:68: dbt-build] Error 1
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make: *** [Makefile:75: benchmark] Error 2
docker compose --profile generator --profile dbt down -v --remove-orphans
 Container p22-spark-worker  Stopping
 Container p22-spark-worker  Stopped
 Container p22-spark-worker  Removing
 Container p22-spark-worker  Removed
 Container p22-spark-master  Stopping
 Container p22-spark-master  Stopped
 Container p22-spark-master  Removing
 Container p22-spark-master  Removed
 Container p22-mc-init  Stopping
 Container p22-kafka  Stopping
 Container p22-mc-init  Stopped
 Container p22-mc-init  Removing
 Container p22-mc-init  Removed
 Container p22-minio  Stopping
 Container p22-minio  Stopped
 Container p22-minio  Removing
 Container p22-minio  Removed
 Container p22-kafka  Stopped
 Container p22-kafka  Removing
 Container p22-kafka  Removed
 Network p22-pipeline-net  Removing
 Volume 22-hudi-cdc-storage_minio-data  Removing
 Volume 22-hudi-cdc-storage_minio-data  Removed
 Network p22-pipeline-net  Removed
Pipeline 22 stopped.
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
Pipeline 22 cleaned.
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-minio  Creating
 Container p22-kafka  Creating
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-kafka  Created
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-kafka  Starting
 Container p22-minio  Starting
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-kafka  Started
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-mc-init  Waiting
 Container p22-kafka  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
============================================================
  Pipeline 22 Benchmark: Kafka + Spark + Apache Hudi
============================================================
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose up -d kafka minio mc-init spark-master spark-worker
 Network p22-pipeline-net  Creating
 Network p22-pipeline-net  Created
 Volume 22-hudi-cdc-storage_minio-data  Creating
 Volume 22-hudi-cdc-storage_minio-data  Created
 Container p22-minio  Creating
 Container p22-kafka  Creating
 Container p22-minio  Created
 Container p22-mc-init  Creating
 Container p22-kafka  Created
 Container p22-mc-init  Created
 Container p22-spark-master  Creating
 Container p22-spark-master  Created
 Container p22-spark-worker  Creating
 Container p22-spark-worker  Created
 Container p22-minio  Starting
 Container p22-kafka  Starting
 Container p22-kafka  Started
 Container p22-minio  Started
 Container p22-minio  Waiting
 Container p22-minio  Healthy
 Container p22-mc-init  Starting
 Container p22-mc-init  Started
 Container p22-kafka  Waiting
 Container p22-mc-init  Waiting
 Container p22-mc-init  Exited
 Container p22-kafka  Healthy
 Container p22-spark-master  Starting
 Container p22-spark-master  Started
 Container p22-spark-master  Waiting
 Container p22-spark-master  Healthy
 Container p22-spark-worker  Starting
 Container p22-spark-worker  Started
Waiting for services...
C:/Users/ghadf/scoop/apps/make/current/bin/make.exe topics
make[2]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips \
	--partitions 6 \
	--replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
Topic taxi.raw_trips created.
make[2]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

=== Pipeline 22: Kafka + Spark + Apache Hudi ===
Kafka:          localhost:9092
Spark UI:       http://localhost:8080
MinIO Console:  http://localhost:9001 (minioadmin/minioadmin)
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm \
	-e MODE=burst \
	-e MAX_EVENTS=10000 \
	data-generator
 Container p22-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.66s
  Rate:    15,078 events/sec
============================================================
  Metrics written to /tmp/generator_metrics.json
Limited data generation complete (10k events).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/bronze_ingest.py
26/02/22 12:57:02 INFO SparkContext: Running Spark version 3.3.3
26/02/22 12:57:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/22 12:57:02 INFO ResourceUtils: ==============================================================
26/02/22 12:57:02 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/22 12:57:02 INFO ResourceUtils: ==============================================================
26/02/22 12:57:02 INFO SparkContext: Submitted application: P22-Bronze-Ingest-Hudi
26/02/22 12:57:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/22 12:57:02 INFO ResourceProfile: Limiting resource is cpu
26/02/22 12:57:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/22 12:57:02 INFO SecurityManager: Changing view acls to: spark
26/02/22 12:57:02 INFO SecurityManager: Changing modify acls to: spark
26/02/22 12:57:02 INFO SecurityManager: Changing view acls groups to: 
26/02/22 12:57:02 INFO SecurityManager: Changing modify acls groups to: 
26/02/22 12:57:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/22 12:57:02 INFO Utils: Successfully started service 'sparkDriver' on port 36831.
26/02/22 12:57:02 INFO SparkEnv: Registering MapOutputTracker
26/02/22 12:57:02 INFO SparkEnv: Registering BlockManagerMaster
26/02/22 12:57:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/22 12:57:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/22 12:57:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/22 12:57:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-27a2006c-fe11-430e-993a-639af08d5e55
26/02/22 12:57:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/22 12:57:02 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/22 12:57:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/22 12:57:03 INFO Executor: Starting executor ID driver on host spark-master
26/02/22 12:57:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/22 12:57:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40479.
26/02/22 12:57:03 INFO NettyBlockTransferService: Server created on spark-master:40479
26/02/22 12:57:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/22 12:57:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 40479, None)
26/02/22 12:57:03 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:40479 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 40479, None)
26/02/22 12:57:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 40479, None)
26/02/22 12:57:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 40479, None)
26/02/22 12:57:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/22 12:57:03 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/22 12:57:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/22 12:57:05 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:57:05 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/22 12:57:06 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:06 WARN HoodieStreamingSink: Ignore TableNotFoundException as it is first microbatch.
26/02/22 12:57:06 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/22 12:57:06 INFO ResolveWriteToStream: Checkpoint root s3a://warehouse/checkpoints/bronze resolved to s3a://warehouse/checkpoints/bronze.
26/02/22 12:57:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/22 12:57:06 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
26/02/22 12:57:06 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/metadata using temp file s3a://warehouse/checkpoints/bronze/.metadata.6401eba9-325c-4a5b-9150-dce4d76f07e4.tmp
26/02/22 12:57:06 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/.metadata.6401eba9-325c-4a5b-9150-dce4d76f07e4.tmp to s3a://warehouse/checkpoints/bronze/metadata
26/02/22 12:57:06 INFO MicroBatchExecution: Starting [id = 4bc4aa1e-58fc-4828-8de9-09d3fa69e188, runId = 45844b57-e571-4d8c-bbc4-a5968355343b]. Use s3a://warehouse/checkpoints/bronze to store the query checkpoint.
26/02/22 12:57:06 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@44aba2b1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3988adca]
26/02/22 12:57:06 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:57:06 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:57:06 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:57:06 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:57:06 INFO AppInfoParser: Kafka startTimeMs: 1771765026915
26/02/22 12:57:06 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Subscribed to topic(s): taxi.raw_trips
26/02/22 12:57:07 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] (Re-)joining group
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:57:07 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] (Re-)joining group
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:57:07 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] (Re-)joining group
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:57:07 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] (Re-)joining group
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
26/02/22 12:57:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
26/02/22 12:57:08 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Client requested disconnect from node 2147483646
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Group coordinator kafka:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Requesting disconnect from last known coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] (Re-)joining group
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Request joining group due to: need to re-join with the given member-id: consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1-fccbcdd2-2aed-4d0e-8b7d-0c9fe963b60d
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] (Re-)joining group
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1-fccbcdd2-2aed-4d0e-8b7d-0c9fe963b60d', protocol='range'}
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1-fccbcdd2-2aed-4d0e-8b7d-0c9fe963b60d=Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])}
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1-fccbcdd2-2aed-4d0e-8b7d-0c9fe963b60d', protocol='range'}
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Notifying assignor about the new Assignment(partitions=[taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5])
26/02/22 12:57:08 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Adding newly assigned partitions: taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-0
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-1
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-2
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-3
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-4
26/02/22 12:57:08 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Found no committed offset for partition taxi.raw_trips-5
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/sources/0/0 using temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.2c2d0687-c8c6-4768-b8f9-69089c95e64a.tmp
26/02/22 12:57:08 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/sources/0/.0.2c2d0687-c8c6-4768-b8f9-69089c95e64a.tmp to s3a://warehouse/checkpoints/bronze/sources/0/0
26/02/22 12:57:08 INFO KafkaMicroBatchStream: Initial offsets: {"taxi.raw_trips":{"2":0,"5":0,"4":0,"1":0,"3":0,"0":0}}
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:08 INFO MicroBatchExecution: Starting new streaming query.
26/02/22 12:57:08 INFO MicroBatchExecution: Stream started from {}
26/02/22 12:57:08 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/offsets/0 using temp file s3a://warehouse/checkpoints/bronze/offsets/.0.f07b8fae-a6ca-45bb-817a-9898940faf10.tmp
26/02/22 12:57:08 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/offsets/.0.f07b8fae-a6ca-45bb-817a-9898940faf10.tmp to s3a://warehouse/checkpoints/bronze/offsets/0
26/02/22 12:57:08 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1771765028340,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
26/02/22 12:57:08 INFO IncrementalExecution: Current batch timestamp = 1771765028340
26/02/22 12:57:08 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:57:08 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:57:08 INFO IncrementalExecution: Current batch timestamp = 1771765028340
26/02/22 12:57:08 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:57:08 INFO KafkaOffsetReaderConsumer: Partitions added: Map()
26/02/22 12:57:08 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/22 12:57:08 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/22 12:57:08 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips as hoodie table s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:09 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark
26/02/22 12:57:09 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:09 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:09 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:57:09 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:09 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:09 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:09 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222125709098 action: commit
26/02/22 12:57:09 INFO HoodieActiveTimeline: Creating a new instant [==>20260222125709098__commit__REQUESTED]
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125709098__commit__REQUESTED__20260222125709638]}
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:09 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/22 12:57:09 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125709098__commit__REQUESTED__20260222125709638]}
26/02/22 12:57:09 INFO HoodieTableMetaClient: Initializing s3a://warehouse/bronze/raw_trips/.hoodie/metadata as hoodie table s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:09 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:09 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:09 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:09 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:09 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:09 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
26/02/22 12:57:09 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:10 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.5 KiB, free 434.3 MiB)
26/02/22 12:57:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/22 12:57:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:40479 (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/22 12:57:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4401 bytes) taskResourceAssignments Map()
26/02/22 12:57:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/22 12:57:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 926 bytes result sent to driver
26/02/22 12:57:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 124 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/22 12:57:10 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 0.419 s
26/02/22 12:57:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/22 12:57:10 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 0.449375 s
26/02/22 12:57:10 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:10 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/22 12:57:10 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/22 12:57:10 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:10 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:10 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
26/02/22 12:57:10 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:10 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:10 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KiB, free 434.3 MiB)
26/02/22 12:57:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.3 MiB)
26/02/22 12:57:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:40479 (size: 1866.0 B, free: 434.4 MiB)
26/02/22 12:57:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/22 12:57:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:57:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/22 12:57:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 803 bytes result sent to driver
26/02/22 12:57:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 16 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/22 12:57:10 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 0.029 s
26/02/22 12:57:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/22 12:57:10 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 0.033315 s
26/02/22 12:57:10 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/22 12:57:10 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/22 12:57:10 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:10 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:10 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
26/02/22 12:57:10 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:10 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:10 INFO DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 393.8 KiB, free 433.9 MiB)
26/02/22 12:57:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 129.8 KiB, free 433.7 MiB)
26/02/22 12:57:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:40479 (size: 129.8 KiB, free: 434.2 MiB)
26/02/22 12:57:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/22 12:57:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/22 12:57:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
26/02/22 12:57:10 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:57:10 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:57:10 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/22 12:57:10 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/22 12:57:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 803 bytes result sent to driver
26/02/22 12:57:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 74 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/22 12:57:10 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 0.107 s
26/02/22 12:57:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
26/02/22 12:57:10 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 0.111127 s
26/02/22 12:57:10 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:10 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:10 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:57:10 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:57:10 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:10 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:10 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:10 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:10 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:10 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:10 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:40479 in memory (size: 129.8 KiB, free: 434.4 MiB)
26/02/22 12:57:10 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:40479 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:10 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:10 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:40479 in memory (size: 1866.0 B, free: 434.4 MiB)
26/02/22 12:57:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:10 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:10 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:10 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:10 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/22 12:57:10 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:10 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:10 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260222125710800]}
26/02/22 12:57:10 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:10 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:10 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:10 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:57:10 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:57:10 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/22 12:57:10 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:10 INFO DAGScheduler: Registering RDD 14 (start at <unknown>:0) as input to shuffle 0
26/02/22 12:57:10 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:10 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
26/02/22 12:57:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/22 12:57:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
26/02/22 12:57:10 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.5 KiB, free 434.4 MiB)
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 434.4 MiB)
26/02/22 12:57:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:40479 (size: 4.7 KiB, free: 434.4 MiB)
26/02/22 12:57:11 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:11 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/22 12:57:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/22 12:57:11 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
26/02/22 12:57:11 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 964 bytes result sent to driver
26/02/22 12:57:11 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 43 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:11 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/22 12:57:11 INFO DAGScheduler: ShuffleMapStage 3 (start at <unknown>:0) finished in 0.168 s
26/02/22 12:57:11 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:11 INFO DAGScheduler: running: Set()
26/02/22 12:57:11 INFO DAGScheduler: waiting: Set(ResultStage 4)
26/02/22 12:57:11 INFO DAGScheduler: failed: Set()
26/02/22 12:57:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.3 KiB, free 434.4 MiB)
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.4 MiB)
26/02/22 12:57:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:40479 (size: 3.9 KiB, free: 434.4 MiB)
26/02/22 12:57:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/22 12:57:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
26/02/22 12:57:11 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
26/02/22 12:57:11 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1246 bytes result sent to driver
26/02/22 12:57:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 80 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/22 12:57:11 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.090 s
26/02/22 12:57:11 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/22 12:57:11 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.186702 s
26/02/22 12:57:11 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:57:11 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/22 12:57:11 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:11 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:11 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
26/02/22 12:57:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
26/02/22 12:57:11 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:11 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 267.1 KiB, free 434.1 MiB)
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 434.0 MiB)
26/02/22 12:57:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:40479 (size: 91.4 KiB, free: 434.3 MiB)
26/02/22 12:57:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:11 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/22 12:57:11 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:11 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/22 12:57:11 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:11 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:57:11 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE
26/02/22 12:57:11 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE in 63 ms
26/02/22 12:57:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/22 12:57:11 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:57:11 INFO MetricsSystemImpl: HBase metrics system started
26/02/22 12:57:11 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/22 12:57:11 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:11 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:11 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/22 12:57:11 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/22 12:57:11 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:40479 in memory (size: 3.9 KiB, free: 434.3 MiB)
26/02/22 12:57:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:40479 in memory (size: 4.7 KiB, free: 434.3 MiB)
26/02/22 12:57:11 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 488 ms.
26/02/22 12:57:11 INFO MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 271.0 B, free 434.0 MiB)
26/02/22 12:57:11 INFO BlockManagerInfo: Added rdd_19_0 in memory on spark-master:40479 (size: 271.0 B, free: 434.3 MiB)
26/02/22 12:57:11 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 1481 bytes result sent to driver
26/02/22 12:57:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 544 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:11 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/22 12:57:11 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.570 s
26/02/22 12:57:11 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/22 12:57:11 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.575550 s
26/02/22 12:57:11 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:57:11 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/22 12:57:11 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:11 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:11 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
26/02/22 12:57:11 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:11 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:11 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 101.2 KiB, free 434.0 MiB)
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.9 MiB)
26/02/22 12:57:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:40479 (size: 36.1 KiB, free: 434.3 MiB)
26/02/22 12:57:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:11 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/22 12:57:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:57:11 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/22 12:57:11 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 854 bytes result sent to driver
26/02/22 12:57:11 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 17 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:11 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/22 12:57:11 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.027 s
26/02/22 12:57:11 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/22 12:57:11 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.030390 s
26/02/22 12:57:11 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:57:11 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/22 12:57:11 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:57:11 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/22 12:57:11 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:11 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:11 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
26/02/22 12:57:11 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:11 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:11 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 101.3 KiB, free 433.8 MiB)
26/02/22 12:57:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.8 MiB)
26/02/22 12:57:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:40479 (size: 36.2 KiB, free: 434.2 MiB)
26/02/22 12:57:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/22 12:57:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:57:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/22 12:57:12 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 900 bytes result sent to driver
26/02/22 12:57:12 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 47 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:12 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/22 12:57:12 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.058 s
26/02/22 12:57:12 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/22 12:57:12 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.060375 s
26/02/22 12:57:12 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: MDT s3a://warehouse/bronze/raw_trips partition FILES has been enabled
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:12 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 1683 in ms
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/22 12:57:12 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:57:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:12 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:57:12 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125709098__commit__REQUESTED__20260222125709638]}
26/02/22 12:57:12 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/22 12:57:12 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125709098__commit__REQUESTED__20260222125709638]}
26/02/22 12:57:12 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:57:12 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:12 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:12 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:12 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:12 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:12 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:12 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:57:12 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:57:12 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:12 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:12 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:12 INFO DAGScheduler: Registering RDD 26 (start at <unknown>:0) as input to shuffle 1
26/02/22 12:57:12 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 6 output partitions
26/02/22 12:57:12 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
26/02/22 12:57:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/22 12:57:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
26/02/22 12:57:12 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 90.9 KiB, free 433.7 MiB)
26/02/22 12:57:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 433.7 MiB)
26/02/22 12:57:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:40479 (size: 31.0 KiB, free: 434.2 MiB)
26/02/22 12:57:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:12 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[26] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:57:12 INFO TaskSchedulerImpl: Adding task set 9.0 with 6 tasks resource profile 0
26/02/22 12:57:12 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:12 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 9) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:12 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 10) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:12 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 11) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:12 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 12) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:12 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 13) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:12 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
26/02/22 12:57:12 INFO Executor: Running task 1.0 in stage 9.0 (TID 9)
26/02/22 12:57:12 INFO Executor: Running task 2.0 in stage 9.0 (TID 10)
26/02/22 12:57:12 INFO Executor: Running task 3.0 in stage 9.0 (TID 11)
26/02/22 12:57:12 INFO Executor: Running task 4.0 in stage 9.0 (TID 12)
26/02/22 12:57:12 INFO Executor: Running task 5.0 in stage 9.0 (TID 13)
26/02/22 12:57:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:40479 in memory (size: 36.1 KiB, free: 434.2 MiB)
26/02/22 12:57:12 INFO BlockManager: Removing RDD 19
26/02/22 12:57:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:40479 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:57:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:40479 in memory (size: 91.4 KiB, free: 434.4 MiB)
26/02/22 12:57:12 INFO CodeGenerator: Code generated in 152.925093 ms
26/02/22 12:57:12 INFO CodeGenerator: Code generated in 15.698264 ms
26/02/22 12:57:12 INFO CodeGenerator: Code generated in 17.704163 ms
26/02/22 12:57:13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:57:13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:57:13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:57:13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:57:13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:57:13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

26/02/22 12:57:13 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:57:13 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:57:13 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:57:13 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:57:13 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:57:13 INFO KafkaMetricsCollector: initializing Kafka metrics collector
26/02/22 12:57:13 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:57:13 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:57:13 INFO AppInfoParser: Kafka startTimeMs: 1771765033031
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Assigned to partition(s): taxi.raw_trips-1
26/02/22 12:57:13 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:57:13 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:57:13 INFO AppInfoParser: Kafka startTimeMs: 1771765033033
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Assigned to partition(s): taxi.raw_trips-0
26/02/22 12:57:13 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:57:13 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:57:13 INFO AppInfoParser: Kafka startTimeMs: 1771765033033
26/02/22 12:57:13 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Assigned to partition(s): taxi.raw_trips-3
26/02/22 12:57:13 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:57:13 INFO AppInfoParser: Kafka startTimeMs: 1771765033034
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Assigned to partition(s): taxi.raw_trips-2
26/02/22 12:57:13 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:57:13 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:57:13 INFO AppInfoParser: Kafka startTimeMs: 1771765033037
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Assigned to partition(s): taxi.raw_trips-5
26/02/22 12:57:13 INFO AppInfoParser: Kafka version: 3.7.1
26/02/22 12:57:13 INFO AppInfoParser: Kafka commitId: e2494e6ffb89f828
26/02/22 12:57:13 INFO AppInfoParser: Kafka startTimeMs: 1771765033039
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Assigned to partition(s): taxi.raw_trips-4
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-4
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-3
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-0
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-2
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-1
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 0 for partition taxi.raw_trips-5
26/02/22 12:57:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:57:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:57:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:57:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:57:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:57:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Cluster ID: p22-hudi-cdc-storage-001
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-3
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-4
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-2
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-5
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 500 for partition taxi.raw_trips-0
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-0
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-3
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-5
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-2
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1000 for partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-5
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-2
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-3
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-4
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 1500 for partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 2000 for partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO LegacyKafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to offset 2500 for partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to earliest offset of partition taxi.raw_trips-4
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-0
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-0 to position FetchPosition{offset=730, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO MemoryStore: Block rdd_6_3 stored as values in memory (estimated size 49.2 KiB, free 434.2 MiB)
26/02/22 12:57:13 INFO BlockManagerInfo: Added rdd_6_3 in memory on spark-master:40479 (size: 49.2 KiB, free: 434.3 MiB)
26/02/22 12:57:13 INFO Executor: Finished task 3.0 in stage 9.0 (TID 11). 1753 bytes result sent to driver
26/02/22 12:57:13 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 11) in 1418 ms on spark-master (executor driver) (1/6)
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-5
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-2
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-2 to position FetchPosition{offset=1194, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-5 to position FetchPosition{offset=1404, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO MemoryStore: Block rdd_6_2 stored as values in memory (estimated size 79.0 KiB, free 434.2 MiB)
26/02/22 12:57:13 INFO BlockManagerInfo: Added rdd_6_2 in memory on spark-master:40479 (size: 79.0 KiB, free: 434.2 MiB)
26/02/22 12:57:13 INFO MemoryStore: Block rdd_6_1 stored as values in memory (estimated size 94.2 KiB, free 434.1 MiB)
26/02/22 12:57:13 INFO BlockManagerInfo: Added rdd_6_1 in memory on spark-master:40479 (size: 94.2 KiB, free: 434.2 MiB)
26/02/22 12:57:13 INFO Executor: Finished task 2.0 in stage 9.0 (TID 10). 1753 bytes result sent to driver
26/02/22 12:57:13 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 10) in 1442 ms on spark-master (executor driver) (2/6)
26/02/22 12:57:13 INFO Executor: Finished task 1.0 in stage 9.0 (TID 9). 1753 bytes result sent to driver
26/02/22 12:57:13 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 9) in 1444 ms on spark-master (executor driver) (3/6)
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-3
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-3 to position FetchPosition{offset=1946, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-1
26/02/22 12:57:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-1 to position FetchPosition{offset=1893, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:14 INFO MemoryStore: Block rdd_6_5 stored as values in memory (estimated size 132.6 KiB, free 433.9 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Added rdd_6_5 in memory on spark-master:40479 (size: 132.6 KiB, free: 434.0 MiB)
26/02/22 12:57:14 INFO MemoryStore: Block rdd_6_0 stored as values in memory (estimated size 126.8 KiB, free 433.8 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Added rdd_6_0 in memory on spark-master:40479 (size: 126.8 KiB, free: 433.9 MiB)
26/02/22 12:57:14 INFO Executor: Finished task 5.0 in stage 9.0 (TID 13). 1753 bytes result sent to driver
26/02/22 12:57:14 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1753 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 13) in 1484 ms on spark-master (executor driver) (4/6)
26/02/22 12:57:14 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 1486 ms on spark-master (executor driver) (5/6)
26/02/22 12:57:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Seeking to latest offset of partition taxi.raw_trips-4
26/02/22 12:57:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting offset for partition taxi.raw_trips-4 to position FetchPosition{offset=2833, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
26/02/22 12:57:14 INFO MemoryStore: Block rdd_6_4 stored as values in memory (estimated size 191.4 KiB, free 433.6 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Added rdd_6_4 in memory on spark-master:40479 (size: 191.4 KiB, free: 433.7 MiB)
26/02/22 12:57:14 INFO Executor: Finished task 4.0 in stage 9.0 (TID 12). 1753 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 12) in 1519 ms on spark-master (executor driver) (6/6)
26/02/22 12:57:14 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
26/02/22 12:57:14 INFO DAGScheduler: ShuffleMapStage 9 (start at <unknown>:0) finished in 1.565 s
26/02/22 12:57:14 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:14 INFO DAGScheduler: running: Set()
26/02/22 12:57:14 INFO DAGScheduler: waiting: Set(ResultStage 10)
26/02/22 12:57:14 INFO DAGScheduler: failed: Set()
26/02/22 12:57:14 INFO DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.5 KiB, free 433.6 MiB)
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.6 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:40479 (size: 3.1 KiB, free: 433.7 MiB)
26/02/22 12:57:14 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:14 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 10 (ShuffledRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:57:14 INFO TaskSchedulerImpl: Adding task set 10.0 with 6 tasks resource profile 0
26/02/22 12:57:14 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14) (spark-master, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 15) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 16) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 17) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 18) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 19) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO Executor: Running task 1.0 in stage 10.0 (TID 14)
26/02/22 12:57:14 INFO Executor: Running task 0.0 in stage 10.0 (TID 15)
26/02/22 12:57:14 INFO Executor: Running task 2.0 in stage 10.0 (TID 16)
26/02/22 12:57:14 INFO Executor: Running task 4.0 in stage 10.0 (TID 18)
26/02/22 12:57:14 INFO Executor: Running task 3.0 in stage 10.0 (TID 17)
26/02/22 12:57:14 INFO Executor: Running task 5.0 in stage 10.0 (TID 19)
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Getting 6 (528.0 B) non-empty blocks including 6 (528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:14 INFO Executor: Finished task 0.0 in stage 10.0 (TID 15). 1235 bytes result sent to driver
26/02/22 12:57:14 INFO Executor: Finished task 5.0 in stage 10.0 (TID 19). 1235 bytes result sent to driver
26/02/22 12:57:14 INFO Executor: Finished task 2.0 in stage 10.0 (TID 16). 1235 bytes result sent to driver
26/02/22 12:57:14 INFO Executor: Finished task 3.0 in stage 10.0 (TID 17). 1235 bytes result sent to driver
26/02/22 12:57:14 INFO Executor: Finished task 4.0 in stage 10.0 (TID 18). 1235 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 15) in 36 ms on spark-master (executor driver) (1/6)
26/02/22 12:57:14 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 17) in 36 ms on spark-master (executor driver) (2/6)
26/02/22 12:57:14 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 18) in 36 ms on spark-master (executor driver) (3/6)
26/02/22 12:57:14 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 16) in 36 ms on spark-master (executor driver) (4/6)
26/02/22 12:57:14 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 19) in 36 ms on spark-master (executor driver) (5/6)
26/02/22 12:57:14 INFO Executor: Finished task 1.0 in stage 10.0 (TID 14). 1284 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 45 ms on spark-master (executor driver) (6/6)
26/02/22 12:57:14 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/22 12:57:14 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.061 s
26/02/22 12:57:14 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/22 12:57:14 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 1.634593 s
26/02/22 12:57:14 INFO BaseSparkCommitActionExecutor: Source read and index timer 1667
26/02/22 12:57:14 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:57:14 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:14 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:14 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
26/02/22 12:57:14 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:14 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:14 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 269.4 KiB, free 433.4 MiB)
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 93.4 KiB, free 433.3 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:40479 in memory (size: 3.1 KiB, free: 433.7 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:40479 (size: 93.4 KiB, free: 433.6 MiB)
26/02/22 12:57:14 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[29] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:14 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/22 12:57:14 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO Executor: Running task 0.0 in stage 11.0 (TID 20)
26/02/22 12:57:14 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:40479 in memory (size: 31.0 KiB, free: 433.7 MiB)
26/02/22 12:57:14 INFO Executor: Finished task 0.0 in stage 11.0 (TID 20). 790 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 20) in 22 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:14 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/22 12:57:14 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.048 s
26/02/22 12:57:14 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/22 12:57:14 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.050805 s
26/02/22 12:57:14 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:14 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:14 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/22 12:57:14 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 10000, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/22 12:57:14 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/22 12:57:14 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/22 12:57:14 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260222125709098.inflight
26/02/22 12:57:14 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:57:14 INFO BaseCommitActionExecutor: Auto commit disabled for 20260222125709098
26/02/22 12:57:14 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:14 INFO DAGScheduler: Registering RDD 30 (start at <unknown>:0) as input to shuffle 2
26/02/22 12:57:14 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:14 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
26/02/22 12:57:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
26/02/22 12:57:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
26/02/22 12:57:14 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 299.7 KiB, free 433.1 MiB)
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.0 KiB, free 433.0 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:40479 (size: 106.0 KiB, free: 433.5 MiB)
26/02/22 12:57:14 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:14 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
26/02/22 12:57:14 INFO TaskSchedulerImpl: Adding task set 12.0 with 6 tasks resource profile 0
26/02/22 12:57:14 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 21) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 22) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 23) (spark-master, executor driver, partition 2, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 24) (spark-master, executor driver, partition 3, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 25) (spark-master, executor driver, partition 4, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 26) (spark-master, executor driver, partition 5, PROCESS_LOCAL, 5385 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO Executor: Running task 0.0 in stage 12.0 (TID 21)
26/02/22 12:57:14 INFO Executor: Running task 2.0 in stage 12.0 (TID 23)
26/02/22 12:57:14 INFO Executor: Running task 3.0 in stage 12.0 (TID 24)
26/02/22 12:57:14 INFO Executor: Running task 1.0 in stage 12.0 (TID 22)
26/02/22 12:57:14 INFO Executor: Running task 4.0 in stage 12.0 (TID 25)
26/02/22 12:57:14 INFO Executor: Running task 5.0 in stage 12.0 (TID 26)
26/02/22 12:57:14 INFO BlockManager: Found block rdd_6_1 locally
26/02/22 12:57:14 INFO BlockManager: Found block rdd_6_3 locally
26/02/22 12:57:14 INFO BlockManager: Found block rdd_6_5 locally
26/02/22 12:57:14 INFO BlockManager: Found block rdd_6_0 locally
26/02/22 12:57:14 INFO BlockManager: Found block rdd_6_4 locally
26/02/22 12:57:14 INFO BlockManager: Found block rdd_6_2 locally
26/02/22 12:57:14 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:40479 in memory (size: 93.4 KiB, free: 433.6 MiB)
26/02/22 12:57:14 INFO Executor: Finished task 3.0 in stage 12.0 (TID 24). 1662 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 24) in 51 ms on spark-master (executor driver) (1/6)
26/02/22 12:57:14 INFO Executor: Finished task 1.0 in stage 12.0 (TID 22). 1662 bytes result sent to driver
26/02/22 12:57:14 INFO Executor: Finished task 2.0 in stage 12.0 (TID 23). 1662 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 22) in 62 ms on spark-master (executor driver) (2/6)
26/02/22 12:57:14 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 23) in 61 ms on spark-master (executor driver) (3/6)
26/02/22 12:57:14 INFO Executor: Finished task 5.0 in stage 12.0 (TID 26). 1662 bytes result sent to driver
26/02/22 12:57:14 INFO Executor: Finished task 0.0 in stage 12.0 (TID 21). 1662 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 26) in 68 ms on spark-master (executor driver) (4/6)
26/02/22 12:57:14 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 21) in 69 ms on spark-master (executor driver) (5/6)
26/02/22 12:57:14 INFO Executor: Finished task 4.0 in stage 12.0 (TID 25). 1662 bytes result sent to driver
26/02/22 12:57:14 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 25) in 74 ms on spark-master (executor driver) (6/6)
26/02/22 12:57:14 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/22 12:57:14 INFO DAGScheduler: ShuffleMapStage 12 (start at <unknown>:0) finished in 0.090 s
26/02/22 12:57:14 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:14 INFO DAGScheduler: running: Set()
26/02/22 12:57:14 INFO DAGScheduler: waiting: Set(ResultStage 13)
26/02/22 12:57:14 INFO DAGScheduler: failed: Set()
26/02/22 12:57:14 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 302.5 KiB, free 433.1 MiB)
26/02/22 12:57:14 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 107.4 KiB, free 432.9 MiB)
26/02/22 12:57:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:40479 (size: 107.4 KiB, free: 433.5 MiB)
26/02/22 12:57:14 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:14 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/22 12:57:14 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:14 INFO Executor: Running task 0.0 in stage 13.0 (TID 27)
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Getting 6 (837.5 KiB) non-empty blocks including 6 (837.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:14 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:57:14 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222125709098/9aebbd6e-af03-4217-8cdc-41848328a773-0_0-13-27_20260222125709098.parquet.marker.CREATE
26/02/22 12:57:14 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222125709098/9aebbd6e-af03-4217-8cdc-41848328a773-0_0-13-27_20260222125709098.parquet.marker.CREATE in 39 ms
26/02/22 12:57:14 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:14 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId 9aebbd6e-af03-4217-8cdc-41848328a773-0
26/02/22 12:57:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:40479 in memory (size: 106.0 KiB, free: 433.6 MiB)
26/02/22 12:57:15 INFO HoodieCreateHandle: Closing the file 9aebbd6e-af03-4217-8cdc-41848328a773-0 as we are done with all the records 10000
26/02/22 12:57:15 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID 9aebbd6e-af03-4217-8cdc-41848328a773-0, took 1044 ms.
26/02/22 12:57:15 INFO MemoryStore: Block rdd_34_0 stored as values in memory (estimated size 295.0 B, free 433.3 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Added rdd_34_0 in memory on spark-master:40479 (size: 295.0 B, free: 433.6 MiB)
26/02/22 12:57:15 INFO Executor: Finished task 0.0 in stage 13.0 (TID 27). 1716 bytes result sent to driver
26/02/22 12:57:15 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 27) in 1084 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:15 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/22 12:57:15 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 1.100 s
26/02/22 12:57:15 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
26/02/22 12:57:15 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 1.197720 s
26/02/22 12:57:15 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/22 12:57:15 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:15 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:15 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
26/02/22 12:57:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/22 12:57:15 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:15 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 303.2 KiB, free 433.0 MiB)
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 107.8 KiB, free 432.9 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:40479 (size: 107.8 KiB, free: 433.5 MiB)
26/02/22 12:57:15 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[36] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:15 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/22 12:57:15 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:15 INFO Executor: Running task 0.0 in stage 15.0 (TID 28)
26/02/22 12:57:15 INFO BlockManager: Found block rdd_34_0 locally
26/02/22 12:57:15 INFO Executor: Finished task 0.0 in stage 15.0 (TID 28). 1720 bytes result sent to driver
26/02/22 12:57:15 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 28) in 13 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:15 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/22 12:57:15 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.031 s
26/02/22 12:57:15 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/22 12:57:15 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.034259 s
26/02/22 12:57:15 INFO BaseHoodieWriteClient: Committing 20260222125709098 action commit
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125709098__commit__INFLIGHT__20260222125714252]}
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:15 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:15 INFO CommitUtils: Creating  metadata for INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125709098__commit__INFLIGHT__20260222125714252]}
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:15 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:15 INFO BaseHoodieWriteClient: Committing 20260222125709098 action commit
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:15 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:15 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:15 INFO HoodieTableMetadataUtil: Updating at 20260222125709098 from Commit/INSERT. #partitions_updated=2, #files_added=1
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:15 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:15 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:57:15 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:15 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:15 INFO HoodieBackedTableMetadataWriter: New commit at 20260222125709098 being applied to MDT.
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:15 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125711939]}
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:15 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222125709098 action: deltacommit
26/02/22 12:57:15 INFO HoodieActiveTimeline: Creating a new instant [==>20260222125709098__deltacommit__REQUESTED]
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:15 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125709098__deltacommit__REQUESTED__20260222125715808]}
26/02/22 12:57:15 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:15 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:57:15 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:15 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:15 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:15 INFO DAGScheduler: Registering RDD 45 (start at <unknown>:0) as input to shuffle 3
26/02/22 12:57:15 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:15 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
26/02/22 12:57:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/22 12:57:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)
26/02/22 12:57:15 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 10.1 KiB, free 432.9 MiB)
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 432.9 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:40479 (size: 5.4 KiB, free: 433.5 MiB)
26/02/22 12:57:15 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[45] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:15 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/22 12:57:15 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:57:15 INFO Executor: Running task 0.0 in stage 16.0 (TID 29)
26/02/22 12:57:15 INFO MemoryStore: Block rdd_43_0 stored as values in memory (estimated size 342.0 B, free 432.9 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Added rdd_43_0 in memory on spark-master:40479 (size: 342.0 B, free: 433.5 MiB)
26/02/22 12:57:15 INFO Executor: Finished task 0.0 in stage 16.0 (TID 29). 1093 bytes result sent to driver
26/02/22 12:57:15 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 29) in 17 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:15 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/22 12:57:15 INFO DAGScheduler: ShuffleMapStage 16 (start at <unknown>:0) finished in 0.024 s
26/02/22 12:57:15 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:15 INFO DAGScheduler: running: Set()
26/02/22 12:57:15 INFO DAGScheduler: waiting: Set(ResultStage 17)
26/02/22 12:57:15 INFO DAGScheduler: failed: Set()
26/02/22 12:57:15 INFO DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KiB, free 432.9 MiB)
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.9 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:40479 (size: 3.1 KiB, free: 433.5 MiB)
26/02/22 12:57:15 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (ShuffledRDD[46] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:15 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
26/02/22 12:57:15 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 30) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:15 INFO Executor: Running task 0.0 in stage 17.0 (TID 30)
26/02/22 12:57:15 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:15 INFO Executor: Finished task 0.0 in stage 17.0 (TID 30). 1366 bytes result sent to driver
26/02/22 12:57:15 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 30) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:15 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/22 12:57:15 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.014 s
26/02/22 12:57:15 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/22 12:57:15 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.044420 s
26/02/22 12:57:15 INFO BaseSparkCommitActionExecutor: Source read and index timer 55
26/02/22 12:57:15 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:57:15 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:40479 in memory (size: 107.8 KiB, free: 433.6 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:40479 in memory (size: 3.1 KiB, free: 433.6 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:40479 in memory (size: 5.4 KiB, free: 433.6 MiB)
26/02/22 12:57:15 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:15 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:15 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
26/02/22 12:57:15 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:15 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:15 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 262.2 KiB, free 433.1 MiB)
26/02/22 12:57:15 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 433.0 MiB)
26/02/22 12:57:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:40479 (size: 90.5 KiB, free: 433.5 MiB)
26/02/22 12:57:15 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[48] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:15 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/22 12:57:15 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/22 12:57:15 INFO Executor: Running task 0.0 in stage 18.0 (TID 31)
26/02/22 12:57:15 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:15 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:15 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:15 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:15 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:57:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:40479 in memory (size: 107.4 KiB, free: 433.7 MiB)
26/02/22 12:57:16 INFO Executor: Finished task 0.0 in stage 18.0 (TID 31). 837 bytes result sent to driver
26/02/22 12:57:16 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 31) in 36 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:16 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/22 12:57:16 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.056 s
26/02/22 12:57:16 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/22 12:57:16 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.059886 s
26/02/22 12:57:16 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:16 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:16 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/22 12:57:16 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260222125709098.deltacommit.inflight
26/02/22 12:57:16 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:57:16 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260222125709098
26/02/22 12:57:16 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:16 INFO DAGScheduler: Registering RDD 49 (start at <unknown>:0) as input to shuffle 4
26/02/22 12:57:16 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:16 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
26/02/22 12:57:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/22 12:57:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
26/02/22 12:57:16 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 266.9 KiB, free 433.1 MiB)
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 92.9 KiB, free 433.0 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:40479 (size: 92.9 KiB, free: 433.6 MiB)
26/02/22 12:57:16 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[49] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:16 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/22 12:57:16 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 32) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:57:16 INFO Executor: Running task 0.0 in stage 19.0 (TID 32)
26/02/22 12:57:16 INFO BlockManager: Found block rdd_43_0 locally
26/02/22 12:57:16 INFO Executor: Finished task 0.0 in stage 19.0 (TID 32). 1050 bytes result sent to driver
26/02/22 12:57:16 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 32) in 19 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:16 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/22 12:57:16 INFO DAGScheduler: ShuffleMapStage 19 (start at <unknown>:0) finished in 0.032 s
26/02/22 12:57:16 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:16 INFO DAGScheduler: running: Set()
26/02/22 12:57:16 INFO DAGScheduler: waiting: Set(ResultStage 20)
26/02/22 12:57:16 INFO DAGScheduler: failed: Set()
26/02/22 12:57:16 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 372.6 KiB, free 432.7 MiB)
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.6 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:40479 (size: 132.2 KiB, free: 433.4 MiB)
26/02/22 12:57:16 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:16 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/22 12:57:16 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 33) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:16 INFO Executor: Running task 0.0 in stage 20.0 (TID 33)
26/02/22 12:57:16 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:16 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260222125709098 for file files-0000-0
26/02/22 12:57:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:16 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips/.hoodie/metadata.
26/02/22 12:57:16 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:16 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:16 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/22 12:57:16 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:57:16 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:57:16 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222125709098/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND
26/02/22 12:57:16 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222125709098/files/.files-0000-0_00000000000000010.log.2_0-20-33.marker.APPEND in 40 ms
26/02/22 12:57:16 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/bronze/raw_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-20-33', fileLen=-1}
26/02/22 12:57:16 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:16 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:16 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-20-33, took 471 ms.
26/02/22 12:57:16 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 342.0 B, free 432.6 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:40479 (size: 342.0 B, free: 433.4 MiB)
26/02/22 12:57:16 INFO Executor: Finished task 0.0 in stage 20.0 (TID 33). 1626 bytes result sent to driver
26/02/22 12:57:16 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 33) in 521 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:16 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/22 12:57:16 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.538 s
26/02/22 12:57:16 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/22 12:57:16 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.577276 s
26/02/22 12:57:16 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/22 12:57:16 INFO BaseSparkCommitActionExecutor: Committing 20260222125709098, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/22 12:57:16 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:16 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:16 INFO DAGScheduler: Final stage: ResultStage 21 (start at <unknown>:0)
26/02/22 12:57:16 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:16 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:16 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:16 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:40479 in memory (size: 132.2 KiB, free: 433.6 MiB)
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 101.2 KiB, free 432.9 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:40479 in memory (size: 92.9 KiB, free: 433.7 MiB)
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.3 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:40479 (size: 36.1 KiB, free: 433.6 MiB)
26/02/22 12:57:16 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[56] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:16 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/22 12:57:16 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:57:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:40479 in memory (size: 90.5 KiB, free: 433.7 MiB)
26/02/22 12:57:16 INFO Executor: Running task 0.0 in stage 21.0 (TID 34)
26/02/22 12:57:16 INFO Executor: Finished task 0.0 in stage 21.0 (TID 34). 804 bytes result sent to driver
26/02/22 12:57:16 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 34) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:16 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/22 12:57:16 INFO DAGScheduler: ResultStage 21 (start at <unknown>:0) finished in 0.025 s
26/02/22 12:57:16 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/22 12:57:16 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.027831 s
26/02/22 12:57:16 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:40479 in memory (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:57:16 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:16 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:16 INFO DAGScheduler: Final stage: ResultStage 22 (start at <unknown>:0)
26/02/22 12:57:16 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:16 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:16 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 101.1 KiB, free 433.6 MiB)
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.6 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:40479 (size: 36.1 KiB, free: 433.7 MiB)
26/02/22 12:57:16 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:16 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/22 12:57:16 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 35) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:57:16 INFO Executor: Running task 0.0 in stage 22.0 (TID 35)
26/02/22 12:57:16 INFO Executor: Finished task 0.0 in stage 22.0 (TID 35). 857 bytes result sent to driver
26/02/22 12:57:16 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 35) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:16 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/22 12:57:16 INFO DAGScheduler: ResultStage 22 (start at <unknown>:0) finished in 0.025 s
26/02/22 12:57:16 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/22 12:57:16 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.044625 s
26/02/22 12:57:16 INFO HoodieActiveTimeline: Marking instant complete [==>20260222125709098__deltacommit__INFLIGHT]
26/02/22 12:57:16 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/20260222125709098.deltacommit
26/02/22 12:57:16 INFO HoodieActiveTimeline: Completed [==>20260222125709098__deltacommit__INFLIGHT]
26/02/22 12:57:16 INFO BaseSparkCommitActionExecutor: Committed 20260222125709098
26/02/22 12:57:16 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:16 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:16 INFO DAGScheduler: Final stage: ResultStage 23 (start at <unknown>:0)
26/02/22 12:57:16 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:16 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:16 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 101.3 KiB, free 433.5 MiB)
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:40479 (size: 36.2 KiB, free: 433.7 MiB)
26/02/22 12:57:16 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[60] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:16 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
26/02/22 12:57:16 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 36) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4421 bytes) taskResourceAssignments Map()
26/02/22 12:57:16 INFO Executor: Running task 0.0 in stage 23.0 (TID 36)
26/02/22 12:57:16 INFO Executor: Finished task 0.0 in stage 23.0 (TID 36). 900 bytes result sent to driver
26/02/22 12:57:16 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 36) in 28 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:16 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/22 12:57:16 INFO DAGScheduler: ResultStage 23 (start at <unknown>:0) finished in 0.036 s
26/02/22 12:57:16 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/22 12:57:16 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 0.037748 s
26/02/22 12:57:16 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/.temp/20260222125709098
26/02/22 12:57:16 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:16 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:16 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__deltacommit__COMPLETED__20260222125716811]}
26/02/22 12:57:16 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:16 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:16 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:16 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__deltacommit__COMPLETED__20260222125716811]}
26/02/22 12:57:16 INFO HoodieActiveTimeline: Marking instant complete [==>20260222125709098__commit__INFLIGHT]
26/02/22 12:57:16 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/bronze/raw_trips/.hoodie/20260222125709098.commit
26/02/22 12:57:16 INFO HoodieActiveTimeline: Completed [==>20260222125709098__commit__INFLIGHT]
26/02/22 12:57:16 INFO SparkContext: Starting job: start at <unknown>:0
26/02/22 12:57:16 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
26/02/22 12:57:16 INFO DAGScheduler: Final stage: ResultStage 24 (start at <unknown>:0)
26/02/22 12:57:16 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:16 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:16 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0), which has no missing parents
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 101.3 KiB, free 433.4 MiB)
26/02/22 12:57:16 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.3 MiB)
26/02/22 12:57:16 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:40479 (size: 36.2 KiB, free: 433.6 MiB)
26/02/22 12:57:16 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[62] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:16 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
26/02/22 12:57:16 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 37) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:57:16 INFO Executor: Running task 0.0 in stage 24.0 (TID 37)
26/02/22 12:57:16 INFO Executor: Finished task 0.0 in stage 24.0 (TID 37). 964 bytes result sent to driver
26/02/22 12:57:16 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 37) in 21 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:16 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/22 12:57:16 INFO DAGScheduler: ResultStage 24 (start at <unknown>:0) finished in 0.028 s
26/02/22 12:57:16 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/22 12:57:16 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.029942 s
26/02/22 12:57:17 INFO FSUtils: Removed directory at s3a://warehouse/bronze/raw_trips/.hoodie/.temp/20260222125709098
26/02/22 12:57:17 INFO BaseHoodieWriteClient: Committed 20260222125709098
26/02/22 12:57:17 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
26/02/22 12:57:17 INFO MapPartitionsRDD: Removing RDD 34 from persistence list
26/02/22 12:57:17 INFO BlockManager: Removing RDD 6
26/02/22 12:57:17 INFO UnionRDD: Removing RDD 43 from persistence list
26/02/22 12:57:17 INFO BlockManager: Removing RDD 34
26/02/22 12:57:17 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
26/02/22 12:57:17 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/22 12:57:17 INFO BlockManager: Removing RDD 43
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO BlockManager: Removing RDD 53
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__deltacommit__COMPLETED__20260222125716811]}
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:17 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:17 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__deltacommit__COMPLETED__20260222125716811]}
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:17 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:17 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260222125717008
26/02/22 12:57:17 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:57:17 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__deltacommit__COMPLETED__20260222125716811]}
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:17 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:17 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/bronze/raw_trips/.hoodie/metadata
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__deltacommit__COMPLETED__20260222125716811]}
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/22 12:57:17 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:57:17 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/bronze/raw_trips.
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__deltacommit__COMPLETED__20260222125716811]}
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Commit 20260222125709098 successful!
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:57:17 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:57:17 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/22 12:57:17 INFO HoodieStreamingSink: Micro batch id=0 succeeded for commit=20260222125709098
26/02/22 12:57:17 INFO HoodieStreamingSink: Current value of latestCommittedBatchId: -1. Setting latestCommittedBatchId to batchId 0.
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieStreamingSink: Micro batch id=0 succeeded
26/02/22 12:57:17 INFO CheckpointFileManager: Writing atomically to s3a://warehouse/checkpoints/bronze/commits/0 using temp file s3a://warehouse/checkpoints/bronze/commits/.0.9ec9652d-32dc-4d01-9686-54cf93cf373d.tmp
26/02/22 12:57:17 INFO CheckpointFileManager: Renamed temp file s3a://warehouse/checkpoints/bronze/commits/.0.9ec9652d-32dc-4d01-9686-54cf93cf373d.tmp to s3a://warehouse/checkpoints/bronze/commits/0
26/02/22 12:57:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4bc4aa1e-58fc-4828-8de9-09d3fa69e188",
  "runId" : "45844b57-e571-4d8c-bbc4-a5968355343b",
  "name" : null,
  "timestamp" : "2026-02-22T12:57:08.275Z",
  "batchId" : 0,
  "numInputRows" : 10000,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 1117.443289753045,
  "durationMs" : {
    "addBatch" : 8399,
    "getBatch" : 16,
    "latestOffset" : 33,
    "queryPlanning" : 320,
    "triggerExecution" : 8948,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[taxi.raw_trips]]",
    "startOffset" : null,
    "endOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "latestOffset" : {
      "taxi.raw_trips" : {
        "2" : 1194,
        "5" : 1404,
        "4" : 2833,
        "1" : 1893,
        "3" : 1946,
        "0" : 730
      }
    },
    "numInputRows" : 10000,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 1117.443289753045,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "HoodieStreamingSink[s3a://warehouse/bronze/raw_trips]",
    "numOutputRows" : -1
  }
}
26/02/22 12:57:17 INFO MicroBatchExecution: Finished processing all available data for the trigger, terminating this Trigger.AvailableNow query
26/02/22 12:57:17 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Revoke previously assigned partitions taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5
26/02/22 12:57:17 INFO ConsumerRebalanceListenerInvoker: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] The pause flag in partitions [taxi.raw_trips-0, taxi.raw_trips-1, taxi.raw_trips-2, taxi.raw_trips-3, taxi.raw_trips-4, taxi.raw_trips-5] will be removed due to revocation.
26/02/22 12:57:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Member consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1-fccbcdd2-2aed-4d0e-8b7d-0c9fe963b60d sending LeaveGroup request to coordinator kafka:9092 (id: 2147483646 rack: null) due to the consumer is being closed
26/02/22 12:57:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:57:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:57:17 INFO Metrics: Metrics scheduler closed
26/02/22 12:57:17 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:57:17 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:57:17 INFO Metrics: Metrics reporters closed
26/02/22 12:57:17 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-driver-0-1 unregistered
26/02/22 12:57:17 INFO DataSourceUtils: Getting table path..
26/02/22 12:57:17 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:17 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:17 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:17 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:17 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 9 ms
26/02/22 12:57:17 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:40479 in memory (size: 36.1 KiB, free: 434.3 MiB)
26/02/22 12:57:17 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:40479 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:17 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:40479 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:17 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:57:17 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:57:17 INFO DAGScheduler: Got job 18 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:57:17 INFO DAGScheduler: Final stage: ResultStage 25 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:57:17 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:17 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:17 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/22 12:57:17 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:40479 (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:17 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[64] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:17 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
26/02/22 12:57:17 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 38) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/22 12:57:17 INFO Executor: Running task 0.0 in stage 25.0 (TID 38)
26/02/22 12:57:17 INFO Executor: Finished task 0.0 in stage 25.0 (TID 38). 1123 bytes result sent to driver
26/02/22 12:57:17 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 38) in 12 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:17 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
26/02/22 12:57:17 INFO DAGScheduler: ResultStage 25 (collect at HoodieSparkEngineContext.java:116) finished in 0.020 s
26/02/22 12:57:17 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
26/02/22 12:57:17 INFO DAGScheduler: Job 18 finished: collect at HoodieSparkEngineContext.java:116, took 0.022547 s
26/02/22 12:57:17 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:17 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:17 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:57:17 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:57:17 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:57:17 INFO CodeGenerator: Code generated in 12.972046 ms
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.0 MiB)
26/02/22 12:57:17 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:40479 (size: 35.2 KiB, free: 434.3 MiB)
26/02/22 12:57:17 INFO SparkContext: Created broadcast 24 from count at <unknown>:0
26/02/22 12:57:17 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:57:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:57:17 INFO DAGScheduler: Registering RDD 68 (count at <unknown>:0) as input to shuffle 5
26/02/22 12:57:17 INFO DAGScheduler: Got map stage job 19 (count at <unknown>:0) with 1 output partitions
26/02/22 12:57:17 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (count at <unknown>:0)
26/02/22 12:57:17 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:17 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:17 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0), which has no missing parents
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.0 MiB)
26/02/22 12:57:17 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:40479 (size: 7.4 KiB, free: 434.3 MiB)
26/02/22 12:57:17 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[68] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:17 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/22 12:57:17 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 39) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:57:17 INFO Executor: Running task 0.0 in stage 26.0 (TID 39)
26/02/22 12:57:17 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/9aebbd6e-af03-4217-8cdc-41848328a773-0_0-13-27_20260222125709098.parquet, range: 0-681092, partition values: [empty row]
26/02/22 12:57:17 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:17 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:17 INFO Executor: Finished task 0.0 in stage 26.0 (TID 39). 2056 bytes result sent to driver
26/02/22 12:57:17 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 39) in 131 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:17 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/22 12:57:17 INFO DAGScheduler: ShuffleMapStage 26 (count at <unknown>:0) finished in 0.153 s
26/02/22 12:57:17 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:17 INFO DAGScheduler: running: Set()
26/02/22 12:57:17 INFO DAGScheduler: waiting: Set()
26/02/22 12:57:17 INFO DAGScheduler: failed: Set()
26/02/22 12:57:17 INFO CodeGenerator: Code generated in 7.039849 ms
26/02/22 12:57:17 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:57:17 INFO DAGScheduler: Got job 20 (count at <unknown>:0) with 1 output partitions
26/02/22 12:57:17 INFO DAGScheduler: Final stage: ResultStage 28 (count at <unknown>:0)
26/02/22 12:57:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
26/02/22 12:57:17 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:17 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0), which has no missing parents
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.0 MiB)
26/02/22 12:57:17 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.0 MiB)
26/02/22 12:57:17 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:40479 (size: 5.5 KiB, free: 434.3 MiB)
26/02/22 12:57:17 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[71] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:17 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
26/02/22 12:57:17 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 40) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:57:17 INFO Executor: Running task 0.0 in stage 28.0 (TID 40)
26/02/22 12:57:17 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:17 INFO Executor: Finished task 0.0 in stage 28.0 (TID 40). 2458 bytes result sent to driver
26/02/22 12:57:17 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 40) in 17 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:17 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
26/02/22 12:57:17 INFO DAGScheduler: ResultStage 28 (count at <unknown>:0) finished in 0.022 s
26/02/22 12:57:17 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
26/02/22 12:57:17 INFO DAGScheduler: Job 20 finished: count at <unknown>:0, took 0.024341 s
Bronze ingest complete. Total rows in Hudi table: 10,000
============================================================
  Bronze Ingest (Hudi COW) COMPLETE
============================================================
26/02/22 12:57:17 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/22 12:57:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/22 12:57:17 INFO MemoryStore: MemoryStore cleared
26/02/22 12:57:17 INFO BlockManager: BlockManager stopped
26/02/22 12:57:18 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/22 12:57:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/22 12:57:18 INFO SparkContext: Successfully stopped SparkContext
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO Metrics: Metrics scheduler closed
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:57:18 INFO Metrics: Metrics reporters closed
26/02/22 12:57:18 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-2 unregistered
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO Metrics: Metrics scheduler closed
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:57:18 INFO Metrics: Metrics reporters closed
26/02/22 12:57:18 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-5 unregistered
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO Metrics: Metrics scheduler closed
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:57:18 INFO Metrics: Metrics reporters closed
26/02/22 12:57:18 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-6 unregistered
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO Metrics: Metrics scheduler closed
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:57:18 INFO Metrics: Metrics reporters closed
26/02/22 12:57:18 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-7 unregistered
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO Metrics: Metrics scheduler closed
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:57:18 INFO Metrics: Metrics reporters closed
26/02/22 12:57:18 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-3 unregistered
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4, groupId=spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor] Request joining group due to: consumer pro-actively leaving the group
26/02/22 12:57:18 INFO Metrics: Metrics scheduler closed
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/22 12:57:18 INFO Metrics: Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
26/02/22 12:57:18 INFO Metrics: Metrics reporters closed
26/02/22 12:57:18 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ce05fa50-1d42-44ab-a308-5b89683da39b--1125050931-executor-4 unregistered
26/02/22 12:57:18 INFO ShutdownHookManager: Shutdown hook called
26/02/22 12:57:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3eb0a865-25ac-4a5f-ad7d-98ba28fa31de/pyspark-11af023e-0792-4bf9-9449-8f9e17045840
26/02/22 12:57:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3eb0a865-25ac-4a5f-ad7d-98ba28fa31de
26/02/22 12:57:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-972a05c2-4e6d-4acf-bbcf-e090865ab616
Bronze ingest complete.
MSYS_NO_PATHCONV=1 docker compose exec -T spark-master /opt/spark/bin/spark-submit \
	--master local[*] \
	/opt/spark-jobs/silver_transform.py
26/02/22 12:57:20 INFO SparkContext: Running Spark version 3.3.3
26/02/22 12:57:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/22 12:57:20 INFO ResourceUtils: ==============================================================
26/02/22 12:57:20 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/22 12:57:20 INFO ResourceUtils: ==============================================================
26/02/22 12:57:20 INFO SparkContext: Submitted application: P22-Silver-Transform-Hudi
26/02/22 12:57:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/22 12:57:20 INFO ResourceProfile: Limiting resource is cpu
26/02/22 12:57:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/22 12:57:20 INFO SecurityManager: Changing view acls to: spark
26/02/22 12:57:20 INFO SecurityManager: Changing modify acls to: spark
26/02/22 12:57:20 INFO SecurityManager: Changing view acls groups to: 
26/02/22 12:57:20 INFO SecurityManager: Changing modify acls groups to: 
26/02/22 12:57:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
26/02/22 12:57:20 INFO Utils: Successfully started service 'sparkDriver' on port 39415.
26/02/22 12:57:20 INFO SparkEnv: Registering MapOutputTracker
26/02/22 12:57:20 INFO SparkEnv: Registering BlockManagerMaster
26/02/22 12:57:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/22 12:57:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/22 12:57:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/22 12:57:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6f283638-4b30-48d1-aea3-00df3ac2a2ee
26/02/22 12:57:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/22 12:57:20 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/22 12:57:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/22 12:57:20 INFO Executor: Starting executor ID driver on host spark-master
26/02/22 12:57:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/02/22 12:57:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45527.
26/02/22 12:57:20 INFO NettyBlockTransferService: Server created on spark-master:45527
26/02/22 12:57:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/22 12:57:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 45527, None)
26/02/22 12:57:20 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:45527 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 45527, None)
26/02/22 12:57:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 45527, None)
26/02/22 12:57:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 45527, None)
26/02/22 12:57:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/22 12:57:20 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
26/02/22 12:57:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/02/22 12:57:21 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:57:21 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/02/22 12:57:21 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
26/02/22 12:57:21 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
26/02/22 12:57:21 INFO DataSourceUtils: Getting table path..
26/02/22 12:57:21 INFO TablePathUtils: Getting table path from path : s3a://warehouse/bronze/raw_trips
26/02/22 12:57:21 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/bronze/raw_trips
26/02/22 12:57:21 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:21 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:21 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/bronze/raw_trips
26/02/22 12:57:21 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:57:21 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:22 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:22 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/bronze/raw_trips/.hoodie/hoodie.properties
26/02/22 12:57:22 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125709098__commit__COMPLETED__20260222125716929]}
26/02/22 12:57:22 INFO BaseHoodieTableFileIndex: Refresh table bronze_raw_trips, spent: 18 ms
26/02/22 12:57:23 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:57:23 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:57:23 INFO DAGScheduler: Got job 0 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:57:23 INFO DAGScheduler: Final stage: ResultStage 0 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:57:23 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:23 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:57:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
26/02/22 12:57:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.3 MiB)
26/02/22 12:57:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:45527 (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/22 12:57:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4364 bytes) taskResourceAssignments Map()
26/02/22 12:57:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/02/22 12:57:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1123 bytes result sent to driver
26/02/22 12:57:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 80 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/22 12:57:23 INFO DAGScheduler: ResultStage 0 (collect at HoodieSparkEngineContext.java:116) finished in 0.291 s
26/02/22 12:57:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/22 12:57:23 INFO DAGScheduler: Job 0 finished: collect at HoodieSparkEngineContext.java:116, took 0.313752 s
26/02/22 12:57:23 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:23 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:23 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:57:23 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:57:23 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:57:23 INFO CodeGenerator: Code generated in 65.729829 ms
26/02/22 12:57:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.5 KiB, free 434.1 MiB)
26/02/22 12:57:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 434.0 MiB)
26/02/22 12:57:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:45527 (size: 35.2 KiB, free: 434.3 MiB)
26/02/22 12:57:23 INFO SparkContext: Created broadcast 1 from count at <unknown>:0
26/02/22 12:57:23 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:57:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:57:23 INFO DAGScheduler: Registering RDD 5 (count at <unknown>:0) as input to shuffle 0
26/02/22 12:57:23 INFO DAGScheduler: Got map stage job 1 (count at <unknown>:0) with 1 output partitions
26/02/22 12:57:23 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at <unknown>:0)
26/02/22 12:57:23 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:23 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:23 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0), which has no missing parents
26/02/22 12:57:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
26/02/22 12:57:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.0 MiB)
26/02/22 12:57:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:45527 (size: 7.4 KiB, free: 434.3 MiB)
26/02/22 12:57:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/22 12:57:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:57:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/02/22 12:57:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:45527 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:24 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/9aebbd6e-af03-4217-8cdc-41848328a773-0_0-13-27_20260222125709098.parquet, range: 0-681092, partition values: [empty row]
26/02/22 12:57:24 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:24 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2099 bytes result sent to driver
26/02/22 12:57:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 136 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/22 12:57:24 INFO DAGScheduler: ShuffleMapStage 1 (count at <unknown>:0) finished in 0.171 s
26/02/22 12:57:24 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:24 INFO DAGScheduler: running: Set()
26/02/22 12:57:24 INFO DAGScheduler: waiting: Set()
26/02/22 12:57:24 INFO DAGScheduler: failed: Set()
26/02/22 12:57:24 INFO CodeGenerator: Code generated in 6.057972 ms
26/02/22 12:57:24 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:57:24 INFO DAGScheduler: Got job 2 (count at <unknown>:0) with 1 output partitions
26/02/22 12:57:24 INFO DAGScheduler: Final stage: ResultStage 3 (count at <unknown>:0)
26/02/22 12:57:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
26/02/22 12:57:24 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0), which has no missing parents
26/02/22 12:57:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
26/02/22 12:57:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)
26/02/22 12:57:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:45527 (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:57:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/02/22 12:57:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:57:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
26/02/22 12:57:24 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
26/02/22 12:57:24 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2458 bytes result sent to driver
26/02/22 12:57:24 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 31 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:24 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/02/22 12:57:24 INFO DAGScheduler: ResultStage 3 (count at <unknown>:0) finished in 0.036 s
26/02/22 12:57:24 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
26/02/22 12:57:24 INFO DAGScheduler: Job 2 finished: count at <unknown>:0, took 0.042958 s
Bronze rows read: 10,000
26/02/22 12:57:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/22 12:57:24 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips as hoodie table s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:24 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:24 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:24 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:24 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:24 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:24 INFO EmbeddedTimelineService: Overriding hostIp to (spark-master) found in spark-conf. It was null
26/02/22 12:57:24 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:24 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:24 INFO log: Logging initialized @5661ms to org.apache.hudi.org.apache.jetty.util.log.Slf4jLog
26/02/22 12:57:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:45527 in memory (size: 5.5 KiB, free: 434.4 MiB)
26/02/22 12:57:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:45527 in memory (size: 7.4 KiB, free: 434.4 MiB)
26/02/22 12:57:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:45527 in memory (size: 35.2 KiB, free: 434.4 MiB)
26/02/22 12:57:24 INFO Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

26/02/22 12:57:24 INFO Javalin: Starting Javalin ...
26/02/22 12:57:24 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1217 days old. Consider checking for a newer version.).
26/02/22 12:57:24 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.20+8
26/02/22 12:57:24 INFO Server: Started @5845ms
26/02/22 12:57:24 INFO Javalin: Listening on http://localhost:33771/
26/02/22 12:57:24 INFO Javalin: Javalin started in 92ms \o/
26/02/22 12:57:24 INFO TimelineService: Starting Timeline server on port :33771
26/02/22 12:57:24 INFO EmbeddedTimelineService: Started embedded timeline server at spark-master:33771
26/02/22 12:57:24 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
26/02/22 12:57:24 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:57:24 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:57:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(tpep_pickup_datetime),IsNotNull(tpep_dropoff_datetime),IsNotNull(trip_distance),IsNotNull(fare_amount),GreaterThanOrEqual(trip_distance,0.0),GreaterThanOrEqual(fare_amount,0.0)
26/02/22 12:57:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(tpep_pickup_datetime#6),isnotnull(tpep_dropoff_datetime#7),isnotnull(trip_distance#9),isnotnull(fare_amount#15),isnotnull(cast(tpep_pickup_datetime#6 as timestamp)),isnotnull(cast(tpep_dropoff_datetime#7 as timestamp)),(trip_distance#9 >= 0.0),(fare_amount#15 >= 0.0),(cast(tpep_pickup_datetime#6 as timestamp) >= 2024-01-01 00:00:00)
26/02/22 12:57:25 INFO FileSourceStrategy: Output Data Schema: struct<VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: double, trip_distance: double ... 17 more fields>
26/02/22 12:57:25 INFO CodeGenerator: Code generated in 48.365861 ms
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.1 KiB, free 434.2 MiB)
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.2 MiB)
26/02/22 12:57:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:45527 (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:57:25 INFO SparkContext: Created broadcast 4 from toRdd at HoodieSparkUtils.scala:111
26/02/22 12:57:25 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:57:25 INFO HoodieFileIndex: Total file slices: 1; candidate file slices after data skipping: 1; skipping percentage 0.0
26/02/22 12:57:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:25 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:57:25 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:57:25 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222125724679 action: commit
26/02/22 12:57:25 INFO HoodieActiveTimeline: Creating a new instant [==>20260222125724679__commit__REQUESTED]
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125724679__commit__REQUESTED__20260222125725231]}
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:25 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125724679__commit__REQUESTED__20260222125725231]}
26/02/22 12:57:25 INFO HoodieTableMetaClient: Initializing s3a://warehouse/silver/cleaned_trips/.hoodie/metadata as hoodie table s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:57:25 INFO DAGScheduler: Got job 3 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:57:25 INFO DAGScheduler: Final stage: ResultStage 4 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:57:25 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:25 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:25 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 101.5 KiB, free 434.1 MiB)
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 434.0 MiB)
26/02/22 12:57:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:45527 (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:57:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/22 12:57:25 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4405 bytes) taskResourceAssignments Map()
26/02/22 12:57:25 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
26/02/22 12:57:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 969 bytes result sent to driver
26/02/22 12:57:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 21 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/22 12:57:25 INFO DAGScheduler: ResultStage 4 (collect at HoodieSparkEngineContext.java:116) finished in 0.032 s
26/02/22 12:57:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/02/22 12:57:25 INFO DAGScheduler: Job 3 finished: collect at HoodieSparkEngineContext.java:116, took 0.035038 s
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:25 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
26/02/22 12:57:25 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
26/02/22 12:57:25 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
26/02/22 12:57:25 INFO DAGScheduler: Got job 4 (count at HoodieJavaRDD.java:115) with 1 output partitions
26/02/22 12:57:25 INFO DAGScheduler: Final stage: ResultStage 5 (count at HoodieJavaRDD.java:115)
26/02/22 12:57:25 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:25 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:25 INFO DAGScheduler: Submitting ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.0 KiB, free 434.0 MiB)
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.0 MiB)
26/02/22 12:57:25 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:45527 (size: 1866.0 B, free: 434.3 MiB)
26/02/22 12:57:25 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ParallelCollectionRDD[18] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
26/02/22 12:57:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()
26/02/22 12:57:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
26/02/22 12:57:25 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 760 bytes result sent to driver
26/02/22 12:57:25 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
26/02/22 12:57:25 INFO DAGScheduler: ResultStage 5 (count at HoodieJavaRDD.java:115) finished in 0.015 s
26/02/22 12:57:25 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
26/02/22 12:57:25 INFO DAGScheduler: Job 4 finished: count at HoodieJavaRDD.java:115, took 0.017247 s
26/02/22 12:57:25 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
26/02/22 12:57:25 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
26/02/22 12:57:25 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
26/02/22 12:57:25 INFO DAGScheduler: Got job 5 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
26/02/22 12:57:25 INFO DAGScheduler: Final stage: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155)
26/02/22 12:57:25 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:25 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:25 INFO DAGScheduler: Submitting ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 393.9 KiB, free 433.6 MiB)
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 130.0 KiB, free 433.5 MiB)
26/02/22 12:57:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:45527 (size: 130.0 KiB, free: 434.2 MiB)
26/02/22 12:57:25 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ParallelCollectionRDD[19] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:25 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/02/22 12:57:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4344 bytes) taskResourceAssignments Map()
26/02/22 12:57:25 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
26/02/22 12:57:25 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:57:25 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:57:25 INFO HoodieLogFormatWriter: HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
26/02/22 12:57:25 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0. This is unsupported
26/02/22 12:57:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 803 bytes result sent to driver
26/02/22 12:57:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 63 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/02/22 12:57:25 INFO DAGScheduler: ResultStage 6 (foreach at HoodieSparkEngineContext.java:155) finished in 0.089 s
26/02/22 12:57:25 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/02/22 12:57:25 INFO DAGScheduler: Job 5 finished: foreach at HoodieSparkEngineContext.java:155, took 0.092197 s
26/02/22 12:57:25 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:25 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:25 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:57:25 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:57:25 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:25 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:25 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:25 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:25 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:25 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:25 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
26/02/22 12:57:25 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20260222125725772]}
26/02/22 12:57:25 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:25 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:25 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:25 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:57:25 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:57:25 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
26/02/22 12:57:25 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
26/02/22 12:57:25 INFO DAGScheduler: Registering RDD 23 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 1
26/02/22 12:57:25 INFO DAGScheduler: Got job 6 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
26/02/22 12:57:25 INFO DAGScheduler: Final stage: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
26/02/22 12:57:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
26/02/22 12:57:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)
26/02/22 12:57:25 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.5 KiB, free 433.5 MiB)
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.5 MiB)
26/02/22 12:57:25 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:45527 (size: 4.7 KiB, free: 434.2 MiB)
26/02/22 12:57:25 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/22 12:57:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4583 bytes) taskResourceAssignments Map()
26/02/22 12:57:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
26/02/22 12:57:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 964 bytes result sent to driver
26/02/22 12:57:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 15 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/22 12:57:25 INFO DAGScheduler: ShuffleMapStage 7 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.026 s
26/02/22 12:57:25 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:25 INFO DAGScheduler: running: Set()
26/02/22 12:57:25 INFO DAGScheduler: waiting: Set(ResultStage 8)
26/02/22 12:57:25 INFO DAGScheduler: failed: Set()
26/02/22 12:57:25 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.3 KiB, free 433.5 MiB)
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.5 MiB)
26/02/22 12:57:25 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:45527 (size: 3.9 KiB, free: 434.2 MiB)
26/02/22 12:57:25 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:25 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/22 12:57:25 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:25 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
26/02/22 12:57:25 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:25 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 1246 bytes result sent to driver
26/02/22 12:57:25 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 32 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:25 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/22 12:57:25 INFO DAGScheduler: ResultStage 8 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.036 s
26/02/22 12:57:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/02/22 12:57:25 INFO DAGScheduler: Job 6 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.069002 s
26/02/22 12:57:25 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:57:25 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
26/02/22 12:57:25 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:57:25 INFO DAGScheduler: Got job 7 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:57:25 INFO DAGScheduler: Final stage: ResultStage 10 (collect at HoodieJavaRDD.java:177)
26/02/22 12:57:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/22 12:57:25 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:25 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 267.2 KiB, free 433.2 MiB)
26/02/22 12:57:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 91.3 KiB, free 433.1 MiB)
26/02/22 12:57:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:45527 (size: 91.3 KiB, free: 434.1 MiB)
26/02/22 12:57:25 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:25 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
26/02/22 12:57:25 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:25 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)
26/02/22 12:57:26 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:26 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:45527 in memory (size: 3.9 KiB, free: 434.1 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:45527 in memory (size: 130.0 KiB, free: 434.2 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:45527 in memory (size: 1866.0 B, free: 434.2 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:45527 in memory (size: 4.7 KiB, free: 434.2 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:45527 in memory (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:57:26 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE
26/02/22 12:57:26 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-10-8_00000000000000010.hfile.marker.CREATE in 36 ms
26/02/22 12:57:26 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
26/02/22 12:57:26 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/02/22 12:57:26 INFO MetricsSystemImpl: HBase metrics system started
26/02/22 12:57:26 INFO MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
26/02/22 12:57:26 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:26 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:26 INFO HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
26/02/22 12:57:26 INFO HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
26/02/22 12:57:26 INFO HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 310 ms.
26/02/22 12:57:26 INFO MemoryStore: Block rdd_28_0 stored as values in memory (estimated size 272.0 B, free 433.8 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Added rdd_28_0 in memory on spark-master:45527 (size: 272.0 B, free: 434.3 MiB)
26/02/22 12:57:26 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1482 bytes result sent to driver
26/02/22 12:57:26 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 345 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:26 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/22 12:57:26 INFO DAGScheduler: ResultStage 10 (collect at HoodieJavaRDD.java:177) finished in 0.359 s
26/02/22 12:57:26 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/22 12:57:26 INFO DAGScheduler: Job 7 finished: collect at HoodieJavaRDD.java:177, took 0.363796 s
26/02/22 12:57:26 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:57:26 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
26/02/22 12:57:26 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:57:26 INFO DAGScheduler: Got job 8 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:57:26 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:57:26 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:26 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:26 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:57:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 101.2 KiB, free 433.7 MiB)
26/02/22 12:57:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.7 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:45527 (size: 36.1 KiB, free: 434.2 MiB)
26/02/22 12:57:26 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:26 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/02/22 12:57:26 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:57:26 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)
26/02/22 12:57:26 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 855 bytes result sent to driver
26/02/22 12:57:26 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 16 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:26 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/02/22 12:57:26 INFO DAGScheduler: ResultStage 11 (collect at HoodieSparkEngineContext.java:150) finished in 0.026 s
26/02/22 12:57:26 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/02/22 12:57:26 INFO DAGScheduler: Job 8 finished: collect at HoodieSparkEngineContext.java:150, took 0.029783 s
26/02/22 12:57:26 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:57:26 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
26/02/22 12:57:26 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
26/02/22 12:57:26 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
26/02/22 12:57:26 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:57:26 INFO DAGScheduler: Got job 9 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/22 12:57:26 INFO DAGScheduler: Final stage: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:57:26 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:26 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:26 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:57:26 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 101.3 KiB, free 433.6 MiB)
26/02/22 12:57:26 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:45527 (size: 36.2 KiB, free: 434.2 MiB)
26/02/22 12:57:26 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[33] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:26 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
26/02/22 12:57:26 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:57:26 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
26/02/22 12:57:26 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 904 bytes result sent to driver
26/02/22 12:57:26 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 32 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:26 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/22 12:57:26 INFO DAGScheduler: ResultStage 12 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.040 s
26/02/22 12:57:26 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
26/02/22 12:57:26 INFO DAGScheduler: Job 9 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.045030 s
26/02/22 12:57:26 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/00000000000000010
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: MDT s3a://warehouse/silver/cleaned_trips partition FILES has been enabled
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:26 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:26 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 1040 in ms
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:45527 in memory (size: 36.2 KiB, free: 434.2 MiB)
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:45527 in memory (size: 91.3 KiB, free: 434.3 MiB)
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:45527 in memory (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO BlockManager: Removing RDD 28
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 00000000000000010002
26/02/22 12:57:26 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:57:26 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:26 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:26 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:57:26 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125724679__commit__REQUESTED__20260222125725231]}
26/02/22 12:57:26 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO BaseHoodieWriteClient: Scheduling compaction at instant time: 00000000000000010001
26/02/22 12:57:26 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125724679__commit__REQUESTED__20260222125725231]}
26/02/22 12:57:26 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:57:26 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:26 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:26 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:26 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:26 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:26 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:26 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:57:26 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:57:26 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:57:26 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:57:26 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:57:26 INFO DAGScheduler: Registering RDD 34 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 3
26/02/22 12:57:26 INFO DAGScheduler: Registering RDD 40 (distinct at HoodieJavaRDD.java:157) as input to shuffle 2
26/02/22 12:57:26 INFO DAGScheduler: Got job 10 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:57:26 INFO DAGScheduler: Final stage: ResultStage 15 (collect at HoodieJavaRDD.java:177)
26/02/22 12:57:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/22 12:57:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 14)
26/02/22 12:57:26 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:57:26 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 104.0 KiB, free 434.1 MiB)
26/02/22 12:57:26 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KiB, free 434.0 MiB)
26/02/22 12:57:26 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:45527 (size: 31.5 KiB, free: 434.3 MiB)
26/02/22 12:57:26 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:26 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
26/02/22 12:57:26 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()
26/02/22 12:57:26 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)
26/02/22 12:57:26 INFO FileScanRDD: Reading File path: s3a://warehouse/bronze/raw_trips/9aebbd6e-af03-4217-8cdc-41848328a773-0_0-13-27_20260222125709098.parquet, range: 0-681092, partition values: [empty row]
26/02/22 12:57:26 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:26 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(tpep_pickup_datetime, null), noteq(tpep_dropoff_datetime, null)), noteq(trip_distance, null)), noteq(fare_amount, null)), gteq(trip_distance, 0.0)), gteq(fare_amount, 0.0))
26/02/22 12:57:26 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:26 INFO CodecPool: Got brand-new decompressor [.gz]
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 1824 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 454 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ShuffleMapStage 13 (mapToPair at HoodieJavaRDD.java:149) finished in 0.467 s
26/02/22 12:57:27 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:27 INFO DAGScheduler: running: Set()
26/02/22 12:57:27 INFO DAGScheduler: waiting: Set(ResultStage 15, ShuffleMapStage 14)
26/02/22 12:57:27 INFO DAGScheduler: failed: Set()
26/02/22 12:57:27 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 30.1 KiB, free 434.0 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 434.0 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:45527 (size: 13.5 KiB, free: 434.3 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[40] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 12) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 14.0 (TID 12)
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:27 INFO MemoryStore: Block rdd_36_0 stored as values in memory (estimated size 994.0 KiB, free 433.0 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added rdd_36_0 in memory on spark-master:45527 (size: 994.0 KiB, free: 433.4 MiB)
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 14.0 (TID 12). 1394 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 12) in 107 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ShuffleMapStage 14 (distinct at HoodieJavaRDD.java:157) finished in 0.116 s
26/02/22 12:57:27 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:27 INFO DAGScheduler: running: Set()
26/02/22 12:57:27 INFO DAGScheduler: waiting: Set(ResultStage 15)
26/02/22 12:57:27 INFO DAGScheduler: failed: Set()
26/02/22 12:57:27 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 6.3 KiB, free 433.0 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.0 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:45527 (size: 3.4 KiB, free: 433.3 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[42] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 15.0 (TID 13)
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 1237 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 7 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ResultStage 15 (collect at HoodieJavaRDD.java:177) finished in 0.012 s
26/02/22 12:57:27 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/22 12:57:27 INFO DAGScheduler: Job 10 finished: collect at HoodieJavaRDD.java:177, took 0.602145 s
26/02/22 12:57:27 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:57:27 INFO DAGScheduler: Got job 11 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:57:27 INFO DAGScheduler: Final stage: ResultStage 16 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:57:27 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:27 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:27 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 268.6 KiB, free 432.7 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 432.6 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:45527 (size: 93.3 KiB, free: 433.3 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[44] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 761 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ResultStage 16 (collect at HoodieSparkEngineContext.java:150) finished in 0.024 s
26/02/22 12:57:27 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
26/02/22 12:57:27 INFO DAGScheduler: Job 11 finished: collect at HoodieSparkEngineContext.java:150, took 0.026608 s
26/02/22 12:57:27 INFO MapPartitionsRDD: Removing RDD 36 from persistence list
26/02/22 12:57:27 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:27 INFO BlockManager: Removing RDD 36
26/02/22 12:57:27 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:27 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/22 12:57:27 INFO DAGScheduler: Registering RDD 47 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 5
26/02/22 12:57:27 INFO DAGScheduler: Registering RDD 37 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 4
26/02/22 12:57:27 INFO DAGScheduler: Registering RDD 55 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 6
26/02/22 12:57:27 INFO DAGScheduler: Got job 12 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/22 12:57:27 INFO DAGScheduler: Final stage: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105)
26/02/22 12:57:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
26/02/22 12:57:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
26/02/22 12:57:27 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 270.6 KiB, free 433.4 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 94.4 KiB, free 433.3 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:45527 (size: 94.4 KiB, free: 434.1 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[47] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4319 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 29.0 KiB, free 433.2 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 433.2 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:45527 (size: 13.1 KiB, free: 434.1 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[37] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:45527 in memory (size: 3.4 KiB, free: 434.1 MiB)
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (spark-master, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:45527 in memory (size: 13.5 KiB, free: 434.1 MiB)
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 878 bytes result sent to driver
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:45527 in memory (size: 93.3 KiB, free: 434.2 MiB)
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 24 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ShuffleMapStage 18 (mapToPair at HoodieJavaRDD.java:149) finished in 0.034 s
26/02/22 12:57:27 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:27 INFO DAGScheduler: running: Set(ShuffleMapStage 19)
26/02/22 12:57:27 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/22 12:57:27 INFO DAGScheduler: failed: Set()
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:45527 in memory (size: 31.5 KiB, free: 434.3 MiB)
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 1394 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 84 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ShuffleMapStage 19 (mapToPair at HoodieJavaRDD.java:149) finished in 0.099 s
26/02/22 12:57:27 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:27 INFO DAGScheduler: running: Set()
26/02/22 12:57:27 INFO DAGScheduler: waiting: Set(ShuffleMapStage 20, ResultStage 21)
26/02/22 12:57:27 INFO DAGScheduler: failed: Set()
26/02/22 12:57:27 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.8 KiB, free 433.7 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.7 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:45527 (size: 5.1 KiB, free: 434.3 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:27 INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 994.0 KiB, free 432.8 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added rdd_53_0 in memory on spark-master:45527 (size: 994.0 KiB, free: 433.3 MiB)
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 1394 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 90 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ShuffleMapStage 20 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.095 s
26/02/22 12:57:27 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:27 INFO DAGScheduler: running: Set()
26/02/22 12:57:27 INFO DAGScheduler: waiting: Set(ResultStage 21)
26/02/22 12:57:27 INFO DAGScheduler: failed: Set()
26/02/22 12:57:27 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.5 KiB, free 432.8 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.8 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:45527 (size: 3.1 KiB, free: 433.3 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (ShuffledRDD[56] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1284 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 8 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ResultStage 21 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.013 s
26/02/22 12:57:27 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/22 12:57:27 INFO DAGScheduler: Job 12 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.222557 s
26/02/22 12:57:27 INFO BaseSparkCommitActionExecutor: Source read and index timer 235
26/02/22 12:57:27 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:57:27 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/22 12:57:27 INFO DAGScheduler: Got job 13 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/22 12:57:27 INFO DAGScheduler: Final stage: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285)
26/02/22 12:57:27 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:27 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:27 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 269.4 KiB, free 432.5 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 93.7 KiB, free 432.4 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:45527 (size: 93.7 KiB, free: 433.2 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:45527 in memory (size: 3.1 KiB, free: 433.2 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[58] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4333 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:45527 in memory (size: 13.1 KiB, free: 433.2 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:45527 in memory (size: 5.1 KiB, free: 433.2 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:45527 in memory (size: 94.4 KiB, free: 433.3 MiB)
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 833 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 29 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ResultStage 22 (collectAsMap at UpsertPartitioner.java:285) finished in 0.051 s
26/02/22 12:57:27 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/02/22 12:57:27 INFO DAGScheduler: Job 13 finished: collectAsMap at UpsertPartitioner.java:285, took 0.053043 s
26/02/22 12:57:27 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:27 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:27 INFO UpsertPartitioner: For partitionPath :  Total Small Files => 0
26/02/22 12:57:27 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 9855, totalInsertBuckets => 1, recordsPerBucket => 122880
26/02/22 12:57:27 INFO UpsertPartitioner: Total insert buckets for partition path  => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
26/02/22 12:57:27 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 1, updateLocationToBucket size: 0
26/02/22 12:57:27 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260222125724679.inflight
26/02/22 12:57:27 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:57:27 INFO BaseCommitActionExecutor: Auto commit disabled for 20260222125724679
26/02/22 12:57:27 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1073
26/02/22 12:57:27 INFO DAGScheduler: Registering RDD 59 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 7
26/02/22 12:57:27 INFO DAGScheduler: Got job 14 (count at HoodieSparkSqlWriter.scala:1073) with 1 output partitions
26/02/22 12:57:27 INFO DAGScheduler: Final stage: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073)
26/02/22 12:57:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
26/02/22 12:57:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 26)
26/02/22 12:57:27 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 274.5 KiB, free 432.6 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 95.3 KiB, free 432.5 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:45527 (size: 95.3 KiB, free: 433.2 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[59] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4323 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 26.0 (TID 20)
26/02/22 12:57:27 INFO BlockManager: Found block rdd_53_0 locally
26/02/22 12:57:27 INFO Executor: Finished task 0.0 in stage 26.0 (TID 20). 1050 bytes result sent to driver
26/02/22 12:57:27 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 48 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:27 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/22 12:57:27 INFO DAGScheduler: ShuffleMapStage 26 (mapToPair at HoodieJavaRDD.java:149) finished in 0.061 s
26/02/22 12:57:27 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:27 INFO DAGScheduler: running: Set()
26/02/22 12:57:27 INFO DAGScheduler: waiting: Set(ResultStage 27)
26/02/22 12:57:27 INFO DAGScheduler: failed: Set()
26/02/22 12:57:27 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073), which has no missing parents
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 313.8 KiB, free 432.2 MiB)
26/02/22 12:57:27 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 108.0 KiB, free 432.1 MiB)
26/02/22 12:57:27 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:45527 (size: 108.0 KiB, free: 433.1 MiB)
26/02/22 12:57:27 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[64] at filter at HoodieSparkSqlWriter.scala:1073) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:27 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
26/02/22 12:57:27 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:27 INFO Executor: Running task 0.0 in stage 27.0 (TID 21)
26/02/22 12:57:28 INFO ShuffleBlockFetcherIterator: Getting 1 (1186.9 KiB) non-empty blocks including 1 (1186.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:28 INFO SimpleExecutor: Starting consumer, consuming records from the records iterator directly
26/02/22 12:57:28 INFO MarkerHandler: Request: create marker: 6f5f7c41-5232-4a66-8e1a-8201b174644c-0_0-27-21_20260222125724679.parquet.marker.CREATE
26/02/22 12:57:28 INFO TimelineServerBasedWriteMarkers: [timeline-server-based] Created marker file /6f5f7c41-5232-4a66-8e1a-8201b174644c-0_0-27-21_20260222125724679.parquet.marker.CREATE in 228 ms
26/02/22 12:57:28 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:45527 in memory (size: 95.3 KiB, free: 433.2 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:45527 in memory (size: 93.7 KiB, free: 433.3 MiB)
26/02/22 12:57:28 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:28 INFO HoodieCreateHandle: New CreateHandle for partition : with fileId 6f5f7c41-5232-4a66-8e1a-8201b174644c-0
26/02/22 12:57:28 INFO HoodieCreateHandle: Closing the file 6f5f7c41-5232-4a66-8e1a-8201b174644c-0 as we are done with all the records 9855
26/02/22 12:57:28 INFO HoodieCreateHandle: CreateHandle for partitionPath  fileID 6f5f7c41-5232-4a66-8e1a-8201b174644c-0, took 594 ms.
26/02/22 12:57:28 INFO MemoryStore: Block rdd_63_0 stored as values in memory (estimated size 295.0 B, free 432.8 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Added rdd_63_0 in memory on spark-master:45527 (size: 295.0 B, free: 433.3 MiB)
26/02/22 12:57:28 INFO Executor: Finished task 0.0 in stage 27.0 (TID 21). 1749 bytes result sent to driver
26/02/22 12:57:28 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 624 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:28 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/22 12:57:28 INFO DAGScheduler: ResultStage 27 (count at HoodieSparkSqlWriter.scala:1073) finished in 0.639 s
26/02/22 12:57:28 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/22 12:57:28 INFO DAGScheduler: Job 14 finished: count at HoodieSparkSqlWriter.scala:1073, took 0.705805 s
26/02/22 12:57:28 INFO HoodieSparkSqlWriterInternal: Proceeding to commit the write.
26/02/22 12:57:28 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:107
26/02/22 12:57:28 INFO DAGScheduler: Got job 15 (collect at SparkRDDWriteClient.java:107) with 1 output partitions
26/02/22 12:57:28 INFO DAGScheduler: Final stage: ResultStage 32 (collect at SparkRDDWriteClient.java:107)
26/02/22 12:57:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/22 12:57:28 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:28 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107), which has no missing parents
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 314.4 KiB, free 432.5 MiB)
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 108.4 KiB, free 432.4 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:45527 (size: 108.4 KiB, free: 433.2 MiB)
26/02/22 12:57:28 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at SparkRDDWriteClient.java:107) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:28 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
26/02/22 12:57:28 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 22) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:28 INFO Executor: Running task 0.0 in stage 32.0 (TID 22)
26/02/22 12:57:28 INFO BlockManager: Found block rdd_63_0 locally
26/02/22 12:57:28 INFO Executor: Finished task 0.0 in stage 32.0 (TID 22). 1710 bytes result sent to driver
26/02/22 12:57:28 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 22) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:28 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/22 12:57:28 INFO DAGScheduler: ResultStage 32 (collect at SparkRDDWriteClient.java:107) finished in 0.019 s
26/02/22 12:57:28 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/22 12:57:28 INFO DAGScheduler: Job 15 finished: collect at SparkRDDWriteClient.java:107, took 0.021885 s
26/02/22 12:57:28 INFO BaseHoodieWriteClient: Committing 20260222125724679 action commit
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125724679__commit__INFLIGHT__20260222125727875]}
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:28 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:28 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:57:28 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:57:28 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125724679__commit__INFLIGHT__20260222125727875]}
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:28 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:28 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:57:28 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:57:28 INFO BaseHoodieWriteClient: Committing 20260222125724679 action commit
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:28 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:28 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:28 INFO HoodieTableMetadataUtil: Updating at 20260222125724679 from Commit/UPSERT. #partitions_updated=2, #files_added=1
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:28 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:28 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:57:28 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:28 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:28 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:28 INFO HoodieBackedTableMetadataWriter: New commit at 20260222125724679 being applied to MDT.
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:28 INFO CleanerUtils: Cleaned failed attempts if any
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20260222125726412]}
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:28 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:28 INFO BaseHoodieWriteClient: Generate a new instant time: 20260222125724679 action: deltacommit
26/02/22 12:57:28 INFO HoodieActiveTimeline: Creating a new instant [==>20260222125724679__deltacommit__REQUESTED]
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:28 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20260222125724679__deltacommit__REQUESTED__20260222125728798]}
26/02/22 12:57:28 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:28 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:28 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:28 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
26/02/22 12:57:28 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:28 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:28 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:45527 in memory (size: 108.4 KiB, free: 433.3 MiB)
26/02/22 12:57:28 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
26/02/22 12:57:28 INFO DAGScheduler: Registering RDD 74 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 8
26/02/22 12:57:28 INFO DAGScheduler: Got job 16 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
26/02/22 12:57:28 INFO DAGScheduler: Final stage: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105)
26/02/22 12:57:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
26/02/22 12:57:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 33)
26/02/22 12:57:28 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 10.1 KiB, free 432.8 MiB)
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 432.8 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:45527 (size: 5.4 KiB, free: 433.3 MiB)
26/02/22 12:57:28 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[74] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:28 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
26/02/22 12:57:28 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 23) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:57:28 INFO Executor: Running task 0.0 in stage 33.0 (TID 23)
26/02/22 12:57:28 INFO MemoryStore: Block rdd_72_0 stored as values in memory (estimated size 342.0 B, free 432.8 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Added rdd_72_0 in memory on spark-master:45527 (size: 342.0 B, free: 433.3 MiB)
26/02/22 12:57:28 INFO Executor: Finished task 0.0 in stage 33.0 (TID 23). 1093 bytes result sent to driver
26/02/22 12:57:28 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 23) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:28 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/22 12:57:28 INFO DAGScheduler: ShuffleMapStage 33 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.012 s
26/02/22 12:57:28 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:28 INFO DAGScheduler: running: Set()
26/02/22 12:57:28 INFO DAGScheduler: waiting: Set(ResultStage 34)
26/02/22 12:57:28 INFO DAGScheduler: failed: Set()
26/02/22 12:57:28 INFO DAGScheduler: Submitting ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 5.5 KiB, free 432.8 MiB)
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.7 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:45527 (size: 3.1 KiB, free: 433.3 MiB)
26/02/22 12:57:28 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (ShuffledRDD[75] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:28 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
26/02/22 12:57:28 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 24) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:28 INFO Executor: Running task 0.0 in stage 34.0 (TID 24)
26/02/22 12:57:28 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:28 INFO Executor: Finished task 0.0 in stage 34.0 (TID 24). 1366 bytes result sent to driver
26/02/22 12:57:28 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 24) in 6 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:28 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
26/02/22 12:57:28 INFO DAGScheduler: ResultStage 34 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.010 s
26/02/22 12:57:28 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
26/02/22 12:57:28 INFO DAGScheduler: Job 16 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.023401 s
26/02/22 12:57:28 INFO BaseSparkCommitActionExecutor: Source read and index timer 36
26/02/22 12:57:28 INFO UpsertPartitioner: AvgRecordSize => 1024
26/02/22 12:57:28 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:285
26/02/22 12:57:28 INFO DAGScheduler: Got job 17 (collectAsMap at UpsertPartitioner.java:285) with 1 output partitions
26/02/22 12:57:28 INFO DAGScheduler: Final stage: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285)
26/02/22 12:57:28 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:28 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:28 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284), which has no missing parents
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 262.2 KiB, free 432.5 MiB)
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 432.4 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:45527 (size: 90.5 KiB, free: 433.2 MiB)
26/02/22 12:57:28 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:28 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:45527 in memory (size: 5.4 KiB, free: 433.2 MiB)
26/02/22 12:57:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[77] at mapToPair at UpsertPartitioner.java:284) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:28 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
26/02/22 12:57:28 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 25) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4337 bytes) taskResourceAssignments Map()
26/02/22 12:57:28 INFO Executor: Running task 0.0 in stage 35.0 (TID 25)
26/02/22 12:57:28 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:45527 in memory (size: 3.1 KiB, free: 433.2 MiB)
26/02/22 12:57:28 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:28 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:28 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:28 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Building file system view for partition (files)
26/02/22 12:57:28 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:45527 in memory (size: 108.0 KiB, free: 433.3 MiB)
26/02/22 12:57:28 INFO Executor: Finished task 0.0 in stage 35.0 (TID 25). 837 bytes result sent to driver
26/02/22 12:57:28 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 25) in 31 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:28 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/22 12:57:28 INFO DAGScheduler: ResultStage 35 (collectAsMap at UpsertPartitioner.java:285) finished in 0.045 s
26/02/22 12:57:28 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/22 12:57:28 INFO DAGScheduler: Job 17 finished: collectAsMap at UpsertPartitioner.java:285, took 0.047018 s
26/02/22 12:57:28 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:28 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:28 INFO UpsertPartitioner: Total Buckets: 1, bucketInfoMap size: 1, partitionPathToInsertBucketInfos size: 0, updateLocationToBucket size: 1
26/02/22 12:57:28 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260222125724679.deltacommit.inflight
26/02/22 12:57:28 INFO BaseSparkCommitActionExecutor: no validators configured.
26/02/22 12:57:28 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20260222125724679
26/02/22 12:57:28 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
26/02/22 12:57:28 INFO DAGScheduler: Registering RDD 78 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 9
26/02/22 12:57:28 INFO DAGScheduler: Got job 18 (collect at HoodieJavaRDD.java:177) with 1 output partitions
26/02/22 12:57:28 INFO DAGScheduler: Final stage: ResultStage 37 (collect at HoodieJavaRDD.java:177)
26/02/22 12:57:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)
26/02/22 12:57:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 36)
26/02/22 12:57:28 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 267.0 KiB, free 432.6 MiB)
26/02/22 12:57:28 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 93.0 KiB, free 432.5 MiB)
26/02/22 12:57:28 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:45527 (size: 93.0 KiB, free: 433.2 MiB)
26/02/22 12:57:28 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[78] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:28 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
26/02/22 12:57:28 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 26) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4751 bytes) taskResourceAssignments Map()
26/02/22 12:57:28 INFO Executor: Running task 0.0 in stage 36.0 (TID 26)
26/02/22 12:57:28 INFO BlockManager: Found block rdd_72_0 locally
26/02/22 12:57:28 INFO Executor: Finished task 0.0 in stage 36.0 (TID 26). 1050 bytes result sent to driver
26/02/22 12:57:28 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 26) in 10 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:28 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/22 12:57:28 INFO DAGScheduler: ShuffleMapStage 36 (mapToPair at HoodieJavaRDD.java:149) finished in 0.018 s
26/02/22 12:57:28 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:28 INFO DAGScheduler: running: Set()
26/02/22 12:57:28 INFO DAGScheduler: waiting: Set(ResultStage 37)
26/02/22 12:57:28 INFO DAGScheduler: failed: Set()
26/02/22 12:57:28 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 372.6 KiB, free 432.1 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 132.2 KiB, free 432.0 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:45527 in memory (size: 90.5 KiB, free: 433.3 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:45527 (size: 132.2 KiB, free: 433.2 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[83] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 27) (spark-master, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 37.0 (TID 27)
26/02/22 12:57:29 INFO ShuffleBlockFetcherIterator: Getting 1 (405.0 B) non-empty blocks including 1 (405.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:29 INFO BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20260222125724679 for file files-0000-0
26/02/22 12:57:29 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:29 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:29 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips/.hoodie/metadata.
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
26/02/22 12:57:29 INFO HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
26/02/22 12:57:29 INFO HoodieLogFormat$WriterBuilder: HoodieLogFile on path s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
26/02/22 12:57:29 INFO DirectWriteMarkers: Creating Marker Path=s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222125724679/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND
26/02/22 12:57:29 INFO DirectWriteMarkers: [direct] Created marker file s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222125724679/files/.files-0000-0_00000000000000010.log.2_0-37-27.marker.APPEND in 35 ms
26/02/22 12:57:29 INFO HoodieLogFormatWriter: Callback failed. Rolling over to HoodieLogFile{pathStr='s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-37-27', fileLen=-1}
26/02/22 12:57:29 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:29 INFO CodecPool: Got brand-new compressor [.gz]
26/02/22 12:57:29 INFO HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.2_0-37-27, took 366 ms.
26/02/22 12:57:29 INFO MemoryStore: Block rdd_82_0 stored as values in memory (estimated size 343.0 B, free 432.3 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added rdd_82_0 in memory on spark-master:45527 (size: 343.0 B, free: 433.2 MiB)
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 37.0 (TID 27). 1627 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 27) in 409 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ResultStage 37 (collect at HoodieJavaRDD.java:177) finished in 0.427 s
26/02/22 12:57:29 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
26/02/22 12:57:29 INFO DAGScheduler: Job 18 finished: collect at HoodieJavaRDD.java:177, took 0.448395 s
26/02/22 12:57:29 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
26/02/22 12:57:29 INFO BaseSparkCommitActionExecutor: Committing 20260222125724679, action Type deltacommit, operation Type UPSERT_PREPPED
26/02/22 12:57:29 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:57:29 INFO DAGScheduler: Got job 19 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:57:29 INFO DAGScheduler: Final stage: ResultStage 38 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:57:29 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:29 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:29 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 101.2 KiB, free 432.2 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.2 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:45527 (size: 36.1 KiB, free: 433.1 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[85] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 28) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 38.0 (TID 28)
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 38.0 (TID 28). 804 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 28) in 9 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ResultStage 38 (collect at HoodieSparkEngineContext.java:150) finished in 0.016 s
26/02/22 12:57:29 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/22 12:57:29 INFO DAGScheduler: Job 19 finished: collect at HoodieSparkEngineContext.java:150, took 0.017492 s
26/02/22 12:57:29 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
26/02/22 12:57:29 INFO DAGScheduler: Got job 20 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
26/02/22 12:57:29 INFO DAGScheduler: Final stage: ResultStage 39 (collect at HoodieSparkEngineContext.java:150)
26/02/22 12:57:29 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:29 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:29 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 101.1 KiB, free 432.1 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 432.1 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:45527 in memory (size: 36.1 KiB, free: 433.2 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:45527 (size: 36.1 KiB, free: 433.1 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[87] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 29) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:45527 in memory (size: 132.2 KiB, free: 433.3 MiB)
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 39.0 (TID 29)
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:45527 in memory (size: 93.0 KiB, free: 433.4 MiB)
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 39.0 (TID 29). 857 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 29) in 17 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ResultStage 39 (collect at HoodieSparkEngineContext.java:150) finished in 0.028 s
26/02/22 12:57:29 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/22 12:57:29 INFO DAGScheduler: Job 20 finished: collect at HoodieSparkEngineContext.java:150, took 0.030431 s
26/02/22 12:57:29 INFO HoodieActiveTimeline: Marking instant complete [==>20260222125724679__deltacommit__INFLIGHT]
26/02/22 12:57:29 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/20260222125724679.deltacommit
26/02/22 12:57:29 INFO HoodieActiveTimeline: Completed [==>20260222125724679__deltacommit__INFLIGHT]
26/02/22 12:57:29 INFO BaseSparkCommitActionExecutor: Committed 20260222125724679
26/02/22 12:57:29 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:57:29 INFO DAGScheduler: Got job 21 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
26/02/22 12:57:29 INFO DAGScheduler: Final stage: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:57:29 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:29 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:29 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 101.3 KiB, free 433.0 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.9 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:45527 (size: 36.2 KiB, free: 433.3 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[89] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 30) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4425 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 40.0 (TID 30)
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 40.0 (TID 30). 904 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 30) in 18 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ResultStage 40 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.023 s
26/02/22 12:57:29 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
26/02/22 12:57:29 INFO DAGScheduler: Job 21 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.025854 s
26/02/22 12:57:29 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/.temp/20260222125724679
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO FileSystemViewManager: Creating View Manager with storage type MEMORY.
26/02/22 12:57:29 INFO FileSystemViewManager: Creating in-memory based Table View
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO HoodieActiveTimeline: Marking instant complete [==>20260222125724679__commit__INFLIGHT]
26/02/22 12:57:29 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://warehouse/silver/cleaned_trips/.hoodie/20260222125724679.commit
26/02/22 12:57:29 INFO HoodieActiveTimeline: Completed [==>20260222125724679__commit__INFLIGHT]
26/02/22 12:57:29 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
26/02/22 12:57:29 INFO DAGScheduler: Got job 22 (collectAsMap at HoodieSparkEngineContext.java:164) with 2 output partitions
26/02/22 12:57:29 INFO DAGScheduler: Final stage: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164)
26/02/22 12:57:29 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:29 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:29 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 101.3 KiB, free 432.8 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 432.8 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:45527 (size: 36.2 KiB, free: 433.3 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[91] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 31) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4415 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 32) (spark-master, executor driver, partition 1, PROCESS_LOCAL, 4411 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 41.0 (TID 31)
26/02/22 12:57:29 INFO Executor: Running task 1.0 in stage 41.0 (TID 32)
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 41.0 (TID 31). 894 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 31) in 19 ms on spark-master (executor driver) (1/2)
26/02/22 12:57:29 INFO Executor: Finished task 1.0 in stage 41.0 (TID 32). 890 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 32) in 44 ms on spark-master (executor driver) (2/2)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.050 s
26/02/22 12:57:29 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/22 12:57:29 INFO DAGScheduler: Job 22 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.052210 s
26/02/22 12:57:29 INFO FSUtils: Removed directory at s3a://warehouse/silver/cleaned_trips/.hoodie/.temp/20260222125724679
26/02/22 12:57:29 INFO BaseHoodieWriteClient: Committed 20260222125724679
26/02/22 12:57:29 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
26/02/22 12:57:29 INFO BlockManager: Removing RDD 53
26/02/22 12:57:29 INFO MapPartitionsRDD: Removing RDD 63 from persistence list
26/02/22 12:57:29 INFO BlockManager: Removing RDD 63
26/02/22 12:57:29 INFO UnionRDD: Removing RDD 72 from persistence list
26/02/22 12:57:29 INFO BlockManager: Removing RDD 72
26/02/22 12:57:29 INFO MapPartitionsRDD: Removing RDD 82 from persistence list
26/02/22 12:57:29 INFO BlockManager: Removing RDD 82
26/02/22 12:57:29 INFO BaseHoodieWriteClient: Start to clean synchronously.
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:57:29 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:57:29 INFO BaseHoodieWriteClient: Cleaner started
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:57:29 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:57:29 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time: 20260222125729691
26/02/22 12:57:29 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:33771, Timeout=300
26/02/22 12:57:29 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:33771/v1/hoodie/view/compactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222125724679&timelinehash=84e8685380c59b38175a6934f62aad370bc60ec5eaddb6c35ba6d7676a5b8f23)
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:33771/v1/hoodie/view/logcompactions/pending/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222125724679&timelinehash=84e8685380c59b38175a6934f62aad370bc60ec5eaddb6c35ba6d7676a5b8f23)
26/02/22 12:57:29 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
26/02/22 12:57:29 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO FileSystemViewManager: Creating View Manager with storage type REMOTE_FIRST.
26/02/22 12:57:29 INFO FileSystemViewManager: Creating remote first table view
26/02/22 12:57:29 INFO BaseHoodieWriteClient: Start to archive synchronously.
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://warehouse/silver/cleaned_trips/.hoodie/metadata
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
26/02/22 12:57:29 INFO HoodieTimelineArchiver: No Instants to archive
26/02/22 12:57:29 INFO FileSystemViewManager: Creating remote view for basePath s3a://warehouse/silver/cleaned_trips. Server=spark-master:33771, Timeout=300
26/02/22 12:57:29 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://warehouse/silver/cleaned_trips.
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO RemoteHoodieTableFileSystemView: Sending request : (http://spark-master:33771/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fwarehouse%2Fsilver%2Fcleaned_trips&lastinstantts=20260222125724679&timelinehash=84e8685380c59b38175a6934f62aad370bc60ec5eaddb6c35ba6d7676a5b8f23)
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__deltacommit__COMPLETED__20260222125729515]}
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Commit 20260222125724679 successful!
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Compaction Scheduled is Optional.empty
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Clustering Scheduled is Optional.empty
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Is Async Compaction Enabled ? false
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Config.inlineCompactionEnabled ? false
26/02/22 12:57:29 INFO HoodieSparkSqlWriterInternal: Config.asyncClusteringEnabled ? false
26/02/22 12:57:29 WARN HoodieSparkSqlWriterInternal: Closing write client
26/02/22 12:57:29 INFO BaseHoodieClient: Stopping Timeline service !!
26/02/22 12:57:29 INFO EmbeddedTimelineService: Closing Timeline server
26/02/22 12:57:29 INFO TimelineService: Closing Timeline Service
26/02/22 12:57:29 INFO Javalin: Stopping Javalin ...
26/02/22 12:57:29 INFO Javalin: Javalin has stopped
26/02/22 12:57:29 INFO TimelineService: Closed Timeline Service
26/02/22 12:57:29 INFO EmbeddedTimelineService: Closed Timeline server
26/02/22 12:57:29 INFO DataSourceUtils: Getting table path..
26/02/22 12:57:29 INFO TablePathUtils: Getting table path from path : s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO DefaultSource: Obtained hudi table path: s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://warehouse/silver/cleaned_trips
26/02/22 12:57:29 INFO DefaultSource: Is bootstrapped table => false, tableType is: COPY_ON_WRITE, queryType is: snapshot
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:29 INFO HoodieTableConfig: Loading table properties from s3a://warehouse/silver/cleaned_trips/.hoodie/hoodie.properties
26/02/22 12:57:29 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20260222125724679__commit__COMPLETED__20260222125729596]}
26/02/22 12:57:29 INFO BaseHoodieTableFileIndex: Refresh table silver_cleaned_trips, spent: 6 ms
26/02/22 12:57:29 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:45527 in memory (size: 36.1 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:45527 in memory (size: 36.2 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO BlockManager: Removing RDD 82
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:45527 in memory (size: 36.2 KiB, free: 434.4 MiB)
26/02/22 12:57:29 INFO BlockManager: Removing RDD 72
26/02/22 12:57:29 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
26/02/22 12:57:29 INFO DAGScheduler: Got job 23 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
26/02/22 12:57:29 INFO DAGScheduler: Final stage: ResultStage 42 (collect at HoodieSparkEngineContext.java:116)
26/02/22 12:57:29 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:29 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:29 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 101.1 KiB, free 434.1 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 434.0 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:45527 (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[93] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4368 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 42.0 (TID 33)
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 42.0 (TID 33). 1131 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 7 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ResultStage 42 (collect at HoodieSparkEngineContext.java:116) finished in 0.011 s
26/02/22 12:57:29 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/22 12:57:29 INFO DAGScheduler: Job 23 finished: collect at HoodieSparkEngineContext.java:116, took 0.012802 s
26/02/22 12:57:29 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
26/02/22 12:57:29 INFO ClusteringUtils: Found 0 files in pending clustering operations
26/02/22 12:57:29 INFO FileSourceStrategy: Pushed Filters: 
26/02/22 12:57:29 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/22 12:57:29 INFO FileSourceStrategy: Output Data Schema: struct<>
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 210.5 KiB, free 433.8 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 433.8 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:45527 (size: 35.2 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 35 from count at <unknown>:0
26/02/22 12:57:29 INFO HoodieFileIndex: No partition predicates provided, listing full table (1 partitions)
26/02/22 12:57:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/22 12:57:29 INFO DAGScheduler: Registering RDD 97 (count at <unknown>:0) as input to shuffle 10
26/02/22 12:57:29 INFO DAGScheduler: Got map stage job 24 (count at <unknown>:0) with 1 output partitions
26/02/22 12:57:29 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (count at <unknown>:0)
26/02/22 12:57:29 INFO DAGScheduler: Parents of final stage: List()
26/02/22 12:57:29 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:29 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.8 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:45527 (size: 7.4 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[97] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 34) (spark-master, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 43.0 (TID 34)
26/02/22 12:57:29 INFO FileScanRDD: Reading File path: s3a://warehouse/silver/cleaned_trips/6f5f7c41-5232-4a66-8e1a-8201b174644c-0_0-27-21_20260222125724679.parquet, range: 0-1019913, partition values: [empty row]
26/02/22 12:57:29 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:29 INFO S3AInputStream: Switching to Random IO seek policy
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 43.0 (TID 34). 2056 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 34) in 25 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ShuffleMapStage 43 (count at <unknown>:0) finished in 0.030 s
26/02/22 12:57:29 INFO DAGScheduler: looking for newly runnable stages
26/02/22 12:57:29 INFO DAGScheduler: running: Set()
26/02/22 12:57:29 INFO DAGScheduler: waiting: Set()
26/02/22 12:57:29 INFO DAGScheduler: failed: Set()
26/02/22 12:57:29 INFO SparkContext: Starting job: count at <unknown>:0
26/02/22 12:57:29 INFO DAGScheduler: Got job 25 (count at <unknown>:0) with 1 output partitions
26/02/22 12:57:29 INFO DAGScheduler: Final stage: ResultStage 45 (count at <unknown>:0)
26/02/22 12:57:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
26/02/22 12:57:29 INFO DAGScheduler: Missing parents: List()
26/02/22 12:57:29 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0), which has no missing parents
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 11.1 KiB, free 433.7 MiB)
26/02/22 12:57:29 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.8 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:45527 in memory (size: 7.4 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:45527 (size: 5.5 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1509
26/02/22 12:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[100] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
26/02/22 12:57:29 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
26/02/22 12:57:29 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (spark-master, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
26/02/22 12:57:29 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:45527 in memory (size: 36.3 KiB, free: 434.3 MiB)
26/02/22 12:57:29 INFO Executor: Running task 0.0 in stage 45.0 (TID 35)
26/02/22 12:57:29 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/02/22 12:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/02/22 12:57:29 INFO Executor: Finished task 0.0 in stage 45.0 (TID 35). 2458 bytes result sent to driver
26/02/22 12:57:29 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 4 ms on spark-master (executor driver) (1/1)
26/02/22 12:57:29 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/22 12:57:29 INFO DAGScheduler: ResultStage 45 (count at <unknown>:0) finished in 0.011 s
26/02/22 12:57:29 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/22 12:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/22 12:57:29 INFO DAGScheduler: Job 25 finished: count at <unknown>:0, took 0.012807 s
Silver transform complete. Rows written: 9,855
Rows filtered out: 145
============================================================
  Silver Transform (Hudi Upsert) COMPLETE
============================================================
26/02/22 12:57:30 INFO BlockManager: Removing RDD 53
26/02/22 12:57:30 INFO BlockManager: Removing RDD 63
26/02/22 12:57:30 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:45527 in memory (size: 36.1 KiB, free: 434.4 MiB)
26/02/22 12:57:30 INFO BlockManager: Removing RDD 36
26/02/22 12:57:30 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/22 12:57:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/22 12:57:30 INFO MemoryStore: MemoryStore cleared
26/02/22 12:57:30 INFO BlockManager: BlockManager stopped
26/02/22 12:57:30 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/22 12:57:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/22 12:57:30 INFO SparkContext: Successfully stopped SparkContext
26/02/22 12:57:30 INFO ShutdownHookManager: Shutdown hook called
26/02/22 12:57:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce3f1a45-9b01-4cae-bd4d-6ae3d61b9d50
26/02/22 12:57:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-fa1f929d-d709-4a4e-a686-3d388b77f032/pyspark-c21f9b54-bb8c-4cc2-8dd3-f0e9f16fbe33
26/02/22 12:57:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-fa1f929d-d709-4a4e-a686-3d388b77f032
26/02/22 12:57:30 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
26/02/22 12:57:30 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
26/02/22 12:57:30 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
Silver transform complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir ."
 Container p22-minio  Running
[0m12:57:32  Running with dbt=1.11.5
[0m12:57:33  Installing dbt-labs/dbt_utils
[0m12:57:36  Installed from version 1.3.3
[0m12:57:36  Up to date!
[0m12:57:37  Running with dbt=1.11.5
[0m12:57:37  Registered adapter: duckdb=1.10.0
[0m12:57:37  Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:57:39  Found 14 models, 74 data tests, 3 seeds, 1 source, 606 macros
[0m12:57:39  
[0m12:57:39  Concurrency: 4 threads (target='dev')
[0m12:57:39  
[0m12:57:40  1 of 91 START sql table model main.dim_dates ................................... [RUN]
[0m12:57:40  2 of 91 START sql view model main.stg_yellow_trips ............................. [RUN]
[0m12:57:40  3 of 91 START seed file main_main.payment_type_lookup .......................... [RUN]
[0m12:57:40  4 of 91 START seed file main_main.rate_code_lookup ............................. [RUN]
[0m12:57:40  4 of 91 OK loaded seed file main_main.rate_code_lookup ......................... [[32mCREATE 7[0m in 0.11s]
[0m12:57:40  5 of 91 START seed file main_main.taxi_zone_lookup ............................. [RUN]
[0m12:57:40  3 of 91 OK loaded seed file main_main.payment_type_lookup ...................... [[32mCREATE 6[0m in 0.12s]
[0m12:57:40  6 of 91 START test not_null_rate_code_lookup_rate_code_id ...................... [RUN]
[0m12:57:40  2 of 91 OK created sql view model main.stg_yellow_trips ........................ [[32mOK[0m in 0.12s]
[0m12:57:40  7 of 91 START test unique_rate_code_lookup_rate_code_id ........................ [RUN]
[0m12:57:40  1 of 91 OK created sql table model main.dim_dates .............................. [[32mOK[0m in 0.14s]
[0m12:57:40  8 of 91 START test not_null_payment_type_lookup_payment_type_id ................ [RUN]
[0m12:57:40  5 of 91 OK loaded seed file main_main.taxi_zone_lookup ......................... [[32mCREATE 265[0m in 0.06s]
[0m12:57:40  9 of 91 START test unique_payment_type_lookup_payment_type_id .................. [RUN]
[0m12:57:40  6 of 91 PASS not_null_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.06s]
[0m12:57:40  8 of 91 PASS not_null_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.06s]
[0m12:57:40  10 of 91 START test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [RUN]
[0m12:57:40  7 of 91 PASS unique_rate_code_lookup_rate_code_id .............................. [[32mPASS[0m in 0.07s]
[0m12:57:40  11 of 91 START test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [RUN]
[0m12:57:40  12 of 91 START test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ........ [RUN]
[0m12:57:40  9 of 91 PASS unique_payment_type_lookup_payment_type_id ........................ [[32mPASS[0m in 0.07s]
[0m12:57:40  13 of 91 START test assert_fare_not_exceeds_total .............................. [RUN]
[0m12:57:40  11 of 91 PASS accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[32mPASS[0m in 0.13s]
[0m12:57:40  10 of 91 PASS accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[32mPASS[0m in 0.13s]
[0m12:57:40  12 of 91 PASS accepted_values_stg_yellow_trips_vendor_id__1__2__6 .............. [[32mPASS[0m in 0.12s]
[0m12:57:40  14 of 91 START test not_null_stg_yellow_trips_dropoff_datetime ................. [RUN]
[0m12:57:40  15 of 91 START test not_null_stg_yellow_trips_dropoff_location_id .............. [RUN]
[0m12:57:40  16 of 91 START test not_null_stg_yellow_trips_fare_amount ...................... [RUN]
[0m12:57:40  13 of 91 PASS assert_fare_not_exceeds_total .................................... [[32mPASS[0m in 0.12s]
[0m12:57:40  17 of 91 START test not_null_stg_yellow_trips_pickup_datetime .................. [RUN]
[0m12:57:41  16 of 91 PASS not_null_stg_yellow_trips_fare_amount ............................ [[32mPASS[0m in 0.05s]
[0m12:57:41  15 of 91 PASS not_null_stg_yellow_trips_dropoff_location_id .................... [[32mPASS[0m in 0.06s]
[0m12:57:41  14 of 91 PASS not_null_stg_yellow_trips_dropoff_datetime ....................... [[32mPASS[0m in 0.06s]
[0m12:57:41  18 of 91 START test not_null_stg_yellow_trips_pickup_location_id ............... [RUN]
[0m12:57:41  19 of 91 START test not_null_stg_yellow_trips_total_amount ..................... [RUN]
[0m12:57:41  20 of 91 START test not_null_stg_yellow_trips_trip_distance_miles .............. [RUN]
[0m12:57:41  17 of 91 PASS not_null_stg_yellow_trips_pickup_datetime ........................ [[32mPASS[0m in 0.17s]
[0m12:57:41  21 of 91 START test not_null_stg_yellow_trips_trip_id .......................... [RUN]
[0m12:57:41  18 of 91 PASS not_null_stg_yellow_trips_pickup_location_id ..................... [[32mPASS[0m in 0.16s]
[0m12:57:41  20 of 91 PASS not_null_stg_yellow_trips_trip_distance_miles .................... [[32mPASS[0m in 0.05s]
[0m12:57:41  19 of 91 PASS not_null_stg_yellow_trips_total_amount ........................... [[32mPASS[0m in 0.05s]
[0m12:57:41  22 of 91 START test not_null_stg_yellow_trips_vendor_id ........................ [RUN]
[0m12:57:41  23 of 91 START test unique_stg_yellow_trips_trip_id ............................ [RUN]
[0m12:57:41  24 of 91 START test not_null_dim_dates_date_key ................................ [RUN]
[0m12:57:41  21 of 91 PASS not_null_stg_yellow_trips_trip_id ................................ [[32mPASS[0m in 0.05s]
[0m12:57:41  25 of 91 START test not_null_dim_dates_day_of_week_name ........................ [RUN]
[0m12:57:41  24 of 91 PASS not_null_dim_dates_date_key ...................................... [[32mPASS[0m in 0.05s]
[0m12:57:41  22 of 91 PASS not_null_stg_yellow_trips_vendor_id .............................. [[32mPASS[0m in 0.06s]
[0m12:57:41  26 of 91 START test not_null_dim_dates_is_holiday .............................. [RUN]
[0m12:57:41  27 of 91 START test not_null_dim_dates_is_weekend .............................. [RUN]
[0m12:57:41  23 of 91 PASS unique_stg_yellow_trips_trip_id .................................. [[32mPASS[0m in 0.07s]
[0m12:57:41  28 of 91 START test unique_dim_dates_date_key .................................. [RUN]
[0m12:57:41  25 of 91 PASS not_null_dim_dates_day_of_week_name .............................. [[32mPASS[0m in 0.06s]
[0m12:57:41  29 of 91 START test not_null_taxi_zone_lookup_Borough .......................... [RUN]
[0m12:57:41  26 of 91 PASS not_null_dim_dates_is_holiday .................................... [[32mPASS[0m in 0.05s]
[0m12:57:41  27 of 91 PASS not_null_dim_dates_is_weekend .................................... [[32mPASS[0m in 0.05s]
[0m12:57:41  30 of 91 START test not_null_taxi_zone_lookup_LocationID ....................... [RUN]
[0m12:57:41  28 of 91 PASS unique_dim_dates_date_key ........................................ [[32mPASS[0m in 0.05s]
[0m12:57:41  31 of 91 START test unique_taxi_zone_lookup_LocationID ......................... [RUN]
[0m12:57:41  32 of 91 START sql view model main.stg_rate_codes .............................. [RUN]
[0m12:57:41  29 of 91 PASS not_null_taxi_zone_lookup_Borough ................................ [[32mPASS[0m in 0.05s]
[0m12:57:41  33 of 91 START sql view model main.stg_payment_types ........................... [RUN]
[0m12:57:41  31 of 91 PASS unique_taxi_zone_lookup_LocationID ............................... [[32mPASS[0m in 0.04s]
[0m12:57:41  34 of 91 START sql view model main.int_trip_metrics ............................ [RUN]
[0m12:57:41  30 of 91 PASS not_null_taxi_zone_lookup_LocationID ............................. [[32mPASS[0m in 0.06s]
[0m12:57:41  35 of 91 START sql view model main.stg_taxi_zones .............................. [RUN]
[0m12:57:41  33 of 91 OK created sql view model main.stg_payment_types ...................... [[32mOK[0m in 0.06s]
[0m12:57:41  36 of 91 START test not_null_stg_payment_types_payment_type_id ................. [RUN]
[0m12:57:41  32 of 91 OK created sql view model main.stg_rate_codes ......................... [[32mOK[0m in 0.07s]
[0m12:57:41  37 of 91 START test not_null_stg_payment_types_payment_type_name ............... [RUN]
[0m12:57:41  35 of 91 OK created sql view model main.stg_taxi_zones ......................... [[32mOK[0m in 0.06s]
[0m12:57:41  34 of 91 OK created sql view model main.int_trip_metrics ....................... [[32mOK[0m in 0.07s]
[0m12:57:41  38 of 91 START test unique_stg_payment_types_payment_type_id ................... [RUN]
[0m12:57:41  39 of 91 START test not_null_stg_rate_codes_rate_code_id ....................... [RUN]
[0m12:57:41  36 of 91 PASS not_null_stg_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m12:57:41  37 of 91 PASS not_null_stg_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m12:57:41  40 of 91 START test not_null_stg_rate_codes_rate_code_name ..................... [RUN]
[0m12:57:41  41 of 91 START test unique_stg_rate_codes_rate_code_id ......................... [RUN]
[0m12:57:41  38 of 91 PASS unique_stg_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m12:57:41  40 of 91 PASS not_null_stg_rate_codes_rate_code_name ........................... [[32mPASS[0m in 0.04s]
[0m12:57:41  39 of 91 PASS not_null_stg_rate_codes_rate_code_id ............................. [[32mPASS[0m in 0.05s]
[0m12:57:41  42 of 91 START test not_null_stg_taxi_zones_borough ............................ [RUN]
[0m12:57:41  43 of 91 START test not_null_stg_taxi_zones_location_id ........................ [RUN]
[0m12:57:41  41 of 91 PASS unique_stg_rate_codes_rate_code_id ............................... [[32mPASS[0m in 0.04s]
[0m12:57:41  44 of 91 START test not_null_stg_taxi_zones_zone_name .......................... [RUN]
[0m12:57:41  45 of 91 START test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:57:41  43 of 91 PASS not_null_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.05s]
[0m12:57:41  42 of 91 PASS not_null_stg_taxi_zones_borough .................................. [[32mPASS[0m in 0.05s]
[0m12:57:41  46 of 91 START test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m12:57:41  44 of 91 PASS not_null_stg_taxi_zones_zone_name ................................ [[32mPASS[0m in 0.05s]
[0m12:57:41  47 of 91 START test unique_stg_taxi_zones_location_id .......................... [RUN]
[0m12:57:41  48 of 91 START test assert_trip_duration_positive .............................. [RUN]
[0m12:57:41  45 of 91 PASS relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.06s]
[0m12:57:41  49 of 91 START test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [RUN]
[0m12:57:41  47 of 91 PASS unique_stg_taxi_zones_location_id ................................ [[32mPASS[0m in 0.04s]
[0m12:57:41  50 of 91 START test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [RUN]
[0m12:57:41  46 of 91 PASS relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.06s]
[0m12:57:41  48 of 91 PASS assert_trip_duration_positive .................................... [[32mPASS[0m in 0.04s]
[0m12:57:41  51 of 91 START test not_null_int_trip_metrics_is_weekend ....................... [RUN]
[0m12:57:41  52 of 91 START test not_null_int_trip_metrics_pickup_date ...................... [RUN]
[0m12:57:41  49 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0 ..... [[32mPASS[0m in 0.05s]
[0m12:57:41  53 of 91 START test not_null_int_trip_metrics_pickup_hour ...................... [RUN]
[0m12:57:41  51 of 91 PASS not_null_int_trip_metrics_is_weekend ............................. [[32mPASS[0m in 0.04s]
[0m12:57:41  54 of 91 START test not_null_int_trip_metrics_trip_duration_minutes ............ [RUN]
[0m12:57:41  53 of 91 PASS not_null_int_trip_metrics_pickup_hour ............................ [[32mPASS[0m in 0.04s]
[0m12:57:41  55 of 91 START test not_null_int_trip_metrics_trip_id .......................... [RUN]
[0m12:57:41  52 of 91 PASS not_null_int_trip_metrics_pickup_date ............................ [[32mPASS[0m in 0.06s]
[0m12:57:41  50 of 91 PASS dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[32mPASS[0m in 0.07s]
[0m12:57:41  56 of 91 START sql table model main.dim_payment_types .......................... [RUN]
[0m12:57:41  57 of 91 START sql table model main.dim_locations .............................. [RUN]
[0m12:57:41  55 of 91 PASS not_null_int_trip_metrics_trip_id ................................ [[32mPASS[0m in 0.04s]
[0m12:57:41  54 of 91 PASS not_null_int_trip_metrics_trip_duration_minutes .................. [[32mPASS[0m in 0.06s]
[0m12:57:41  58 of 91 START sql view model main.int_daily_summary ........................... [RUN]
[0m12:57:41  59 of 91 START sql view model main.int_hourly_patterns ......................... [RUN]
[0m12:57:41  56 of 91 OK created sql table model main.dim_payment_types ..................... [[32mOK[0m in 0.07s]
[0m12:57:41  57 of 91 OK created sql table model main.dim_locations ......................... [[32mOK[0m in 0.07s]
[0m12:57:41  60 of 91 START test not_null_dim_payment_types_payment_type_id ................. [RUN]
[0m12:57:41  61 of 91 START test not_null_dim_payment_types_payment_type_name ............... [RUN]
[0m12:57:41  58 of 91 OK created sql view model main.int_daily_summary ...................... [[32mOK[0m in 0.07s]
[0m12:57:41  59 of 91 OK created sql view model main.int_hourly_patterns .................... [[32mOK[0m in 0.08s]
[0m12:57:41  62 of 91 START test unique_dim_payment_types_payment_type_id ................... [RUN]
[0m12:57:41  63 of 91 START test not_null_dim_locations_borough ............................. [RUN]
[0m12:57:41  61 of 91 PASS not_null_dim_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m12:57:41  60 of 91 PASS not_null_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m12:57:41  64 of 91 START test not_null_dim_locations_location_id ......................... [RUN]
[0m12:57:41  65 of 91 START test not_null_dim_locations_zone_name ........................... [RUN]
[0m12:57:41  63 of 91 PASS not_null_dim_locations_borough ................................... [[32mPASS[0m in 0.05s]
[0m12:57:41  62 of 91 PASS unique_dim_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.06s]
[0m12:57:41  64 of 91 PASS not_null_dim_locations_location_id ............................... [[32mPASS[0m in 0.04s]
[0m12:57:41  66 of 91 START test unique_dim_locations_location_id ........................... [RUN]
[0m12:57:41  65 of 91 PASS not_null_dim_locations_zone_name ................................. [[32mPASS[0m in 0.05s]
[0m12:57:41  67 of 91 START test dbt_utils_accepted_range_int_daily_summary_total_trips__0 .. [RUN]
[0m12:57:41  68 of 91 START test not_null_int_daily_summary_pickup_date ..................... [RUN]
[0m12:57:41  69 of 91 START test not_null_int_daily_summary_total_revenue ................... [RUN]
[0m12:57:41  66 of 91 PASS unique_dim_locations_location_id ................................. [[32mPASS[0m in 0.05s]
[0m12:57:41  70 of 91 START test not_null_int_daily_summary_total_trips ..................... [RUN]
[0m12:57:41  68 of 91 PASS not_null_int_daily_summary_pickup_date ........................... [[32mPASS[0m in 0.06s]
[0m12:57:41  71 of 91 START test unique_int_daily_summary_pickup_date ....................... [RUN]
[0m12:57:41  67 of 91 PASS dbt_utils_accepted_range_int_daily_summary_total_trips__0 ........ [[32mPASS[0m in 0.08s]
[0m12:57:41  69 of 91 PASS not_null_int_daily_summary_total_revenue ......................... [[32mPASS[0m in 0.08s]
[0m12:57:41  72 of 91 START test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [RUN]
[0m12:57:41  73 of 91 START test not_null_int_hourly_patterns_pickup_date ................... [RUN]
[0m12:57:42  70 of 91 PASS not_null_int_daily_summary_total_trips ........................... [[32mPASS[0m in 0.08s]
[0m12:57:42  72 of 91 PASS dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0 .. [[32mPASS[0m in 0.05s]
[0m12:57:42  74 of 91 START test not_null_int_hourly_patterns_pickup_hour ................... [RUN]
[0m12:57:42  71 of 91 PASS unique_int_daily_summary_pickup_date ............................. [[32mPASS[0m in 0.07s]
[0m12:57:42  75 of 91 START test not_null_int_hourly_patterns_total_trips ................... [RUN]
[0m12:57:42  73 of 91 PASS not_null_int_hourly_patterns_pickup_date ......................... [[32mPASS[0m in 0.05s]
[0m12:57:42  76 of 91 START sql incremental model main.fct_trips ............................ [RUN]
[0m12:57:42  77 of 91 START sql table model main.mart_daily_revenue ......................... [RUN]
[0m12:57:42  74 of 91 PASS not_null_int_hourly_patterns_pickup_hour ......................... [[32mPASS[0m in 0.07s]
[0m12:57:42  75 of 91 PASS not_null_int_hourly_patterns_total_trips ......................... [[32mPASS[0m in 0.09s]
[0m12:57:42  78 of 91 START sql table model main.mart_hourly_demand ......................... [RUN]
[0m12:57:42  77 of 91 OK created sql table model main.mart_daily_revenue .................... [[32mOK[0m in 0.11s]
[0m12:57:42  79 of 91 START test not_null_mart_daily_revenue_date_key ....................... [RUN]
[0m12:57:42  80 of 91 START test not_null_mart_daily_revenue_total_revenue .................. [RUN]
[0m12:57:42  76 of 91 OK created sql incremental model main.fct_trips ....................... [[32mOK[0m in 0.19s]
[0m12:57:42  81 of 91 START test unique_mart_daily_revenue_date_key ......................... [RUN]
[0m12:57:42  79 of 91 PASS not_null_mart_daily_revenue_date_key ............................. [[32mPASS[0m in 0.11s]
[0m12:57:42  80 of 91 PASS not_null_mart_daily_revenue_total_revenue ........................ [[32mPASS[0m in 0.11s]
[0m12:57:42  82 of 91 START test not_null_fct_trips_pickup_datetime ......................... [RUN]
[0m12:57:42  83 of 91 START test not_null_fct_trips_total_amount ............................ [RUN]
[0m12:57:42  81 of 91 PASS unique_mart_daily_revenue_date_key ............................... [[32mPASS[0m in 0.04s]
[0m12:57:42  84 of 91 START test not_null_fct_trips_trip_id ................................. [RUN]
[0m12:57:42  78 of 91 OK created sql table model main.mart_hourly_demand .................... [[32mOK[0m in 0.18s]
[0m12:57:42  85 of 91 START test unique_fct_trips_trip_id ................................... [RUN]
[0m12:57:42  83 of 91 PASS not_null_fct_trips_total_amount .................................. [[32mPASS[0m in 0.04s]
[0m12:57:42  86 of 91 START test not_null_mart_hourly_demand_is_weekend ..................... [RUN]
[0m12:57:42  82 of 91 PASS not_null_fct_trips_pickup_datetime ............................... [[32mPASS[0m in 0.06s]
[0m12:57:42  87 of 91 START test not_null_mart_hourly_demand_pickup_hour .................... [RUN]
[0m12:57:42  84 of 91 PASS not_null_fct_trips_trip_id ....................................... [[32mPASS[0m in 0.05s]
[0m12:57:42  85 of 91 PASS unique_fct_trips_trip_id ......................................... [[32mPASS[0m in 0.04s]
[0m12:57:42  88 of 91 START sql table model main.mart_location_performance .................. [RUN]
[0m12:57:42  86 of 91 PASS not_null_mart_hourly_demand_is_weekend ........................... [[32mPASS[0m in 0.05s]
[0m12:57:42  87 of 91 PASS not_null_mart_hourly_demand_pickup_hour .......................... [[32mPASS[0m in 0.05s]
[0m12:57:42  88 of 91 OK created sql table model main.mart_location_performance ............. [[32mOK[0m in 0.04s]
[0m12:57:42  89 of 91 START test not_null_mart_location_performance_pickup_location_id ...... [RUN]
[0m12:57:42  90 of 91 START test not_null_mart_location_performance_total_pickups ........... [RUN]
[0m12:57:42  91 of 91 START test unique_mart_location_performance_pickup_location_id ........ [RUN]
[0m12:57:42  89 of 91 PASS not_null_mart_location_performance_pickup_location_id ............ [[32mPASS[0m in 0.04s]
[0m12:57:42  90 of 91 PASS not_null_mart_location_performance_total_pickups ................. [[32mPASS[0m in 0.05s]
[0m12:57:42  91 of 91 PASS unique_mart_location_performance_pickup_location_id .............. [[32mPASS[0m in 0.05s]
[0m12:57:42  
[0m12:57:42  Finished running 1 incremental model, 3 seeds, 6 table models, 74 data tests, 7 view models in 0 hours 0 minutes and 2.95 seconds (2.95s).
[0m12:57:42  
[0m12:57:42  [32mCompleted successfully[0m
[0m12:57:42  
[0m12:57:42  Done. PASS=91 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=91
dbt build complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/22-hudi-cdc-storage'

============================================================
  BENCHMARK COMPLETE
  Total elapsed: 75s
============================================================
Results saved to benchmark_results/latest.json
docker compose --profile generator --profile dbt down -v --remove-orphans
 Container p22-spark-worker  Stopping
 Container p22-spark-worker  Stopped
 Container p22-spark-worker  Removing
 Container p22-spark-worker  Removed
 Container p22-spark-master  Stopping
 Container p22-spark-master  Stopped
 Container p22-spark-master  Removing
 Container p22-spark-master  Removed
 Container p22-kafka  Stopping
 Container p22-mc-init  Stopping
 Container p22-mc-init  Stopped
 Container p22-mc-init  Removing
 Container p22-mc-init  Removed
 Container p22-minio  Stopping
 Container p22-minio  Stopped
 Container p22-minio  Removing
 Container p22-minio  Removed
 Container p22-kafka  Stopped
 Container p22-kafka  Removing
 Container p22-kafka  Removed
 Network p22-pipeline-net  Removing
 Volume 22-hudi-cdc-storage_minio-data  Removing
 Volume 22-hudi-cdc-storage_minio-data  Removed
 Network p22-pipeline-net  Removed
Pipeline 22 stopped.
docker compose --profile generator --profile dbt down -v --remove-orphans
Pipeline 22 stopped.
Pipeline 22 cleaned.
