docker compose --profile generator --profile dbt --profile lakekeeper down -v
Pipeline 01 stopped and volumes removed.
docker compose up -d
 Network p01-pipeline-net  Creating
 Network p01-pipeline-net  Created
 Volume 01-kafka-flink-iceberg_flink-checkpoints  Creating
 Volume 01-kafka-flink-iceberg_flink-checkpoints  Created
 Volume 01-kafka-flink-iceberg_minio-data  Creating
 Volume 01-kafka-flink-iceberg_minio-data  Created
 Container p01-kafka  Creating
 Container p01-minio  Creating
 Container p01-minio  Created
 Container p01-mc-init  Creating
 Container p01-kafka  Created
 Container p01-mc-init  Created
 Container p01-flink-jobmanager  Creating
 Container p01-flink-jobmanager  Created
 Container p01-flink-taskmanager  Creating
 Container p01-flink-taskmanager  Created
 Container p01-minio  Starting
 Container p01-kafka  Starting
 Container p01-minio  Started
 Container p01-minio  Waiting
 Container p01-kafka  Started
 Container p01-minio  Healthy
 Container p01-mc-init  Starting
 Container p01-mc-init  Started
 Container p01-kafka  Waiting
 Container p01-mc-init  Waiting
 Container p01-mc-init  Exited
 Container p01-kafka  Healthy
 Container p01-flink-jobmanager  Starting
 Container p01-flink-jobmanager  Started
 Container p01-flink-jobmanager  Waiting
 Container p01-flink-jobmanager  Healthy
 Container p01-flink-taskmanager  Starting
 Container p01-flink-taskmanager  Started

=== Pipeline 01: Kafka + Flink + Iceberg ===
Kafka:            localhost:9092
Flink Dashboard:  http://localhost:8083
MinIO Console:    http://localhost:9001  (minioadmin/minioadmin)

Next steps:
  make create-topics   # Create Kafka topics
  make generate        # Produce taxi events to Kafka
  make process         # Submit Flink SQL jobs
  make dbt-build       # Run dbt transformations
============================================================
  Pipeline 01 Benchmark: Kafka + Flink + Iceberg
============================================================
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
docker compose --profile generator --profile dbt --profile lakekeeper down -v
Pipeline 01 stopped and volumes removed.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
docker compose up -d
 Network p01-pipeline-net  Creating
 Network p01-pipeline-net  Created
 Volume 01-kafka-flink-iceberg_flink-checkpoints  Creating
 Volume 01-kafka-flink-iceberg_flink-checkpoints  Created
 Volume 01-kafka-flink-iceberg_minio-data  Creating
 Volume 01-kafka-flink-iceberg_minio-data  Created
 Container p01-minio  Creating
 Container p01-kafka  Creating
 Container p01-minio  Created
 Container p01-mc-init  Creating
 Container p01-kafka  Created
 Container p01-mc-init  Created
 Container p01-flink-jobmanager  Creating
 Container p01-flink-jobmanager  Created
 Container p01-flink-taskmanager  Creating
 Container p01-flink-taskmanager  Created
 Container p01-kafka  Starting
 Container p01-minio  Starting
 Container p01-minio  Started
 Container p01-minio  Waiting
 Container p01-kafka  Started
 Container p01-minio  Healthy
 Container p01-mc-init  Starting
 Container p01-mc-init  Started
 Container p01-mc-init  Waiting
 Container p01-kafka  Waiting
 Container p01-mc-init  Exited
 Container p01-kafka  Healthy
 Container p01-flink-jobmanager  Starting
 Container p01-flink-jobmanager  Started
 Container p01-flink-jobmanager  Waiting
 Container p01-flink-jobmanager  Healthy
 Container p01-flink-taskmanager  Starting
 Container p01-flink-taskmanager  Started

=== Pipeline 01: Kafka + Flink + Iceberg ===
Kafka:            localhost:9092
Flink Dashboard:  http://localhost:8083
MinIO Console:    http://localhost:9001  (minioadmin/minioadmin)

Next steps:
  make create-topics   # Create Kafka topics
  make generate        # Produce taxi events to Kafka
  make process         # Submit Flink SQL jobs
  make dbt-build       # Run dbt transformations
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
Waiting for services to stabilize...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
docker compose exec kafka /bin/bash /opt/kafka/scripts/create-topics.sh
============================================================
  Creating Kafka Topics
  Bootstrap server: localhost:9092
============================================================
Waiting for Kafka to be ready...
Kafka is ready.

Creating topic: taxi.raw_trips
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
  taxi.raw_trips created (3 partitions, 72h retention)

Creating topic: taxi.raw_trips.dlq
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.dlq.
  taxi.raw_trips.dlq created (1 partition, 7d retention)

============================================================
  Topics:
taxi.raw_trips
taxi.raw_trips.dlq

  Topic Details:
Topic: taxi.raw_trips	TopicId: t73OCN03QD-O-2Z1UYxP3Q	PartitionCount: 3	ReplicationFactor: 1	Configs: cleanup.policy=delete,segment.bytes=104857600,retention.ms=259200000
	Topic: taxi.raw_trips	Partition: 0	Leader: 1	Replicas: 1	Isr: 1	Elr: 	LastKnownElr: 
	Topic: taxi.raw_trips	Partition: 1	Leader: 1	Replicas: 1	Isr: 1	Elr: 	LastKnownElr: 
	Topic: taxi.raw_trips	Partition: 2	Leader: 1	Replicas: 1	Isr: 1	Elr: 	LastKnownElr: 
============================================================
  Topic creation complete.
============================================================
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
MSYS_NO_PATHCONV=1 docker compose run --rm -e MAX_EVENTS=10000 data-generator
 Container p01-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.52s
  Rate:    19,393 events/sec
============================================================
  Metrics written to /tmp/generator_metrics.json
Limited data generation complete (10k events).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
Waiting for Flink processing to catch up...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
=== Bronze: Kafka Ã¢â€ â€™ Iceberg ===
MSYS_NO_PATHCONV=1 docker compose exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/05-bronze.sql
2026-02-21 14:42:35,278 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-02-21 14:42:35,341 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).
2026-02-21 14:42:35,341 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started
Successfully initialized from sql script: /opt/flink/sql/00-init.sql
Feb 21, 2026 2:42:36 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
[34;1m[INFO] Executing SQL from file.[0m

Command history file path: /root/.flink-sql-history
Flink SQL> -- =============================================================================
> -- Pipeline 01: Bronze Layer (Kafka â†’ Iceberg)[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> CREATE TABLE IF NOT EXISTS bronze.raw_trips (
>     VendorID                BIGINT,
>     tpep_pickup_datetime    TIMESTAMP(3),
>     tpep_dropoff_datetime   TIMESTAMP(3),
>     passenger_count         BIGINT,
>     trip_distance           DOUBLE,
>     RatecodeID              BIGINT,
>     store_and_fwd_flag      STRING,
>     PULocationID            BIGINT,
>     DOLocationID            BIGINT,
>     payment_type            BIGINT,
>     fare_amount             DOUBLE,
>     extra                   DOUBLE,
>     mta_tax                 DOUBLE,
>     tip_amount              DOUBLE,
>     tolls_amount            DOUBLE,
>     improvement_surcharge   DOUBLE,
>     total_amount            DOUBLE,
>     congestion_surcharge    DOUBLE,
>     Airport_fee             DOUBLE,
>     ingestion_ts            TIMESTAMP(3)
> )
> WITH (
>     'format-version' = '1',
>     'write.format.default' = 'parquet',
>     'write.parquet.compression-codec' = 'zstd',
>     'write.metadata.delete-after-commit.enabled' = 'true',
>     'write.metadata.previous-versions-max' = '10',
>     'write.target-file-size-bytes' = '134217728'
> )[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> INSERT INTO iceberg_catalog.bronze.raw_trips
> SELECT
>     VendorID,
>     TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,
>     TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')[34;1m[INFO] Complete execution of the SQL update statement.[0m

Flink SQL> 
Shutting down the session...
done.
Bronze layer complete.
=== Silver: Bronze Ã¢â€ â€™ Cleaned Iceberg ===
MSYS_NO_PATHCONV=1 docker compose exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/06-silver.sql
2026-02-21 14:42:50,740 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-02-21 14:42:50,764 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).
2026-02-21 14:42:50,764 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started
Successfully initialized from sql script: /opt/flink/sql/00-init.sql
Feb 21, 2026 2:42:51 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
[34;1m[INFO] Executing SQL from file.[0m

Command history file path: /root/.flink-sql-history
Flink SQL> -- =============================================================================
> -- Pipeline 01: Silver Layer (Bronze Iceberg â†’ Silver Iceberg)[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> CREATE TABLE IF NOT EXISTS silver.cleaned_trips (
>     trip_id                 STRING,
>     vendor_id               INT,
>     rate_code_id            INT,
>     pickup_location_id      INT,
>     dropoff_location_id     INT,
>     payment_type_id         INT,
>     pickup_datetime         TIMESTAMP(3),
>     dropoff_datetime        TIMESTAMP(3),
>     passenger_count         INT,
>     trip_distance_miles     DOUBLE,
>     store_and_fwd_flag      STRING,
>     fare_amount             DECIMAL(10, 2),
>     extra_amount            DECIMAL(10, 2),
>     mta_tax                 DECIMAL(10, 2),
>     tip_amount              DECIMAL(10, 2),
>     tolls_amount            DECIMAL(10, 2),
>     improvement_surcharge   DECIMAL(10, 2),
>     total_amount            DECIMAL(10, 2),
>     congestion_surcharge    DECIMAL(10, 2),
>     airport_fee             DECIMAL(10, 2),
>     pickup_date             DATE
> ) PARTITIONED BY (pickup_date)
> WITH (
>     'format-version' = '2',
>     'write.format.default' = 'parquet',
>     'write.parquet.compression-codec' = 'zstd',
>     'write.metadata.delete-after-commit.enabled' = 'true',
>     'write.metadata.previous-versions-max' = '10',
>     'write.target-file-size-bytes' = '134217728'
> )[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> -- Deduplication: ROW_NUMBER partitioned by natural key, keeping latest ingestion
> INSERT INTO iceberg_catalog.silver.cleaned_trips
> WITH deduped AS (
>     SELECT *,
>         ROW_NUMBER() OVER (
>             PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,
>                          PULocationID, DOLocationID, fare_amount, total_amount
>             ORDER BY ingestion_ts DESC
>         ) AS rn
>     FROM iceberg_catalog.bronze.raw_trips
>     WHERE tpep_pickup_datetime IS NOT NULL
>       AND tpep_dropoff_datetime IS NOT NULL
>       AND trip_distance >= 0
>       AND fare_amount >= 0
>       AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'
>       AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01'
> )
> SELECT
>     CAST(MD5(CONCAT_WS('|',
>         CAST(VendorID AS STRING),
>         CAST(tpep_pickup_datetime AS STRING),
>         CAST(tpep_dropoff_datetime AS STRING),
>         CAST(PULocationID AS STRING),
>         CAST(DOLocationID AS STRING),
>         CAST(fare_amount AS STRING),
>         CAST(total_amount AS STRING)
>     )) AS STRING) AS trip_id,
>     CAST(VendorID AS INT)       AS vendor_id,
>     CAST(RatecodeID AS INT)     AS rate_code_id,
>     CAST(PULocationID AS INT)   AS pickup_location_id,
>     CAST(DOLocationID AS INT)   AS dropoff_location_id,
>     CAST(payment_type AS INT)   AS payment_type_id,
>     tpep_pickup_datetime        AS pickup_datetime,
>     tpep_dropoff_datetime       AS dropoff_datetime,
>     CAST(passenger_count AS INT) AS passenger_count,
>     trip_distance               AS trip_distance_miles,
>     store_and_fwd_flag,
>     CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,
>     CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,
>     CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,
>     CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,
>     CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,
>     CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,
>     CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,
>     CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,
>     CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,
>     CAST(tpep_pickup_datetime AS DATE)[34;1m[INFO] Complete execution of the SQL update statement.[0m

Flink SQL> 
Shutting down the session...
done.
Silver layer complete.
All Flink SQL jobs complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
Waiting for Iceberg metadata commits to finalize...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir ."
 Container p01-minio  Running
[0m14:43:17  Running with dbt=1.11.5
[0m14:43:18  Installing dbt-labs/dbt_utils
[0m14:43:21  Installed from version 1.3.3
[0m14:43:21  Up to date!
[0m14:43:22  Running with dbt=1.11.5
[0m14:43:22  Registered adapter: duckdb=1.10.0
[0m14:43:22  Unable to do partial parsing because profile has changed
[0m14:43:24  Found 16 models, 79 data tests, 4 seeds, 1 source, 606 macros
[0m14:43:24  
[0m14:43:24  Concurrency: 4 threads (target='dev')
[0m14:43:24  
[0m14:43:29  1 of 99 START sql table model main_marts.dim_dates ............................. [RUN]
[0m14:43:29  2 of 99 START sql table model main_staging.stg_yellow_trips .................... [RUN]
[0m14:43:29  3 of 99 START seed file main_raw.payment_type_lookup ........................... [RUN]
[0m14:43:29  4 of 99 START seed file main_raw.rate_code_lookup .............................. [RUN]
[0m14:43:29  3 of 99 OK loaded seed file main_raw.payment_type_lookup ....................... [[32mCREATE 6[0m in 0.12s]
[0m14:43:29  4 of 99 OK loaded seed file main_raw.rate_code_lookup .......................... [[32mCREATE 7[0m in 0.12s]
[0m14:43:29  5 of 99 START seed file main_raw.taxi_zone_lookup .............................. [RUN]
[0m14:43:29  6 of 99 START seed file main_raw.vendor_lookup ................................. [RUN]
[0m14:43:29  2 of 99 OK created sql table model main_staging.stg_yellow_trips ............... [[32mOK[0m in 0.15s]
[0m14:43:29  7 of 99 START test not_null_payment_type_lookup_payment_type_id ................ [RUN]
[0m14:43:29  1 of 99 OK created sql table model main_marts.dim_dates ........................ [[32mOK[0m in 0.18s]
[0m14:43:29  8 of 99 START test unique_payment_type_lookup_payment_type_id .................. [RUN]
[0m14:43:29  5 of 99 OK loaded seed file main_raw.taxi_zone_lookup .......................... [[32mCREATE 265[0m in 0.13s]
[0m14:43:29  9 of 99 START test not_null_rate_code_lookup_rate_code_id ...................... [RUN]
[0m14:43:29  6 of 99 OK loaded seed file main_raw.vendor_lookup ............................. [[32mCREATE 2[0m in 0.13s]
[0m14:43:29  10 of 99 START test unique_rate_code_lookup_rate_code_id ....................... [RUN]
[0m14:43:29  7 of 99 PASS not_null_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.12s]
[0m14:43:29  11 of 99 START test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [RUN]
[0m14:43:29  8 of 99 PASS unique_payment_type_lookup_payment_type_id ........................ [[32mPASS[0m in 0.11s]
[0m14:43:29  12 of 99 START test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [RUN]
[0m14:43:29  9 of 99 PASS not_null_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.05s]
[0m14:43:29  10 of 99 PASS unique_rate_code_lookup_rate_code_id ............................. [[32mPASS[0m in 0.04s]
[0m14:43:29  13 of 99 START test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ........ [RUN]
[0m14:43:29  14 of 99 START test assert_fare_not_exceeds_total .............................. [RUN]
[0m14:43:29  11 of 99 PASS accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[32mPASS[0m in 0.06s]
[0m14:43:29  12 of 99 PASS accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[32mPASS[0m in 0.05s]
[0m14:43:29  15 of 99 START test not_null_stg_yellow_trips_dropoff_datetime ................. [RUN]
[0m14:43:29  16 of 99 START test not_null_stg_yellow_trips_dropoff_location_id .............. [RUN]
[0m14:43:29  14 of 99 PASS assert_fare_not_exceeds_total .................................... [[32mPASS[0m in 0.05s]
[0m14:43:29  13 of 99 PASS accepted_values_stg_yellow_trips_vendor_id__1__2__6 .............. [[32mPASS[0m in 0.06s]
[0m14:43:29  17 of 99 START test not_null_stg_yellow_trips_fare_amount ...................... [RUN]
[0m14:43:29  18 of 99 START test not_null_stg_yellow_trips_pickup_datetime .................. [RUN]
[0m14:43:29  15 of 99 PASS not_null_stg_yellow_trips_dropoff_datetime ....................... [[32mPASS[0m in 0.05s]
[0m14:43:29  16 of 99 PASS not_null_stg_yellow_trips_dropoff_location_id .................... [[32mPASS[0m in 0.05s]
[0m14:43:29  19 of 99 START test not_null_stg_yellow_trips_pickup_location_id ............... [RUN]
[0m14:43:29  20 of 99 START test not_null_stg_yellow_trips_total_amount ..................... [RUN]
[0m14:43:29  17 of 99 PASS not_null_stg_yellow_trips_fare_amount ............................ [[32mPASS[0m in 0.04s]
[0m14:43:29  21 of 99 START test not_null_stg_yellow_trips_trip_distance_miles .............. [RUN]
[0m14:43:29  18 of 99 PASS not_null_stg_yellow_trips_pickup_datetime ........................ [[32mPASS[0m in 0.05s]
[0m14:43:29  22 of 99 START test not_null_stg_yellow_trips_trip_id .......................... [RUN]
[0m14:43:29  19 of 99 PASS not_null_stg_yellow_trips_pickup_location_id ..................... [[32mPASS[0m in 0.05s]
[0m14:43:29  23 of 99 START test not_null_stg_yellow_trips_vendor_id ........................ [RUN]
[0m14:43:29  20 of 99 PASS not_null_stg_yellow_trips_total_amount ........................... [[32mPASS[0m in 0.04s]
[0m14:43:29  24 of 99 START test unique_stg_yellow_trips_trip_id ............................ [RUN]
[0m14:43:29  21 of 99 PASS not_null_stg_yellow_trips_trip_distance_miles .................... [[32mPASS[0m in 0.05s]
[0m14:43:29  22 of 99 PASS not_null_stg_yellow_trips_trip_id ................................ [[32mPASS[0m in 0.05s]
[0m14:43:29  25 of 99 START test not_null_dim_dates_date_key ................................ [RUN]
[0m14:43:29  26 of 99 START test not_null_dim_dates_day_of_week_name ........................ [RUN]
[0m14:43:29  23 of 99 PASS not_null_stg_yellow_trips_vendor_id .............................. [[32mPASS[0m in 0.05s]
[0m14:43:29  24 of 99 PASS unique_stg_yellow_trips_trip_id .................................. [[32mPASS[0m in 0.04s]
[0m14:43:29  27 of 99 START test not_null_dim_dates_is_holiday .............................. [RUN]
[0m14:43:29  28 of 99 START test not_null_dim_dates_is_weekend .............................. [RUN]
[0m14:43:29  25 of 99 PASS not_null_dim_dates_date_key ...................................... [[32mPASS[0m in 0.05s]
[0m14:43:29  26 of 99 PASS not_null_dim_dates_day_of_week_name .............................. [[32mPASS[0m in 0.05s]
[0m14:43:29  29 of 99 START test unique_dim_dates_date_key .................................. [RUN]
[0m14:43:29  30 of 99 START test not_null_taxi_zone_lookup_Borough .......................... [RUN]
[0m14:43:29  27 of 99 PASS not_null_dim_dates_is_holiday .................................... [[32mPASS[0m in 0.05s]
[0m14:43:29  31 of 99 START test not_null_taxi_zone_lookup_LocationID ....................... [RUN]
[0m14:43:29  28 of 99 PASS not_null_dim_dates_is_weekend .................................... [[32mPASS[0m in 0.05s]
[0m14:43:29  29 of 99 PASS unique_dim_dates_date_key ........................................ [[32mPASS[0m in 0.04s]
[0m14:43:29  32 of 99 START test unique_taxi_zone_lookup_LocationID ......................... [RUN]
[0m14:43:29  33 of 99 START test not_null_vendor_lookup_vendor_abbr ......................... [RUN]
[0m14:43:29  30 of 99 PASS not_null_taxi_zone_lookup_Borough ................................ [[32mPASS[0m in 0.05s]
[0m14:43:29  34 of 99 START test not_null_vendor_lookup_vendor_id ........................... [RUN]
[0m14:43:29  33 of 99 PASS not_null_vendor_lookup_vendor_abbr ............................... [[32mPASS[0m in 0.03s]
[0m14:43:29  34 of 99 PASS not_null_vendor_lookup_vendor_id ................................. [[32mPASS[0m in 0.03s]
[0m14:43:29  35 of 99 START test not_null_vendor_lookup_vendor_name ......................... [RUN]
[0m14:43:29  36 of 99 START test unique_vendor_lookup_vendor_abbr ........................... [RUN]
[0m14:43:29  31 of 99 PASS not_null_taxi_zone_lookup_LocationID ............................. [[32mPASS[0m in 0.05s]
[0m14:43:29  32 of 99 PASS unique_taxi_zone_lookup_LocationID ............................... [[32mPASS[0m in 0.05s]
[0m14:43:29  37 of 99 START test unique_vendor_lookup_vendor_id ............................. [RUN]
[0m14:43:29  38 of 99 START sql view model main_staging.stg_payment_types ................... [RUN]
[0m14:43:29  37 of 99 PASS unique_vendor_lookup_vendor_id ................................... [[32mPASS[0m in 0.04s]
[0m14:43:29  35 of 99 PASS not_null_vendor_lookup_vendor_name ............................... [[32mPASS[0m in 0.05s]
[0m14:43:29  39 of 99 START sql view model main_staging.stg_rate_codes ...................... [RUN]
[0m14:43:29  40 of 99 START sql view model main_intermediate.int_trip_metrics ............... [RUN]
[0m14:43:29  36 of 99 PASS unique_vendor_lookup_vendor_abbr ................................. [[32mPASS[0m in 0.06s]
[0m14:43:29  41 of 99 START sql view model main_staging.stg_taxi_zones ...................... [RUN]
[0m14:43:29  38 of 99 OK created sql view model main_staging.stg_payment_types .............. [[32mOK[0m in 0.07s]
[0m14:43:29  42 of 99 START sql view model main_staging.stg_vendors ......................... [RUN]
[0m14:43:29  40 of 99 OK created sql view model main_intermediate.int_trip_metrics .......... [[32mOK[0m in 0.08s]
[0m14:43:29  43 of 99 START test not_null_stg_payment_types_payment_type_id ................. [RUN]
[0m14:43:29  41 of 99 OK created sql view model main_staging.stg_taxi_zones ................. [[32mOK[0m in 0.07s]
[0m14:43:29  39 of 99 OK created sql view model main_staging.stg_rate_codes ................. [[32mOK[0m in 0.09s]
[0m14:43:29  44 of 99 START test not_null_stg_payment_types_payment_type_name ............... [RUN]
[0m14:43:29  45 of 99 START test unique_stg_payment_types_payment_type_id ................... [RUN]
[0m14:43:29  42 of 99 OK created sql view model main_staging.stg_vendors .................... [[32mOK[0m in 0.08s]
[0m14:43:29  46 of 99 START test assert_trip_duration_positive .............................. [RUN]
[0m14:43:29  43 of 99 PASS not_null_stg_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.05s]
[0m14:43:29  44 of 99 PASS not_null_stg_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m14:43:29  47 of 99 START test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [RUN]
[0m14:43:29  48 of 99 START test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [RUN]
[0m14:43:29  45 of 99 PASS unique_stg_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.05s]
[0m14:43:29  49 of 99 START test not_null_int_trip_metrics_is_weekend ....................... [RUN]
[0m14:43:29  46 of 99 PASS assert_trip_duration_positive .................................... [[32mPASS[0m in 0.05s]
[0m14:43:29  50 of 99 START test not_null_int_trip_metrics_pickup_date ...................... [RUN]
[0m14:43:29  49 of 99 PASS not_null_int_trip_metrics_is_weekend ............................. [[32mPASS[0m in 0.05s]
[0m14:43:29  47 of 99 PASS dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0 ..... [[32mPASS[0m in 0.06s]
[0m14:43:29  51 of 99 START test not_null_int_trip_metrics_pickup_hour ...................... [RUN]
[0m14:43:29  52 of 99 START test not_null_int_trip_metrics_trip_duration_minutes ............ [RUN]
[0m14:43:29  48 of 99 PASS dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[32mPASS[0m in 0.07s]
[0m14:43:29  53 of 99 START test not_null_int_trip_metrics_trip_id .......................... [RUN]
[0m14:43:29  50 of 99 PASS not_null_int_trip_metrics_pickup_date ............................ [[32mPASS[0m in 0.06s]
[0m14:43:29  54 of 99 START test not_null_stg_taxi_zones_borough ............................ [RUN]
[0m14:43:29  53 of 99 PASS not_null_int_trip_metrics_trip_id ................................ [[32mPASS[0m in 0.04s]
[0m14:43:29  51 of 99 PASS not_null_int_trip_metrics_pickup_hour ............................ [[32mPASS[0m in 0.05s]
[0m14:43:29  55 of 99 START test not_null_stg_taxi_zones_location_id ........................ [RUN]
[0m14:43:29  52 of 99 PASS not_null_int_trip_metrics_trip_duration_minutes .................. [[32mPASS[0m in 0.05s]
[0m14:43:30  56 of 99 START test not_null_stg_taxi_zones_zone_name .......................... [RUN]
[0m14:43:30  57 of 99 START test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m14:43:30  54 of 99 PASS not_null_stg_taxi_zones_borough .................................. [[32mPASS[0m in 0.04s]
[0m14:43:30  58 of 99 START test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [RUN]
[0m14:43:30  55 of 99 PASS not_null_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.05s]
[0m14:43:30  56 of 99 PASS not_null_stg_taxi_zones_zone_name ................................ [[32mPASS[0m in 0.04s]
[0m14:43:30  59 of 99 START test unique_stg_taxi_zones_location_id .......................... [RUN]
[0m14:43:30  60 of 99 START test not_null_stg_rate_codes_rate_code_id ....................... [RUN]
[0m14:43:30  57 of 99 PASS relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.05s]
[0m14:43:30  61 of 99 START test not_null_stg_rate_codes_rate_code_name ..................... [RUN]
[0m14:43:30  58 of 99 PASS relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[32mPASS[0m in 0.05s]
[0m14:43:30  62 of 99 START test unique_stg_rate_codes_rate_code_id ......................... [RUN]
[0m14:43:30  59 of 99 PASS unique_stg_taxi_zones_location_id ................................ [[32mPASS[0m in 0.05s]
[0m14:43:30  60 of 99 PASS not_null_stg_rate_codes_rate_code_id ............................. [[32mPASS[0m in 0.05s]
[0m14:43:30  63 of 99 START sql table model main_marts.dim_vendors .......................... [RUN]
[0m14:43:30  64 of 99 START sql table model main_marts.dim_payment_types .................... [RUN]
[0m14:43:30  61 of 99 PASS not_null_stg_rate_codes_rate_code_name ........................... [[32mPASS[0m in 0.05s]
[0m14:43:30  65 of 99 START sql view model main_intermediate.int_daily_summary .............. [RUN]
[0m14:43:30  62 of 99 PASS unique_stg_rate_codes_rate_code_id ............................... [[32mPASS[0m in 0.04s]
[0m14:43:30  66 of 99 START sql view model main_intermediate.int_hourly_patterns ............ [RUN]
[0m14:43:30  63 of 99 OK created sql table model main_marts.dim_vendors ..................... [[32mOK[0m in 0.06s]
[0m14:43:30  67 of 99 START sql table model main_marts.dim_locations ........................ [RUN]
[0m14:43:30  64 of 99 OK created sql table model main_marts.dim_payment_types ............... [[32mOK[0m in 0.08s]
[0m14:43:30  65 of 99 OK created sql view model main_intermediate.int_daily_summary ......... [[32mOK[0m in 0.07s]
[0m14:43:30  68 of 99 START test not_null_dim_payment_types_payment_type_id ................. [RUN]
[0m14:43:30  69 of 99 START test not_null_dim_payment_types_payment_type_name ............... [RUN]
[0m14:43:30  66 of 99 OK created sql view model main_intermediate.int_hourly_patterns ....... [[32mOK[0m in 0.08s]
[0m14:43:30  70 of 99 START test unique_dim_payment_types_payment_type_id ................... [RUN]
[0m14:43:30  68 of 99 PASS not_null_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.05s]
[0m14:43:30  69 of 99 PASS not_null_dim_payment_types_payment_type_name ..................... [[32mPASS[0m in 0.05s]
[0m14:43:30  71 of 99 START test dbt_utils_accepted_range_int_daily_summary_total_trips__0 .. [RUN]
[0m14:43:30  72 of 99 START test not_null_int_daily_summary_pickup_date ..................... [RUN]
[0m14:43:30  67 of 99 OK created sql table model main_marts.dim_locations ................... [[32mOK[0m in 0.08s]
[0m14:43:30  73 of 99 START test not_null_int_daily_summary_total_revenue ................... [RUN]
[0m14:43:30  70 of 99 PASS unique_dim_payment_types_payment_type_id ......................... [[32mPASS[0m in 0.06s]
[0m14:43:30  74 of 99 START test not_null_int_daily_summary_total_trips ..................... [RUN]
[0m14:43:30  72 of 99 PASS not_null_int_daily_summary_pickup_date ........................... [[32mPASS[0m in 0.12s]
[0m14:43:30  75 of 99 START test unique_int_daily_summary_pickup_date ....................... [RUN]
[0m14:43:30  71 of 99 PASS dbt_utils_accepted_range_int_daily_summary_total_trips__0 ........ [[32mPASS[0m in 0.13s]
[0m14:43:30  76 of 99 START test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [RUN]
[0m14:43:30  73 of 99 PASS not_null_int_daily_summary_total_revenue ......................... [[32mPASS[0m in 0.12s]
[0m14:43:30  77 of 99 START test not_null_int_hourly_patterns_pickup_date ................... [RUN]
[0m14:43:30  74 of 99 PASS not_null_int_daily_summary_total_trips ........................... [[32mPASS[0m in 0.12s]
[0m14:43:30  78 of 99 START test not_null_int_hourly_patterns_pickup_hour ................... [RUN]
[0m14:43:30  76 of 99 PASS dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0 .. [[32mPASS[0m in 0.04s]
[0m14:43:30  75 of 99 PASS unique_int_daily_summary_pickup_date ............................. [[32mPASS[0m in 0.05s]
[0m14:43:30  77 of 99 PASS not_null_int_hourly_patterns_pickup_date ......................... [[32mPASS[0m in 0.04s]
[0m14:43:30  79 of 99 START test not_null_int_hourly_patterns_total_trips ................... [RUN]
[0m14:43:30  80 of 99 START test not_null_dim_locations_borough ............................. [RUN]
[0m14:43:30  81 of 99 START test not_null_dim_locations_location_id ......................... [RUN]
[0m14:43:30  78 of 99 PASS not_null_int_hourly_patterns_pickup_hour ......................... [[32mPASS[0m in 0.05s]
[0m14:43:30  82 of 99 START test not_null_dim_locations_zone_name ........................... [RUN]
[0m14:43:30  80 of 99 PASS not_null_dim_locations_borough ................................... [[32mPASS[0m in 0.05s]
[0m14:43:30  83 of 99 START test unique_dim_locations_location_id ........................... [RUN]
[0m14:43:30  79 of 99 PASS not_null_int_hourly_patterns_total_trips ......................... [[32mPASS[0m in 0.06s]
[0m14:43:30  81 of 99 PASS not_null_dim_locations_location_id ............................... [[32mPASS[0m in 0.06s]
[0m14:43:30  84 of 99 START sql table model main_marts.mart_daily_revenue ................... [RUN]
[0m14:43:30  82 of 99 PASS not_null_dim_locations_zone_name ................................. [[32mPASS[0m in 0.05s]
[0m14:43:30  85 of 99 START sql table model main_marts.mart_hourly_demand ................... [RUN]
[0m14:43:30  83 of 99 PASS unique_dim_locations_location_id ................................. [[32mPASS[0m in 0.05s]
[0m14:43:30  86 of 99 START sql incremental model main_marts.fct_trips ...................... [RUN]
[0m14:43:30  85 of 99 OK created sql table model main_marts.mart_hourly_demand .............. [[32mOK[0m in 0.10s]
[0m14:43:30  84 of 99 OK created sql table model main_marts.mart_daily_revenue .............. [[32mOK[0m in 0.10s]
[0m14:43:30  87 of 99 START test not_null_mart_hourly_demand_is_weekend ..................... [RUN]
[0m14:43:30  88 of 99 START test not_null_mart_hourly_demand_pickup_hour .................... [RUN]
[0m14:43:30  89 of 99 START test not_null_mart_daily_revenue_date_key ....................... [RUN]
[0m14:43:30  88 of 99 PASS not_null_mart_hourly_demand_pickup_hour .......................... [[32mPASS[0m in 0.05s]
[0m14:43:30  87 of 99 PASS not_null_mart_hourly_demand_is_weekend ........................... [[32mPASS[0m in 0.05s]
[0m14:43:30  90 of 99 START test not_null_mart_daily_revenue_total_revenue .................. [RUN]
[0m14:43:30  91 of 99 START test unique_mart_daily_revenue_date_key ......................... [RUN]
[0m14:43:30  86 of 99 OK created sql incremental model main_marts.fct_trips ................. [[32mOK[0m in 0.11s]
[0m14:43:30  89 of 99 PASS not_null_mart_daily_revenue_date_key ............................. [[32mPASS[0m in 0.05s]
[0m14:43:30  92 of 99 START test not_null_fct_trips_pickup_datetime ......................... [RUN]
[0m14:43:30  93 of 99 START test not_null_fct_trips_total_amount ............................ [RUN]
[0m14:43:30  90 of 99 PASS not_null_mart_daily_revenue_total_revenue ........................ [[32mPASS[0m in 0.04s]
[0m14:43:30  91 of 99 PASS unique_mart_daily_revenue_date_key ............................... [[32mPASS[0m in 0.04s]
[0m14:43:30  94 of 99 START test not_null_fct_trips_trip_id ................................. [RUN]
[0m14:43:30  95 of 99 START test unique_fct_trips_trip_id ................................... [RUN]
[0m14:43:30  92 of 99 PASS not_null_fct_trips_pickup_datetime ............................... [[32mPASS[0m in 0.04s]
[0m14:43:30  93 of 99 PASS not_null_fct_trips_total_amount .................................. [[32mPASS[0m in 0.05s]
[0m14:43:30  94 of 99 PASS not_null_fct_trips_trip_id ....................................... [[32mPASS[0m in 0.05s]
[0m14:43:30  95 of 99 PASS unique_fct_trips_trip_id ......................................... [[32mPASS[0m in 0.04s]
[0m14:43:30  96 of 99 START sql table model main_marts.mart_location_performance ............ [RUN]
[0m14:43:30  96 of 99 OK created sql table model main_marts.mart_location_performance ....... [[32mOK[0m in 0.06s]
[0m14:43:30  97 of 99 START test not_null_mart_location_performance_pickup_location_id ...... [RUN]
[0m14:43:30  98 of 99 START test not_null_mart_location_performance_total_pickups ........... [RUN]
[0m14:43:30  99 of 99 START test unique_mart_location_performance_pickup_location_id ........ [RUN]
[0m14:43:30  99 of 99 PASS unique_mart_location_performance_pickup_location_id .............. [[32mPASS[0m in 0.05s]
[0m14:43:30  97 of 99 PASS not_null_mart_location_performance_pickup_location_id ............ [[32mPASS[0m in 0.05s]
[0m14:43:30  98 of 99 PASS not_null_mart_location_performance_total_pickups ................. [[32mPASS[0m in 0.05s]
[0m14:43:30  
[0m14:43:30  Finished running 1 incremental model, 4 seeds, 8 table models, 79 data tests, 7 view models in 0 hours 0 minutes and 6.38 seconds (6.38s).
[0m14:43:30  
[0m14:43:30  [32mCompleted successfully[0m
[0m14:43:30  
[0m14:43:30  Done. PASS=99 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=99
dbt build complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'

============================================================
  BENCHMARK COMPLETE
  Total elapsed: 118s
============================================================
Results saved to benchmark_results/latest.json
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
docker compose --profile generator --profile dbt --profile lakekeeper down -v
 Container p01-flink-taskmanager  Stopping
 Container p01-flink-taskmanager  Stopped
 Container p01-flink-taskmanager  Removing
 Container p01-flink-taskmanager  Removed
 Container p01-flink-jobmanager  Stopping
 Container p01-flink-jobmanager  Stopped
 Container p01-flink-jobmanager  Removing
 Container p01-flink-jobmanager  Removed
 Container p01-mc-init  Stopping
 Container p01-kafka  Stopping
 Container p01-mc-init  Stopped
 Container p01-mc-init  Removing
 Container p01-mc-init  Removed
 Container p01-minio  Stopping
 Container p01-minio  Stopped
 Container p01-minio  Removing
 Container p01-minio  Removed
 Container p01-kafka  Stopped
 Container p01-kafka  Removing
 Container p01-kafka  Removed
 Network p01-pipeline-net  Removing
 Volume 01-kafka-flink-iceberg_minio-data  Removing
 Volume 01-kafka-flink-iceberg_flink-checkpoints  Removing
 Volume 01-kafka-flink-iceberg_minio-data  Removed
 Volume 01-kafka-flink-iceberg_flink-checkpoints  Removed
 Network p01-pipeline-net  Removed
Pipeline 01 stopped and volumes removed.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/01-kafka-flink-iceberg'
docker compose --profile generator --profile dbt --profile lakekeeper down -v
Pipeline 01 stopped and volumes removed.
docker compose --profile generator --profile dbt --profile lakekeeper down -v --remove-orphans
docker network rm p01-pipeline-net 2>/dev/null || true
Pipeline 01 fully cleaned.
