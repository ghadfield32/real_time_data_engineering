docker compose --profile generator --profile dbt --profile quality down -v
docker compose up -d
 Network p11-pipeline-net  Creating
 Network p11-pipeline-net  Created
 Volume 11-observability-stack_flink-checkpoints  Creating
 Volume 11-observability-stack_flink-checkpoints  Created
 Volume 11-observability-stack_minio-data  Creating
 Volume 11-observability-stack_minio-data  Created
 Container p11-minio  Creating
 Container p11-kafka  Creating
 Container p11-kafka  Created
 Container p11-minio  Created
 Container p11-mc-init  Creating
 Container p11-mc-init  Created
 Container p11-flink-jobmanager  Creating
 Container p11-flink-jobmanager  Created
 Container p11-flink-taskmanager  Creating
 Container p11-flink-taskmanager  Created
 Container p11-kafka  Starting
 Container p11-minio  Starting
 Container p11-kafka  Started
 Container p11-minio  Started
 Container p11-minio  Waiting
 Container p11-minio  Healthy
 Container p11-mc-init  Starting
 Container p11-mc-init  Started
 Container p11-mc-init  Waiting
 Container p11-kafka  Waiting
 Container p11-mc-init  Exited
 Container p11-kafka  Healthy
 Container p11-flink-jobmanager  Starting
 Container p11-flink-jobmanager  Started
 Container p11-flink-jobmanager  Waiting
 Container p11-flink-jobmanager  Healthy
 Container p11-flink-taskmanager  Starting
 Container p11-flink-taskmanager  Started

=== Pipeline 11: Observability Stack ===
Kafka:            localhost:9092
Flink Dashboard:  http://localhost:8081
MinIO Console:    http://localhost:9001
============================================================
  Pipeline 11 Benchmark: Observability Stack
============================================================
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
docker compose up -d
 Container p11-minio  Running
 Container p11-kafka  Running
 Container p11-flink-jobmanager  Running
 Container p11-flink-taskmanager  Running
 Container p11-minio  Waiting
 Container p11-minio  Healthy
 Container p11-mc-init  Starting
 Container p11-mc-init  Started
 Container p11-kafka  Waiting
 Container p11-mc-init  Waiting
 Container p11-mc-init  Exited
 Container p11-kafka  Healthy
 Container p11-flink-jobmanager  Waiting
 Container p11-flink-jobmanager  Healthy

=== Pipeline 11: Observability Stack ===
Kafka:            localhost:9092
Flink Dashboard:  http://localhost:8081
MinIO Console:    http://localhost:9001
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
Waiting for services to stabilize...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
docker compose exec kafka /opt/kafka/bin/kafka-topics.sh \
	--bootstrap-server localhost:9092 \
	--create --if-not-exists \
	--topic taxi.raw_trips --partitions 3 --replication-factor 1
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic taxi.raw_trips.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
MSYS_NO_PATHCONV=1 docker compose run --rm -e MAX_EVENTS=10000 data-generator
 Container p11-kafka  Running
============================================================
  Taxi Trip Event Generator
============================================================
  Broker:     kafka:9092
  Topic:      taxi.raw_trips
  Mode:       burst
  Data:       /data/yellow_tripdata_2024-01.parquet
  Max events: 10,000

  Source: /data/yellow_tripdata_2024-01.parquet (2,964,624 rows, sending 10,000)

============================================================
  GENERATOR COMPLETE
  Events:  10,000
  Elapsed: 0.34s
  Rate:    29,024 events/sec
============================================================
  Metrics written to /tmp/generator_metrics.json
Limited data generation complete (10k events).
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
Waiting for Flink processing to catch up...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
=== Bronze: Kafka Ã¢â€ â€™ Iceberg ===
MSYS_NO_PATHCONV=1 docker compose exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/05-bronze.sql
2026-02-21 14:58:25,551 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-02-21 14:58:25,590 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).
2026-02-21 14:58:25,590 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started
Successfully initialized from sql script: file:/opt/flink/sql/00-init.sql
Feb 21, 2026 2:58:25 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
[34;1m[INFO] Executing SQL from file.[0m

Command history file path: /root/.flink-sql-history
Flink SQL> -- =============================================================================
> -- Pipeline 01: Bronze Layer (Kafka â†’ Iceberg)[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> CREATE TABLE IF NOT EXISTS bronze.raw_trips (
>     VendorID                BIGINT,
>     tpep_pickup_datetime    TIMESTAMP(3),
>     tpep_dropoff_datetime   TIMESTAMP(3),
>     passenger_count         BIGINT,
>     trip_distance           DOUBLE,
>     RatecodeID              BIGINT,
>     store_and_fwd_flag      STRING,
>     PULocationID            BIGINT,
>     DOLocationID            BIGINT,
>     payment_type            BIGINT,
>     fare_amount             DOUBLE,
>     extra                   DOUBLE,
>     mta_tax                 DOUBLE,
>     tip_amount              DOUBLE,
>     tolls_amount            DOUBLE,
>     improvement_surcharge   DOUBLE,
>     total_amount            DOUBLE,
>     congestion_surcharge    DOUBLE,
>     Airport_fee             DOUBLE,
>     ingestion_ts            TIMESTAMP(3)
> )
> WITH (
>     'format-version' = '1',
>     'write.format.default' = 'parquet',
>     'write.parquet.compression-codec' = 'zstd',
>     'write.metadata.delete-after-commit.enabled' = 'true',
>     'write.metadata.previous-versions-max' = '10',
>     'write.target-file-size-bytes' = '134217728'
> )[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> INSERT INTO iceberg_catalog.bronze.raw_trips
> SELECT
>     VendorID,
>     TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss')   AS tpep_pickup_datetime,
>     TO_TIMESTAMP(tpep_dropoff_datetime, 'yyyy-MM-dd''T''HH:mm:ss')[34;1m[INFO] Complete execution of the SQL update statement.[0m

Flink SQL> 
Shutting down the session...
done.
Bronze layer complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
Waiting for Bronze data to settle...
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
=== Silver: Bronze Ã¢â€ â€™ Cleaned Iceberg ===
MSYS_NO_PATHCONV=1 docker compose exec -T flink-jobmanager /opt/flink/bin/sql-client.sh embedded -i /opt/flink/sql/00-init.sql -f /opt/flink/sql/06-silver.sql
2026-02-21 14:58:46,283 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-02-21 14:58:46,318 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).
2026-02-21 14:58:46,318 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started
Successfully initialized from sql script: file:/opt/flink/sql/00-init.sql
Feb 21, 2026 2:58:46 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
[34;1m[INFO] Executing SQL from file.[0m

Command history file path: /root/.flink-sql-history
Flink SQL> -- =============================================================================
> -- Pipeline 01: Silver Layer (Bronze Iceberg â†’ Silver Iceberg)[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> [34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> CREATE TABLE IF NOT EXISTS silver.cleaned_trips (
>     trip_id                 STRING,
>     vendor_id               INT,
>     rate_code_id            INT,
>     pickup_location_id      INT,
>     dropoff_location_id     INT,
>     payment_type_id         INT,
>     pickup_datetime         TIMESTAMP(3),
>     dropoff_datetime        TIMESTAMP(3),
>     passenger_count         INT,
>     trip_distance_miles     DOUBLE,
>     store_and_fwd_flag      STRING,
>     fare_amount             DECIMAL(10, 2),
>     extra_amount            DECIMAL(10, 2),
>     mta_tax                 DECIMAL(10, 2),
>     tip_amount              DECIMAL(10, 2),
>     tolls_amount            DECIMAL(10, 2),
>     improvement_surcharge   DECIMAL(10, 2),
>     total_amount            DECIMAL(10, 2),
>     congestion_surcharge    DECIMAL(10, 2),
>     airport_fee             DECIMAL(10, 2),
>     pickup_date             DATE
> ) PARTITIONED BY (pickup_date)
> WITH (
>     'format-version' = '2',
>     'write.format.default' = 'parquet',
>     'write.parquet.compression-codec' = 'zstd',
>     'write.metadata.delete-after-commit.enabled' = 'true',
>     'write.metadata.previous-versions-max' = '10',
>     'write.target-file-size-bytes' = '134217728'
> )[34;1m[INFO] Execute statement succeeded.[0m

Flink SQL> 
> -- Deduplication: ROW_NUMBER partitioned by natural key, keeping latest ingestion
> INSERT INTO iceberg_catalog.silver.cleaned_trips
> WITH deduped AS (
>     SELECT *,
>         ROW_NUMBER() OVER (
>             PARTITION BY VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,
>                          PULocationID, DOLocationID, fare_amount, total_amount
>             ORDER BY ingestion_ts DESC
>         ) AS rn
>     FROM iceberg_catalog.bronze.raw_trips
>     WHERE tpep_pickup_datetime IS NOT NULL
>       AND tpep_dropoff_datetime IS NOT NULL
>       AND trip_distance >= 0
>       AND fare_amount >= 0
>       AND CAST(tpep_pickup_datetime AS DATE) >= DATE '2024-01-01'
>       AND CAST(tpep_pickup_datetime AS DATE) <  DATE '2024-02-01'
> )
> SELECT
>     CAST(MD5(CONCAT_WS('|',
>         CAST(VendorID AS STRING),
>         CAST(tpep_pickup_datetime AS STRING),
>         CAST(tpep_dropoff_datetime AS STRING),
>         CAST(PULocationID AS STRING),
>         CAST(DOLocationID AS STRING),
>         CAST(fare_amount AS STRING),
>         CAST(total_amount AS STRING)
>     )) AS STRING) AS trip_id,
>     CAST(VendorID AS INT)       AS vendor_id,
>     CAST(RatecodeID AS INT)     AS rate_code_id,
>     CAST(PULocationID AS INT)   AS pickup_location_id,
>     CAST(DOLocationID AS INT)   AS dropoff_location_id,
>     CAST(payment_type AS INT)   AS payment_type_id,
>     tpep_pickup_datetime        AS pickup_datetime,
>     tpep_dropoff_datetime       AS dropoff_datetime,
>     CAST(passenger_count AS INT) AS passenger_count,
>     trip_distance               AS trip_distance_miles,
>     store_and_fwd_flag,
>     CAST(ROUND(fare_amount, 2)             AS DECIMAL(10, 2)) AS fare_amount,
>     CAST(ROUND(extra, 2)                   AS DECIMAL(10, 2)) AS extra_amount,
>     CAST(ROUND(mta_tax, 2)                 AS DECIMAL(10, 2)) AS mta_tax,
>     CAST(ROUND(tip_amount, 2)              AS DECIMAL(10, 2)) AS tip_amount,
>     CAST(ROUND(tolls_amount, 2)            AS DECIMAL(10, 2)) AS tolls_amount,
>     CAST(ROUND(improvement_surcharge, 2)   AS DECIMAL(10, 2)) AS improvement_surcharge,
>     CAST(ROUND(total_amount, 2)            AS DECIMAL(10, 2)) AS total_amount,
>     CAST(ROUND(congestion_surcharge, 2)    AS DECIMAL(10, 2)) AS congestion_surcharge,
>     CAST(ROUND(Airport_fee, 2)             AS DECIMAL(10, 2)) AS airport_fee,
>     CAST(tpep_pickup_datetime AS DATE)[34;1m[INFO] Complete execution of the SQL update statement.[0m

Flink SQL> 
Shutting down the session...
done.
Silver layer complete.
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
make[1]: Entering directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
MSYS_NO_PATHCONV=1 docker compose run --rm --entrypoint /bin/sh dbt -c "dbt deps --profiles-dir . && dbt build --full-refresh --profiles-dir ."
 Container p11-minio  Running
[0m14:58:54  Running with dbt=1.11.5
[0m14:58:55  Installing dbt-labs/dbt_utils
[0m14:58:58  Installed from version 1.3.3
[0m14:58:58  Up to date!
[0m14:58:58  Installing elementary-data/elementary
[0m14:59:05  Installed from version 0.22.1
[0m14:59:05  Up to date!
[0m14:59:07  Running with dbt=1.11.5
[0m14:59:08  Registered adapter: duckdb=1.10.0
[0m14:59:11  Found 44 models, 74 data tests, 3 seeds, 2 operations, 1 source, 1440 macros
[0m14:59:11  
[0m14:59:11  Concurrency: 4 threads (target='dev')
[0m14:59:11  
[0m14:59:16  Elementary: Runtime data: {"config": {}, "dbt_version": "1.11.5", "elementary_version": "0.22.1", "database": "p11_dbt", "schema": "main"}
[0m14:59:16  1 of 1 START hook: elementary.on-run-start.0 ................................... [RUN]
[0m14:59:16  1 of 1 OK hook: elementary.on-run-start.0 ...................................... [[32mOK[0m in 0.06s]
[0m14:59:16  
[0m14:59:16  1 of 121 START sql incremental model main.data_monitoring_metrics .............. [RUN]
[0m14:59:16  2 of 121 START sql incremental model main.dbt_columns .......................... [RUN]
[0m14:59:16  3 of 121 START sql incremental model main.dbt_exposures ........................ [RUN]
[0m14:59:16  4 of 121 START sql incremental model main.dbt_groups ........................... [RUN]
[0m14:59:16  1 of 121 OK created sql incremental model main.data_monitoring_metrics ......... [[32mOK[0m in 0.35s]
[0m14:59:16  4 of 121 OK created sql incremental model main.dbt_groups ...................... [[32mOK[0m in 0.38s]
[0m14:59:16  5 of 121 START sql incremental model main.dbt_invocations ...................... [RUN]
[0m14:59:17  6 of 121 START sql incremental model main.dbt_metrics .......................... [RUN]
[0m14:59:17  3 of 121 OK created sql incremental model main.dbt_exposures ................... [[32mOK[0m in 0.41s]
[0m14:59:17  7 of 121 START sql incremental model main.dbt_models ........................... [RUN]
[0m14:59:17  5 of 121 OK created sql incremental model main.dbt_invocations ................. [[32mOK[0m in 0.41s]
[0m14:59:17  8 of 121 START sql incremental model main.dbt_run_results ...................... [RUN]
[0m14:59:17  2 of 121 ERROR creating sql incremental model main.dbt_columns ................. [[31mERROR[0m in 0.81s]
[0m14:59:17  9 of 121 START sql incremental model main.dbt_seeds ............................ [RUN]
[0m14:59:17  6 of 121 OK created sql incremental model main.dbt_metrics ..................... [[32mOK[0m in 0.43s]
[0m14:59:17  10 of 121 START sql incremental model main.dbt_snapshots ....................... [RUN]
[0m14:59:17  7 of 121 ERROR creating sql incremental model main.dbt_models .................. [[31mERROR[0m in 0.72s]
[0m14:59:17  11 of 121 START sql incremental model main.dbt_source_freshness_results ........ [RUN]
[0m14:59:17  8 of 121 OK created sql incremental model main.dbt_run_results ................. [[32mOK[0m in 0.37s]
[0m14:59:17  12 of 121 START sql incremental model main.dbt_sources ......................... [RUN]
[0m14:59:17  10 of 121 OK created sql incremental model main.dbt_snapshots .................. [[32mOK[0m in 0.30s]
[0m14:59:17  13 of 121 START sql incremental model main.dbt_tests ........................... [RUN]
[0m14:59:17  9 of 121 ERROR creating sql incremental model main.dbt_seeds ................... [[31mERROR[0m in 0.44s]
[0m14:59:17  14 of 121 START sql incremental model main.elementary_test_results ............. [RUN]
[0m14:59:17  11 of 121 OK created sql incremental model main.dbt_source_freshness_results ... [[32mOK[0m in 0.10s]
[0m14:59:17  15 of 121 START sql table model main.metadata .................................. [RUN]
[0m14:59:18  12 of 121 ERROR creating sql incremental model main.dbt_sources ................ [[31mERROR[0m in 0.41s]
[0m14:59:18  14 of 121 OK created sql incremental model main.elementary_test_results ........ [[32mOK[0m in 0.33s]
[0m14:59:18  16 of 121 START sql incremental model main.schema_columns_snapshot ............. [RUN]
[0m14:59:18  17 of 121 START sql table model main.dim_dates ................................. [RUN]
[0m14:59:18  15 of 121 OK created sql table model main.metadata ............................. [[32mOK[0m in 0.34s]
[0m14:59:18  18 of 121 START sql view model main.stg_yellow_trips ........................... [RUN]
[0m14:59:18  16 of 121 OK created sql incremental model main.schema_columns_snapshot ........ [[32mOK[0m in 0.57s]
[0m14:59:18  19 of 121 START seed file main.payment_type_lookup ............................. [RUN]
[0m14:59:18  18 of 121 ERROR creating sql view model main.stg_yellow_trips .................. [[31mERROR[0m in 0.68s]
[0m14:59:18  20 of 121 START seed file main.rate_code_lookup ................................ [RUN]
[0m14:59:18  17 of 121 OK created sql table model main.dim_dates ............................ [[32mOK[0m in 0.74s]
[0m14:59:18  13 of 121 ERROR creating sql incremental model main.dbt_tests .................. [[31mERROR[0m in 1.11s]
[0m14:59:18  21 of 121 START seed file main.taxi_zone_lookup ................................ [RUN]
[0m14:59:18  22 of 121 START sql view model main.metrics_anomaly_score ...................... [RUN]
[0m14:59:19  19 of 121 OK loaded seed file main.payment_type_lookup ......................... [[32mCREATE 6[0m in 0.10s]
[0m14:59:19  23 of 121 START sql view model main.monitors_runs .............................. [RUN]
[0m14:59:19  20 of 121 OK loaded seed file main.rate_code_lookup ............................ [[32mCREATE 7[0m in 0.07s]
[0m14:59:19  21 of 121 OK loaded seed file main.taxi_zone_lookup ............................ [[32mCREATE 265[0m in 0.06s]
[0m14:59:19  24 of 121 START sql view model main.job_run_results ............................ [RUN]
[0m14:59:19  25 of 121 SKIP relation main.model_run_results ................................. [[33mSKIP[0m]
[0m14:59:19  26 of 121 START sql view model main.snapshot_run_results ....................... [RUN]
[0m14:59:19  22 of 121 ERROR creating sql view model main.metrics_anomaly_score ............. [[31mERROR[0m in 0.09s]
[0m14:59:19  27 of 121 SKIP relation main.seed_run_results .................................. [[33mSKIP[0m]
[0m14:59:19  28 of 121 SKIP relation main.alerts_dbt_source_freshness ....................... [[33mSKIP[0m]
[0m14:59:19  29 of 121 START sql view model main.alerts_anomaly_detection ................... [RUN]
[0m14:59:19  23 of 121 OK created sql view model main.monitors_runs ......................... [[32mOK[0m in 0.14s]
[0m14:59:19  30 of 121 START sql view model main.alerts_dbt_tests ........................... [RUN]
[0m14:59:19  26 of 121 OK created sql view model main.snapshot_run_results .................. [[32mOK[0m in 0.14s]
[0m14:59:19  29 of 121 OK created sql view model main.alerts_anomaly_detection .............. [[32mOK[0m in 0.07s]
[0m14:59:19  24 of 121 OK created sql view model main.job_run_results ....................... [[32mOK[0m in 0.13s]
[0m14:59:19  30 of 121 OK created sql view model main.alerts_dbt_tests ...................... [[32mOK[0m in 0.06s]
[0m14:59:19  31 of 121 START sql view model main.alerts_schema_changes ...................... [RUN]
[0m14:59:19  32 of 121 START sql incremental model main.test_result_rows .................... [RUN]
[0m14:59:19  33 of 121 SKIP test accepted_values_stg_yellow_trips_payment_type_id__0__1__2__3__4__5__6  [[33mSKIP[0m]
[0m14:59:19  34 of 121 SKIP test accepted_values_stg_yellow_trips_rate_code_id__1__2__3__4__5__6__99  [[33mSKIP[0m]
[0m14:59:19  35 of 121 SKIP test accepted_values_stg_yellow_trips_vendor_id__1__2__6 ........ [[33mSKIP[0m]
[0m14:59:19  36 of 121 SKIP test assert_fare_not_exceeds_total .............................. [[33mSKIP[0m]
[0m14:59:19  37 of 121 SKIP test not_null_stg_yellow_trips_dropoff_datetime ................. [[33mSKIP[0m]
[0m14:59:19  38 of 121 SKIP test not_null_stg_yellow_trips_dropoff_location_id .............. [[33mSKIP[0m]
[0m14:59:19  39 of 121 SKIP test not_null_stg_yellow_trips_fare_amount ...................... [[33mSKIP[0m]
[0m14:59:19  40 of 121 SKIP test not_null_stg_yellow_trips_pickup_datetime .................. [[33mSKIP[0m]
[0m14:59:19  41 of 121 SKIP test not_null_stg_yellow_trips_pickup_location_id ............... [[33mSKIP[0m]
[0m14:59:19  42 of 121 SKIP test not_null_stg_yellow_trips_total_amount ..................... [[33mSKIP[0m]
[0m14:59:19  43 of 121 SKIP test not_null_stg_yellow_trips_trip_distance_miles .............. [[33mSKIP[0m]
[0m14:59:19  44 of 121 SKIP test not_null_stg_yellow_trips_trip_id .......................... [[33mSKIP[0m]
[0m14:59:19  45 of 121 SKIP test not_null_stg_yellow_trips_vendor_id ........................ [[33mSKIP[0m]
[0m14:59:19  46 of 121 SKIP test unique_stg_yellow_trips_trip_id ............................ [[33mSKIP[0m]
[0m14:59:19  47 of 121 START test not_null_dim_dates_date_key ............................... [RUN]
[0m14:59:19  48 of 121 START test not_null_dim_dates_day_of_week_name ....................... [RUN]
[0m14:59:19  32 of 121 OK created sql incremental model main.test_result_rows ............... [[32mOK[0m in 0.08s]
[0m14:59:19  49 of 121 START test not_null_dim_dates_is_holiday ............................. [RUN]
[0m14:59:19  31 of 121 OK created sql view model main.alerts_schema_changes ................. [[32mOK[0m in 0.09s]
[0m14:59:19  50 of 121 START test not_null_dim_dates_is_weekend ............................. [RUN]
[0m14:59:19  47 of 121 PASS not_null_dim_dates_date_key ..................................... [[32mPASS[0m in 0.07s]
[0m14:59:19  51 of 121 START test unique_dim_dates_date_key ................................. [RUN]
[0m14:59:19  48 of 121 PASS not_null_dim_dates_day_of_week_name ............................. [[32mPASS[0m in 0.07s]
[0m14:59:19  52 of 121 SKIP relation main.dbt_artifacts_hashes .............................. [[33mSKIP[0m]
[0m14:59:19  53 of 121 START test not_null_payment_type_lookup_payment_type_id .............. [RUN]
[0m14:59:19  49 of 121 PASS not_null_dim_dates_is_holiday ................................... [[32mPASS[0m in 0.06s]
[0m14:59:19  50 of 121 PASS not_null_dim_dates_is_weekend ................................... [[32mPASS[0m in 0.05s]
[0m14:59:19  54 of 121 START test unique_payment_type_lookup_payment_type_id ................ [RUN]
[0m14:59:19  55 of 121 START test not_null_rate_code_lookup_rate_code_id .................... [RUN]
[0m14:59:19  51 of 121 PASS unique_dim_dates_date_key ....................................... [[32mPASS[0m in 0.05s]
[0m14:59:19  56 of 121 START test unique_rate_code_lookup_rate_code_id ...................... [RUN]
[0m14:59:19  53 of 121 PASS not_null_payment_type_lookup_payment_type_id .................... [[32mPASS[0m in 0.05s]
[0m14:59:19  57 of 121 START test not_null_taxi_zone_lookup_Borough ......................... [RUN]
[0m14:59:19  55 of 121 PASS not_null_rate_code_lookup_rate_code_id .......................... [[32mPASS[0m in 0.04s]
[0m14:59:19  54 of 121 PASS unique_payment_type_lookup_payment_type_id ...................... [[32mPASS[0m in 0.05s]
[0m14:59:19  58 of 121 START test not_null_taxi_zone_lookup_LocationID ...................... [RUN]
[0m14:59:19  59 of 121 START test unique_taxi_zone_lookup_LocationID ........................ [RUN]
[0m14:59:19  57 of 121 PASS not_null_taxi_zone_lookup_Borough ............................... [[32mPASS[0m in 0.04s]
[0m14:59:19  56 of 121 PASS unique_rate_code_lookup_rate_code_id ............................ [[32mPASS[0m in 0.05s]
[0m14:59:19  60 of 121 SKIP relation main.anomaly_threshold_sensitivity ..................... [[33mSKIP[0m]
[0m14:59:19  61 of 121 SKIP relation main.alerts_dbt_models ................................. [[33mSKIP[0m]
[0m14:59:19  62 of 121 SKIP relation main.int_trip_metrics .................................. [[33mSKIP[0m]
[0m14:59:19  63 of 121 START sql view model main.stg_payment_types .......................... [RUN]
[0m14:59:19  64 of 121 START sql view model main.stg_rate_codes ............................. [RUN]
[0m14:59:19  59 of 121 PASS unique_taxi_zone_lookup_LocationID .............................. [[32mPASS[0m in 0.05s]
[0m14:59:19  58 of 121 PASS not_null_taxi_zone_lookup_LocationID ............................ [[32mPASS[0m in 0.06s]
[0m14:59:19  65 of 121 SKIP test assert_trip_duration_positive .............................. [[33mSKIP[0m]
[0m14:59:19  66 of 121 SKIP test dbt_utils_accepted_range_int_trip_metrics_pickup_hour__23__0  [[33mSKIP[0m]
[0m14:59:19  67 of 121 SKIP test dbt_utils_accepted_range_int_trip_metrics_trip_duration_minutes__720__1  [[33mSKIP[0m]
[0m14:59:19  68 of 121 SKIP test not_null_int_trip_metrics_is_weekend ....................... [[33mSKIP[0m]
[0m14:59:19  69 of 121 SKIP test not_null_int_trip_metrics_pickup_date ...................... [[33mSKIP[0m]
[0m14:59:19  70 of 121 SKIP test not_null_int_trip_metrics_pickup_hour ...................... [[33mSKIP[0m]
[0m14:59:19  71 of 121 SKIP test not_null_int_trip_metrics_trip_duration_minutes ............ [[33mSKIP[0m]
[0m14:59:19  72 of 121 SKIP test not_null_int_trip_metrics_trip_id .......................... [[33mSKIP[0m]
[0m14:59:19  73 of 121 START sql view model main.stg_taxi_zones ............................. [RUN]
[0m14:59:19  74 of 121 SKIP relation main.int_daily_summary ................................. [[33mSKIP[0m]
[0m14:59:19  75 of 121 SKIP relation main.int_hourly_patterns ............................... [[33mSKIP[0m]
[0m14:59:19  76 of 121 SKIP test dbt_utils_accepted_range_int_daily_summary_total_trips__0 .. [[33mSKIP[0m]
[0m14:59:19  77 of 121 SKIP test not_null_int_daily_summary_pickup_date ..................... [[33mSKIP[0m]
[0m14:59:19  78 of 121 SKIP test not_null_int_daily_summary_total_revenue ................... [[33mSKIP[0m]
[0m14:59:19  79 of 121 SKIP test not_null_int_daily_summary_total_trips ..................... [[33mSKIP[0m]
[0m14:59:19  80 of 121 SKIP test unique_int_daily_summary_pickup_date ....................... [[33mSKIP[0m]
[0m14:59:19  81 of 121 SKIP test dbt_utils_accepted_range_int_hourly_patterns_pickup_hour__23__0  [[33mSKIP[0m]
[0m14:59:19  64 of 121 OK created sql view model main.stg_rate_codes ........................ [[32mOK[0m in 0.09s]
[0m14:59:19  82 of 121 SKIP test not_null_int_hourly_patterns_pickup_date ................... [[33mSKIP[0m]
[0m14:59:19  63 of 121 OK created sql view model main.stg_payment_types ..................... [[32mOK[0m in 0.09s]
[0m14:59:19  83 of 121 SKIP test not_null_int_hourly_patterns_pickup_hour ................... [[33mSKIP[0m]
[0m14:59:19  84 of 121 SKIP test not_null_int_hourly_patterns_total_trips ................... [[33mSKIP[0m]
[0m14:59:19  85 of 121 SKIP relation main.mart_daily_revenue ................................ [[33mSKIP[0m]
[0m14:59:19  86 of 121 START test not_null_stg_rate_codes_rate_code_id ...................... [RUN]
[0m14:59:19  87 of 121 START test not_null_stg_rate_codes_rate_code_name .................... [RUN]
[0m14:59:19  88 of 121 START test unique_stg_rate_codes_rate_code_id ........................ [RUN]
[0m14:59:20  73 of 121 OK created sql view model main.stg_taxi_zones ........................ [[32mOK[0m in 0.17s]
[0m14:59:20  89 of 121 START test not_null_stg_payment_types_payment_type_id ................ [RUN]
[0m14:59:20  86 of 121 PASS not_null_stg_rate_codes_rate_code_id ............................ [[32mPASS[0m in 0.15s]
[0m14:59:20  90 of 121 START test not_null_stg_payment_types_payment_type_name .............. [RUN]
[0m14:59:20  88 of 121 PASS unique_stg_rate_codes_rate_code_id .............................. [[32mPASS[0m in 0.15s]
[0m14:59:20  91 of 121 START test unique_stg_payment_types_payment_type_id .................. [RUN]
[0m14:59:20  87 of 121 PASS not_null_stg_rate_codes_rate_code_name .......................... [[32mPASS[0m in 0.16s]
[0m14:59:20  92 of 121 SKIP relation main.mart_hourly_demand ................................ [[33mSKIP[0m]
[0m14:59:20  93 of 121 SKIP test not_null_mart_daily_revenue_date_key ....................... [[33mSKIP[0m]
[0m14:59:20  94 of 121 SKIP test not_null_mart_daily_revenue_total_revenue .................. [[33mSKIP[0m]
[0m14:59:20  95 of 121 SKIP test unique_mart_daily_revenue_date_key ......................... [[33mSKIP[0m]
[0m14:59:20  96 of 121 START test not_null_stg_taxi_zones_borough ........................... [RUN]
[0m14:59:20  89 of 121 PASS not_null_stg_payment_types_payment_type_id ...................... [[32mPASS[0m in 0.07s]
[0m14:59:20  97 of 121 START test not_null_stg_taxi_zones_location_id ....................... [RUN]
[0m14:59:20  90 of 121 PASS not_null_stg_payment_types_payment_type_name .................... [[32mPASS[0m in 0.05s]
[0m14:59:20  98 of 121 START test not_null_stg_taxi_zones_zone_name ......................... [RUN]
[0m14:59:20  91 of 121 PASS unique_stg_payment_types_payment_type_id ........................ [[32mPASS[0m in 0.05s]
[0m14:59:20  99 of 121 SKIP test relationships_stg_yellow_trips_dropoff_location_id__location_id__ref_stg_taxi_zones_  [[33mSKIP[0m]
[0m14:59:20  100 of 121 SKIP test relationships_stg_yellow_trips_pickup_location_id__location_id__ref_stg_taxi_zones_  [[33mSKIP[0m]
[0m14:59:20  101 of 121 START test unique_stg_taxi_zones_location_id ........................ [RUN]
[0m14:59:20  97 of 121 PASS not_null_stg_taxi_zones_location_id ............................. [[32mPASS[0m in 0.05s]
[0m14:59:20  102 of 121 SKIP test not_null_mart_hourly_demand_is_weekend .................... [[33mSKIP[0m]
[0m14:59:20  96 of 121 PASS not_null_stg_taxi_zones_borough ................................. [[32mPASS[0m in 0.06s]
[0m14:59:20  103 of 121 SKIP test not_null_mart_hourly_demand_pickup_hour ................... [[33mSKIP[0m]
[0m14:59:20  104 of 121 START sql table model main.dim_payment_types ........................ [RUN]
[0m14:59:20  98 of 121 PASS not_null_stg_taxi_zones_zone_name ............................... [[32mPASS[0m in 0.05s]
[0m14:59:20  101 of 121 PASS unique_stg_taxi_zones_location_id .............................. [[32mPASS[0m in 0.04s]
[0m14:59:20  105 of 121 START sql table model main.dim_locations ............................ [RUN]
[0m14:59:20  104 of 121 OK created sql table model main.dim_payment_types ................... [[32mOK[0m in 0.07s]
[0m14:59:20  106 of 121 START test not_null_dim_payment_types_payment_type_id ............... [RUN]
[0m14:59:20  107 of 121 START test not_null_dim_payment_types_payment_type_name ............. [RUN]
[0m14:59:20  108 of 121 START test unique_dim_payment_types_payment_type_id ................. [RUN]
[0m14:59:20  105 of 121 OK created sql table model main.dim_locations ....................... [[32mOK[0m in 0.07s]
[0m14:59:20  109 of 121 START test not_null_dim_locations_borough ........................... [RUN]
[0m14:59:20  108 of 121 PASS unique_dim_payment_types_payment_type_id ....................... [[32mPASS[0m in 0.06s]
[0m14:59:20  110 of 121 START test not_null_dim_locations_location_id ....................... [RUN]
[0m14:59:20  107 of 121 PASS not_null_dim_payment_types_payment_type_name ................... [[32mPASS[0m in 0.06s]
[0m14:59:20  106 of 121 PASS not_null_dim_payment_types_payment_type_id ..................... [[32mPASS[0m in 0.07s]
[0m14:59:20  111 of 121 START test not_null_dim_locations_zone_name ......................... [RUN]
[0m14:59:20  109 of 121 PASS not_null_dim_locations_borough ................................. [[32mPASS[0m in 0.06s]
[0m14:59:20  112 of 121 START test unique_dim_locations_location_id ......................... [RUN]
[0m14:59:20  110 of 121 PASS not_null_dim_locations_location_id ............................. [[32mPASS[0m in 0.05s]
[0m14:59:20  111 of 121 PASS not_null_dim_locations_zone_name ............................... [[32mPASS[0m in 0.05s]
[0m14:59:20  112 of 121 PASS unique_dim_locations_location_id ............................... [[32mPASS[0m in 0.05s]
[0m14:59:20  113 of 121 SKIP relation main.fct_trips ........................................ [[33mSKIP[0m]
[0m14:59:20  114 of 121 SKIP test not_null_fct_trips_pickup_datetime ........................ [[33mSKIP[0m]
[0m14:59:20  115 of 121 SKIP test not_null_fct_trips_total_amount ........................... [[33mSKIP[0m]
[0m14:59:20  116 of 121 SKIP test not_null_fct_trips_trip_id ................................ [[33mSKIP[0m]
[0m14:59:20  117 of 121 SKIP test unique_fct_trips_trip_id .................................. [[33mSKIP[0m]
[0m14:59:20  118 of 121 SKIP relation main.mart_location_performance ........................ [[33mSKIP[0m]
[0m14:59:20  119 of 121 SKIP test not_null_mart_location_performance_pickup_location_id ..... [[33mSKIP[0m]
[0m14:59:20  120 of 121 SKIP test not_null_mart_location_performance_total_pickups .......... [[33mSKIP[0m]
[0m14:59:20  121 of 121 SKIP test unique_mart_location_performance_pickup_location_id ....... [[33mSKIP[0m]
[0m14:59:20  
[0m14:59:21  
[0m14:59:21  [33mExited because of keyboard interrupt[0m
[0m14:59:21  
[0m14:59:21  [31mFailure in model dbt_columns (models/edr/dbt_artifacts/dbt_columns.sql)[0m
[0m14:59:21    Runtime Error in model dbt_columns (models/edr/dbt_artifacts/dbt_columns.sql)
  Parser Error: syntax error at or near "s"
  
  LINE 10: ...'[]','{}','p11_dbt','main','dbt_invocations','The environment\'s name, defined in the `DBT_ENV` env var.','model','2026...
                                                                             ^
[0m14:59:21  
[0m14:59:21    compiled code at target/compiled/elementary/models/edr/dbt_artifacts/dbt_columns.sql
[0m14:59:21  
[0m14:59:21  [31mFailure in model dbt_models (models/edr/dbt_artifacts/dbt_models.sql)[0m
[0m14:59:21    Runtime Error in model dbt_models (models/edr/dbt_artifacts/dbt_models.sql)
  Parser Error: syntax error at or near "elementary"
  
  LINE 10: ... data is loaded to this model on an on-run-end hook named \'elementary.upload_run_results\' from each invocation that...
                                                                          ^
[0m14:59:21  
[0m14:59:21    compiled code at target/compiled/elementary/models/edr/dbt_artifacts/dbt_models.sql
[0m14:59:21  
[0m14:59:21  [31mFailure in model dbt_seeds (models/edr/dbt_artifacts/dbt_seeds.sql)[0m
[0m14:59:21    Runtime Error in model dbt_seeds (models/edr/dbt_artifacts/dbt_seeds.sql)
  TransactionContext Error: cannot start a transaction within a transaction
[0m14:59:21  
[0m14:59:21    compiled code at target/compiled/elementary/models/edr/dbt_artifacts/dbt_seeds.sql
[0m14:59:21  
[0m14:59:21  [31mFailure in model dbt_sources (models/edr/dbt_artifacts/dbt_sources.sql)[0m
[0m14:59:21    Runtime Error in model dbt_sources (models/edr/dbt_artifacts/dbt_sources.sql)
  Parser Error: syntax error at or near "s3"
  
  LINE 10: ... record is above an acceptable SLA threshold.','iceberg_scan(\'s3://warehouse/silver/cleaned_trips\', allow_moved_paths...
                                                                             ^
[0m14:59:21  
[0m14:59:21    compiled code at target/compiled/elementary/models/edr/dbt_artifacts/dbt_sources.sql
[0m14:59:21  
[0m14:59:21  [31mFailure in model stg_yellow_trips (models/staging/stg_yellow_trips.sql)[0m
[0m14:59:21    Runtime Error in model stg_yellow_trips (models/staging/stg_yellow_trips.sql)
  Binder Error: Referenced column "tpep_pickup_datetime" not found in FROM clause!
  Candidate bindings: "pickup_datetime", "pickup_date", "trip_distance_miles", "pickup_location_id", "rate_code_id"
  
  LINE 44:     where tpep_pickup_datetime is not null
                     ^
[0m14:59:21  
[0m14:59:21    compiled code at target/compiled/nyc_taxi_pipeline_11/models/staging/stg_yellow_trips.sql
[0m14:59:21  
[0m14:59:21  [31mFailure in model dbt_tests (models/edr/dbt_artifacts/dbt_tests.sql)[0m
[0m14:59:21    Runtime Error in model dbt_tests (models/edr/dbt_artifacts/dbt_tests.sql)
  Parser Error: syntax error at or near "taxi_zone_lookup"
  
  LINE 10: ...name": "LocationID", "model": "{{ get_where_subquery(ref(\'taxi_zone_lookup\')) }}"}',NULL,'unique','[]','[]','[]'...
                                                                         ^
[0m14:59:21  
[0m14:59:21    compiled code at target/compiled/elementary/models/edr/dbt_artifacts/dbt_tests.sql
[0m14:59:21  
[0m14:59:21  [31mFailure in model metrics_anomaly_score (models/edr/data_monitoring/anomaly_detection/metrics_anomaly_score.sql)[0m
[0m14:59:21    Runtime Error in model metrics_anomaly_score (models/edr/data_monitoring/anomaly_detection/metrics_anomaly_score.sql)
  Catalog Error: Scalar Function with name dateadd does not exist!
  Did you mean "date_add"?
  
  LINE 66:     dateadd(day, cast(-7 as integer), cast(date_trunc('day', 
               ^
[0m14:59:21  
[0m14:59:21    compiled code at target/compiled/elementary/models/edr/data_monitoring/anomaly_detection/metrics_anomaly_score.sql
[0m14:59:21  
[0m14:59:21  Done. PASS=57 WARN=0 ERROR=7 SKIP=58 NO-OP=0 TOTAL=122
[0m14:59:21  
[0m14:59:21  Finished running 17 incremental models, 1 project hook, 3 seeds, 7 table models, 74 data tests, 20 view models in 0 hours 0 minutes and 10.20 seconds (10.20s).
[0m14:59:21  Encountered an error:
Runtime Error
  Parser Error: syntax error at or near "dummy_string"
  
  LINE 12: ...        select\n            \n                \n        cast(\'dummy_string\' as varchar(4096)) as id\n\n,\n          ...
                                                                             ^

make[1]: *** [Makefile:46: dbt-build] Error 2
make[1]: Leaving directory 'C:/docker_projects/real_time_data_engineering/pipelines/11-observability-stack'
make: *** [Makefile:63: benchmark] Error 2
docker compose --profile generator --profile dbt --profile quality down -v
 Container p11-flink-taskmanager  Stopping
 Container p11-flink-taskmanager  Stopped
 Container p11-flink-taskmanager  Removing
 Container p11-flink-taskmanager  Removed
 Container p11-flink-jobmanager  Stopping
 Container p11-flink-jobmanager  Stopped
 Container p11-flink-jobmanager  Removing
 Container p11-flink-jobmanager  Removed
 Container p11-mc-init  Stopping
 Container p11-kafka  Stopping
 Container p11-mc-init  Stopped
 Container p11-mc-init  Removing
 Container p11-mc-init  Removed
 Container p11-minio  Stopping
 Container p11-minio  Stopped
 Container p11-minio  Removing
 Container p11-minio  Removed
 Container p11-kafka  Stopped
 Container p11-kafka  Removing
 Container p11-kafka  Removed
 Network p11-pipeline-net  Removing
 Volume 11-observability-stack_minio-data  Removing
 Volume 11-observability-stack_flink-checkpoints  Removing
 Volume 11-observability-stack_minio-data  Removed
 Volume 11-observability-stack_flink-checkpoints  Removed
 Network p11-pipeline-net  Removed
make: *** No rule to make target 'clean'.  Stop.
