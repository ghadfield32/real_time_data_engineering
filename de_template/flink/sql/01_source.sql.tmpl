-- =============================================================================
-- 01_source.sql.tmpl — Kafka Source Table (BATCH mode)
-- =============================================================================
-- Rendered by scripts/render_sql.py from environment variables.
-- Sets batch execution mode + defines the Kafka source table.
--
-- *** CRITICAL: Column names must match JSON keys from Kafka EXACTLY (case-sensitive). ***
-- The generator serialises Parquet column names as-is into JSON keys.
-- NYC Taxi Parquet uses mixed case: VendorID, RatecodeID, PULocationID, DOLocationID,
-- Airport_fee. Use these original names in the Kafka source table — do NOT rename here.
-- All snake_case renaming happens in the Silver layer (06_silver.sql.tmpl).
--
-- *** CRITICAL: Timestamp format from Python datetime.isoformat() is 'T'-separated. ***
-- Use 'yyyy-MM-dd''T''HH:mm:ss' (note double-quoted T in Flink SQL string literals).
-- Using 'yyyy-MM-dd HH:mm:ss' (space) will produce all-NULL timestamps.
--
-- CUSTOMIZE THIS FILE for a new dataset:
--   1. Rename kafka_raw_trips → kafka_raw_{your_domain}
--   2. Replace the column list with your JSON schema fields (match JSON keys exactly)
--   3. Update the timestamp field name and format pattern
--   4. Update event_time computed column expression
--   5. '${TOPIC}' stays as-is — substituted from .env by render_sql.py
-- =============================================================================

-- Batch session settings (table.dml-sync blocks until INSERT finishes)
SET 'execution.runtime-mode' = 'batch';
SET 'table.dml-sync' = 'true';

-- Kafka source: bounded scan (reads all existing messages and stops)
CREATE TABLE IF NOT EXISTS kafka_raw_trips (
    -- NYC Yellow Taxi fields — names match Parquet column names / JSON keys EXACTLY
    VendorID                BIGINT,          -- JSON key: "VendorID"
    tpep_pickup_datetime    STRING,          -- raw ISO timestamp: "2024-01-01T00:32:47"
    tpep_dropoff_datetime   STRING,          -- raw ISO timestamp
    passenger_count         BIGINT,
    trip_distance           DOUBLE,
    RatecodeID              BIGINT,          -- JSON key: "RatecodeID"
    store_and_fwd_flag      STRING,
    PULocationID            BIGINT,          -- JSON key: "PULocationID"
    DOLocationID            BIGINT,          -- JSON key: "DOLocationID"
    payment_type            BIGINT,
    fare_amount             DOUBLE,
    extra                   DOUBLE,
    mta_tax                 DOUBLE,
    tip_amount              DOUBLE,
    tolls_amount            DOUBLE,
    improvement_surcharge   DOUBLE,
    total_amount            DOUBLE,
    congestion_surcharge    DOUBLE,
    Airport_fee             DOUBLE,          -- JSON key: "Airport_fee"

    -- Computed: event time for watermarking
    -- Python isoformat() → 'yyyy-MM-dd''T''HH:mm:ss' (T separator, note escaped T)
    event_time AS TO_TIMESTAMP(tpep_pickup_datetime, 'yyyy-MM-dd''T''HH:mm:ss'),
    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND
) WITH (
    'connector'                     = 'kafka',
    'topic'                         = '${TOPIC}',
    'properties.bootstrap.servers'  = 'broker:9092',
    'properties.group.id'           = 'flink-consumer',
    'scan.startup.mode'             = 'earliest-offset',
    'scan.bounded.mode'             = 'latest-offset',   -- batch: stop at current end
    'format'                        = 'json',
    'json.ignore-parse-errors'      = 'true'
);
